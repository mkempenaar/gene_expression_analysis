[
["index.html", "Analysis of Gene Expression Preface", " Analysis of Gene Expression Marcel Kempenaar 2018-03-08 Preface This project aims to teach the whole process of analysing a dataset containing gene expression data measured with RNA-sequencing technique. The tools we will use to perform the analysis will be the statistical programming language R with its many libraries and the software package RStudio to interact with R. Instead of following the usual weekly deliverables and final assignment, this course is setup around performing your own research, based on published data and given a general guideline with a set of defined end-products. Furthermore, this whole course is done individually. Cooperating and discussing however with your fellow students and helping each other is desired/ expected. "],
["chapter-1.html", " Introduction 1.1 Theme 7 - Analysis of Gene Expression Project 1.2 Project Deliverables 1.3 Project Schedule 1.4 Grading 1.5 Project Proposal 1.6 Lab Journal 1.7 Article", " Introduction 1.1 Theme 7 - Analysis of Gene Expression Project The use of RNA-Sequencing techniques for measuring gene expression is relatively new and replaces microarrays, though in some cases microarrays are still used. Gene expression data gives valuable insights into the workings of cells in certain conditions. Especially when comparing for instance healthy and diseased samples it can become clear which genes are causal or under influence of a specific condition. Finding the genes of interest (genes showing differing expression accross conditions, called the Differentially Expressed Genes (DEGs)) is the goal of this project. While there is no golden standard for analyzing RNA-sequencing datasets as there are many tools (all manufacturers of sequencing equiptment also deliver software packages) we will use proven R libraries for processing, visualizing and analyzing publically available datasets. While in some cases you are allowed to use the actual raw data that is available, it is highly recommended to use the pre-processed data which often is a table with a count value for each gene. This count is the number of reads that was mapped to that gene which corresponds to the relative number of transcripts (mRNA sequences) of that gene present in the cell at the time of sampling. 1.2 Project Deliverables The end products of this course consist of three deliverables; a PDF output file from an RMarkdown ‘lab journal’ where you have logged all steps performed to get to the end result, a final report in the form of a short article and a poster to be presented - if selected - during the Life Science and Technology Poster session at the end of this quarter. 1.3 Project Schedule The aim is to keep to the below schedule during this course. Use the first two weeks to see if you need to focus more on one of the points below and discuss changes to the planning with your teacher. Find a public experiment of interest [week one] Using online resources Data Gathering and Literature Research [week two] Make a final project choice Retrieve the accompanying publication Download and inspect the supplementary files Write and present a short project proposal Data Analysis [weeks three and four] Exploratory Data Analysis Data Annotation Discovering Differentially Expressed Genes (DEGs) Multiple Testing Techniques used: R with bioconductor, the EdgeR and/ or DESeq2 packages, RMarkdown Result Analysis [weeks five and six] Analyzing and Visualizing your results Techniques used: clustering, pathway analysis, gene-enrichment analysis Finalizing analysis and start writing the final report and design a poster [weeks 7 and 8] 1.4 Grading The final grade consists of a weighted average of the work done for weeks 1 &amp; 2 (resulting in the project proposal) (15%), the lab journal containing all performed steps, their code and outputs (50%), the article report and poster (25%) and finally your work attitude (10%). A grade higher then a 5.5 will give a total of 5 EC. The rest of this chapter explains the expected contents for the three graded products (each of these elements is explained in greater detail in other chapters). 1.5 Project Proposal As you are free to choose from thousands of public data sets on a large variety of biological subjects, it is important to demonstrate a good understanding of the experiment, biology and the available data. This is demonstrated by presenting a short project proposal where you briefly explain the subject of the chosen research article, the experimental setup (how many samples were used, etc.) and what types of data are available for you to use. 1.6 Lab Journal As you know from previous projects and (maybe from) working in the laboratory, it is essential to keep a proper lab journal detailing every step you have done during the experiment. The log is to be kept in an R markdown file, showing which steps have been taken in the analysis of the data set. This markdown should be knitted into a single PDF-file once the project is completed thus containing text detailling the steps and any decisions you’ve made, R-code (always visible!) and their resulting output/ images. As a general advice; do not wait with knitting this whole document until the project is done! 1.7 Article The final report is written in article form which is a bit different from a usual report, mainly in its size. The article-report has a maximum number of pages of 4 including all images and references (no appendices!). Contents for this article should be extracted from the lab journal combined with part introduction and part conclusion/ discussion. The sections below describe a template that is available for writing this report and example report(s) will be made available for inspiration. Refer to this chapter again once you start writing the report. 1.7.1 Installing the Article Template The templates are available in an R package and contains both RMarkdown and (another layer on top of the actual markup language, yes, another language…) files. RStudio can use templates for a number of documents, including article-templates. These templates can be installed from a package called rticles by running the install.packages function (note: might already be installed): install.packages(&quot;rticles&quot;, type = &quot;source&quot;) 1.7.2 Using the Template Now that you have the templates package (rticles) installed you can download and use the template project (ZIP-file) available from the course website. Download this file to your project folder, extract its contents and open the report-template.Rmd file contained within the folder in RStudio. Verify that everything is setup correctly by hitting the Knit button at the top, this should create a PDF version of the report. Note that - somehow - the resulting PDF file is named RJwrapper.pdf instead of the expected report-template.pdf. If everything checks out ok you can rename the file to your liking and start editing. This template is based on the R Journal Submission template that you can also find in RStudio in the New file -&gt; R Markdown -&gt; From Template menu. Articles published in the R-journal are based on this template which you can browse for inspiration at the R Journal Website. The available template shows an example of segments/ chapters and briefly describes what each section could or should contain. If you want to write your report in the Dutch language, you can create a new file from the template and change the segment names and content to Dutch. There will however still be some headings added by the template in English which is fine with me, but if you want to keep everything Dutch you can either edit the RJournal.sty file manually (not recommended) or start a new file by yourself and add some nice headings and page options. The top part of the template (between the dashes ---) contains some settings that you need to change such as title, author and abstract. Compare for instance a newly created article from this template with the one offered from the project-repository website. "],
["chapter-2.html", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest 2.2 The NCBI Gene Expression Omnibus 2.3 Creating a Project Proposal", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest This chapter describes a method of finding public datasets of interest. A dataset in the context of this course refers to all data belonging to a certain gene-expression experiment, usually consisting of a number of sequencing-samples combined with meta-data describing the experiment. There are a number of very large databases online that offer access to thousands of - published - experiments and here we will focus on searching and downloading gene expression data from high-throughput (NGS) sequencing techniques (RNA-Seq). With thousands of freely available datasets it is possible to start performing research without the need of actually performing your own lab-experiment. For all common conditions, organisms and tissues you can download samples and compare them over multiple studies to find novel relations between genes and conditions or to verify experiments performed in your laboratory. For instance, the power of public datasets was demonstrated jointly by three of our alumni in an article called Calling genotypes from public RNA-sequencing data enables identification of genetic variants that affect gene-expression levels.(al. 2015). It’s method section begins with the sentence “We downloaded the raw reads for all available human RNA-seq datasets …” of which the amount of data and work will become clear later on. If you are interested in expression Quantitative Trait Loci (eQTL) or Allele-Specific Expression (ASE) analysis, please read this very interesting paper. The following weeks you will be investigating one or more public RNA-sequencing datasets yourself and the rest of this document focuses on databases containing such public data sets and - especially - how to find a dataset or experiment you would like to use. But before we start diving into large and complex databases potentially containing thousands of experiments with millions of samples and terabytes of data, we need to get a rough idea on what we would like to do once we have found something of interest in order to know what we are looking for in the first place. While there is no definitive guide or protocol that can be followed for processing and analysing a gene-expression dataset, the following goals can be considered: Re-do (part of) the analysis described in the accompanying publication. As all datasets - except for the ones that were added very recently - are accompanied by a publication, you can gather a lot of information regarding experimental design and results from just this paper. Most often, the researchers already performed the most interesting research on this dataset and it is therefore a good exercise to try and reproduce their results. Alternatively you can come up with your own ideas and (biological) question(s) that you could try and answer given a dataset instead of redoing the published work. This is of course more challenging and not suitable for all datasets. You might notice that some publications only focus on a small set of genes instead of the whole transcriptome. For instance, when you perform an experiment on yeast where you want to measure the activity of genes involved in alcohol fermentation, researchers might only look at genes from the Glycolysis pathway. This leaves the other 6000+ yeast genes for you to explore and possibly come up with novel relations of gene expression and experimental setup. If the analysis approach explained in the paper is not focusing on a pre-selected list of genes, depending on the experiment you might come up with comparisons not done in the original research. For instance, in an experiment where the maternal age (at birth) is correlated to autism in their offspring, the paternal age is also known but not addressed in the publication. This could be a subject of further research (spoiler alert; no clear conclusion can be drawn from including this extra factor…). Another type of project is to evaluate different methods of either normalisation or statistical analysis to either confirm the published results or find novel genes involved in the experiment. This can be viewed more as a technical research subject which is ok to pursue, however the final report should mainly concern the biological impact of your findings. This means that when the accompanying publication shows very conclusive results, it will probably come down to acknowledging their results (therefore, you extend on point 1 from this list). Because with a very conclusive result you hope to find the same results and if this is not the case it might become hard to formulate a good conclusion in the end. However, if the publication isn’t very specific and only states a conclusion such as ‘Benzene exposure shows increased risk of leukemia’ followed by a list of a few genes that might be involved, you could try to see if you can find other genes that might be involved by changing the analysis approach. Although this last project goal has many risks involved and will need a very sound project proposal, it might result in the most interesting project. As shown before with the linked article, it is possible to combine data from multiple experiments. You could for instance find two very related experiments (i.e. researching the effect of a certain drug) both measuring expression in different tissue types (i.e. liver and brain tissue). After analyzing both experiments, you could present a set of genes that show an effect in both tissues and - more interestingly - genes showing an effect in only one tissue. The list above is a guideline to be kept in mind while browsing for suitable data sets and it will be extended in week three. Also, don’t worry if some of the terms above are unclear, getting a good understanding on the used terminology is part of these first few weeks 2.2 The NCBI Gene Expression Omnibus The easiest method of exploring and finding interesting datasets is by simply using your web-browser to access a data-repository. You could also download a few gigabytes of SQL database and query that if you’d like, but I would advice against that. One such repository is the NCBI Gene Expression Omnibus (GEO). This repository is primarily used to store microarray datasets and describe those experiments, linking to raw data, processed data and an accompanying publication. There are however many more sources of data browsable in the GEO, and for this project we will limit our searches to “Expression profiling by high throughput sequencing” for which almost 17.000 experiments are listed. The GEO Summary page shows all available data sources, click on the right Expression profiling link to get to the full table of relevant experiments. Besides finding an experiment that you are interested in, there are some further requirements that you need to account for when browsing: The experiment must be published and the publication must be fully available, free or otherwise through the Hanze University library. For each sample group, a minimum of three replicates must be present. If one or more groups have less then three replicates, that group cannot be used (the rest of the data might still be usable). The available data must contain at least the count data. Data is offered in multiple formats, always including the RAW data (reads), but also in further processed data such as FPKM, RPKM and TPM (see this interesting blog-post discussing the use of these data). The tools and R analysis libraries that we will use for the downstream analysis rely on unnormalized or unprocessed data which is the count data. These counts simply represent the number of reads mapped to a transcript. For manual analysis, these need to be normalized across samples (which has been done to get the FPKM, RPKM or TPM values) but will be done by the analysis software using the count data (unbiased). Once you’ve found an experiment, write a mail to m.kempenaar@pl.hanze.nl where you link your experiment. Once you receive a go-ahead you can start by reading the accompanying article and writing a project proposal. 2.3 Creating a Project Proposal In week two you are asked to present a short project proposal where you: Briefly explain the subject, methods, results and conclusion of the chosen project, describe the experimental design of the chosen experiment; how many samples and in how many groups are they divided (i.e. how many replicates per group)? Show what your plan is with the dataset, given the choices listed in the section about Finding Public Data. Note that whatever choice you make, the first steps will most likely be the same leading to finding the Differentially Expressed Genes (DEGs). It is very difficult to plan your complete project based on what you read in the article and it is not expected to get a planning for the full remaining 6 weeks. You can however specify a bit more given what you’ve read in the article, such as trying to reproduce a certain figure or repeat a certain analysis step. Summarize your project proposal on max. 1 A4 and, combined with the slides, are the final deliverables for this chapter. References "],
["chapter-3.html", " Exploratory Data Analysis 3.1 Exploring the Available Project Data 3.2 Loading data into R 3.3 Visualizing using summary, boxplot, scatterplot &amp; MA-plot 3.4 Visualizing using heatmap, MDS &amp; PCA", " Exploratory Data Analysis We will be performing some exploratory data analysis with the goal of getting to grips with your chosen data set to properly identify a strategy for the actual analysis steps. During this exploration we will also keep an eye on the quality of the data. Even though the downloadable data is ‘processed’, there might be samples present that deviate too much from the other group of samples (a so called outlier). Creating basic visualizations of the data will give the necessary insight before we continue. We first start by downloading and loading in the actual count data. 3.1 Exploring the Available Project Data Whether you’ve found a dataset through the SRA or GEO, we want to get the data into R to start working with it. For now, we will only download the count data which is most likely stored in a TXT or XLS(X) file format: Data formats: TXT: simple text file containing a minimum of two columns (either tab or comma separated containing i.e. gene / transcript identifier and one of the above mentioned data types). However it can also contain up to 10 data columns either including more information regarding the gene/ transcript (i.e. gene ID, name, symbol, chromosome, start/ stop position, length, etc.) or more numerical columns (i.e. raw read count, normalized readcount, FPKM, etc.). XLS(X): Microsoft Excel file containing the same data columns as mentioned in the TXT files. Data types: Read Count (simple raw count of mapped reads to a certain gene or transcript). FPKM (Fragments Per Kilobase Million, F for Fragment, paired-end sequencing), RPKM (Reads Per Kilobase Million, R for Read, single-end sequencing), Note: if you want to use this, make sure that the raw data is actually single-end (should be stated in the article) TPM (Transcripts Per Kilobase Million, T for Transcript, technique independent), Layouts Either one or more files per sample with one of the above data types or One file containing the data types for all samples (with the samples as data columns in the file) Please watch the video and read the page found at the RNA-Seq blog regarding the meaning and calculation of the above mentioned expression data formats or a more technical document found at The farrago blog page. On GEO you can see what data might be availalbe in the Supplementary column, as shown below: (#fig:GEO_RNA-Seq)Finding an experiment on GEO with a TXT file as supplementary data This overview on GEO contains many links which are not direct links to the items for that dataset, but can be used as filter for browsing the results. If you want to actually download the data, click on the GSE identifier (first column) which brings you to an overview for this experiment containing a lot of information about the experiment (subject, research institute, publication, design, etc.) and links to each individual sample (GSM identifier). Following the link to a sample shows information on how this sample was retrieved with often many (lab) protocols used. Sometimes there is a segment regarding “Data Processing” that refers to techniques and software used for the full analysis and might contain something like: … Differential expression testing between sample groups was performed in Rstudio (v. 1.0.36) using DESeq2 (v.1.14.1) … Back on the experiment overview page you’ll see a (variable) number of links to data files belonging to this experiment, see . For now we are only interested in the count-data which is stored in the TXT-file (see the column File type/resource) called GSE97406_RAW.tar. This file contains all the data that we need, even though it is only 220Kb in size where the experiment started with about 5Gb of read-data for a small bacteria: As a bioinformatician we love to compress all the files so for this particular example, we download the tar-archive file, extract it to find a folder with another archive file for each sample. After extracting these files we end up with 12 TXT-files with just two columns; a gene identifier and a semi-raw count value (~4500 rows of data per sample): Table 3.1: Contents of a TXT-file for sample GSM2563998 Gene ID Count Value aaaD 0 aaaE 0 aaeA 3 aaeB 3 aaeR 50 aaeX 0 aas 118 aat 43 abgA 6 abgB 21 abgR 56 abgT 0 abrB 11 accA 453 accB 2492 accC 1197 Other experiments combine their samples in a single file where each column represents a sample. If you do get an experiment with one file per sample, it’s a small task to combine these (just a few lines of R-code). 3.2 Loading data into R R works best with data in simple text formats. If your project data is offered as an Excel file it is therefore advised to open it in Excel (or OpenOffice Calc) and save/ export the file as a tab-separated text file, alternatively you can search for how to import an Excel file into R. Once you have one or more column based text files they can be read into R simply by using the read.table() function. Follow the following steps to read in the data and start the exploratory data analysis. The resulting document should be semi-treated as a lab journal where you log the process from loading the data to the final analysis steps. Open RStudio Create a new R Markdown document Give it a proper title and select the PDF format Give the document some structure; e.g. create a segment called Exploratory Data Analysis for this week’s work. Whenever you add code to your document make sure that it is both readable (keep the maximum line length &lt; 100 if possible) and there is sufficient documentation either by text around the code ‘chunks’ or by using comments in the code chunk. Read in the data file(s) For the remainder of the document, try to show either the contents, structure or - in this case - dimensions of relevant R objects Show the first five lines of the loaded data set. Including tables in a markdown document can be done using the pander function from the pander(Daróczi and Tsegelskyi 2017) R-library. Give the dimensions (with dim() and the structure (with str()) of the loaded data set. Check the output of the str function to see if all columns are of the expected R data type (e.g. values, factors, character, etc.) Examine the samples included in the experiment and create as many R character objects as needed to store the classification. For instance, if you have eight samples divided into case/ control columns you create an object called case in which you store the column indices of the respective columns in the data set and an object called control with the remaining four data column indices. These are for later use. For some datasets the order in which the samples are listed in your loaded dataset is different from the order that is shown on the GEO website. Most often, the names are different too or they are lacking any description and all you have are the GSM IDs. When creating these variables such as control that need to point to all control samples, you need to make sure that you have the right columns from your dataframe. Go to chapter 5; annotation if the order is unclear or you just have a large number of samples as there might be supporting data available that can help making sense of your sample layout. If you want to include including external images to your log and to better control properties such as height and width for individual images you can use the following code (requires the import of the png and grid libraries). Note: the code below is an example and you need to replace the ImageToInclude.png file reference to an actual image.): # Use the following to the code chunk header to control figure height and width: # {r, fig.height=5, fig.width=4, echo=FALSE} library(png) library(grid) img &lt;- readPNG(&quot;ImageToInclude.png&quot;) grid.raster(img) 3.3 Visualizing using summary, boxplot, scatterplot &amp; MA-plot This segment describes some of the basic steps and visualizations that can be performed during Exploratory Data Analysis. In this part we work with the unnormalized count data. In part this data is normalized before analyzing it further. As mentioned above, the focus of EDA is to get an overview of the data set and while this often requires visualizing the data, these figures do not need to be very pretty. Simple figures are perfectly fine in this stage. Try to create these figures for your own data (and keep them in your log). Also note that these steps are just a selection. Furthermore, make sure that for every visualization you make, add proper axis-labels containing the measurement units (important!). The following three plots are based on GSE74329 titled Transcriptome anlaysis of gastrointestinal tract of pre-weaned cavles. As you might be able to see, all values are log-transformed using the log2 because very often the numerical values have a very high range which will ‘hide’ the details on the plots. See the section about the Fold Change value in chapter 4 for further details. It is fine to use non-log-transformed (simply the raw-)data, otherwise use for instance boxplot(log2(dataset)) for plotting. Instead of using the basic R-plotting library (i.e. plot, boxplot, hist, etc.) you can also opt for using the (challenging) ggplot2 library that is also used for the following three figures. While constructing a ggplot2 plot feels like learning yet another language, there are many resources available online that you can follow. 3.3.1 Statistics Even the most basic statistics can give some insight into the data such as performing a 6-number-statistic on the data columns using the summary() function. Note: you can also use the pander function to pretty-print a summary from a markdown document. 3.3.2 Boxplots A visual representation of these values can be shown in a boxplot. Boxplots are very easy to create from an R data frame object by just passing in the data columns. The following boxplot shows the data for an experiment with a separate boxplot for each sample. This allows a quick overview for spotting irregularities (i.e. checking if the replicates within a sample-group show similar expression profiles). Of course if we consider the amount of data in this single plot, it can only hint at any problems, we need to look in much more detail when doing quality control. Creating a boxplot from a dataframe is easy, but as we can see with using the summary function; the data has a large range with the maximum and average values being very far apart. This will create a lot of outliers in the plot which will be interesting later on, but for the boxplot we can either: * hide them completely using the outliers = FALSE argument to boxplot() (do say so in the figure description!), or * perform a log2() transformation as you can see below. Boxplot comparing basic statistics for all genes across multiple samples 3.3.3 Scatterplots Scatterplots are good for spotting correlations between two sets of numerical data. Data that is similar will show up as a diagonal line in a scatter plot and the more they differ, the bigger the spread of points relative to the diagonal. Downside is that a scatterplot can only contain data from two columns and it might not always be useful to plot all samples from the condition group to all samples from the control group. Other possibilities include each sample against the average of its group (should be fairly diagonal), the averages of both groups in one scatterplot (this probably shows a fair amount of variation depending on the experiment). A scatterplot can be made simply using the plot() functions with the type argument set to p (points). To show the correlation more clearly you can add a linear regression line using abline(lm(y ~ x, col='red', lwd=2). If the regression line points more downwards from the diagonal, the values in sample x are higher and vice versa. A simple scatterplot showing the correlation between two samples or experiments (see the text above). This image shows the values log2 transformed which in most cases gives a clearer picture. 3.3.4 MA-plot An MA-plot is a scatterplot comparing the average expression value per gene (A) against the log-fold-change (M; a log2 value indicating expression change across samples which is taken as the log2(sample1 / sample2))). The points in this plot should be centered around the horizontal 0-value with an even spread both above and below the line. If there are (far) more points above or below the 0 on the y-axis, there is a bias in the data. An MA-plot can be very helpful (it has the same constraints mentioned with the scatterplot) but might not be easy/ possible to create as this is mostly done on normalized count data other than FPKM or RPKM. If you have raw or normalized count data in your data set you can search various online resources on how to create this plot. An MA-plot showing the average normalized count values (A, x-axis) vs. the log2 fold-change (M, y-axis). 3.4 Visualizing using heatmap, MDS &amp; PCA This section adds a few Exploratory Data Analysis techniques where we will measure and look at distances between samples. Measuring distances between two data objects (samples in our case) is a common task in cluster analysis to compare similarity (low distance indicates similar data). In this case we will calculate the distances between our samples and visualize them in a heatmap and using multidimensional scaling (MDS) techniques. 3.4.1 Normalization In the previous section we used the raw count data. You might have one or more samples that have different values (i.e. shifted) compared to other samples. While we need the raw count data to use R packages such as edgeR (Chen et al. 2018) and DeSEQ2 (Love, Anders, and Huber 2017), calculating sample distances should be done on some form of normalized data. This data can either be RPKM/FPKM/TPM/CPM or log-transformed (raw-)read counts. A proper method of transforming raw read count data is using the rlog method from the DESeq2 R Bioconductor library which is shown below. The following code examples shows how to use this library to normalize the count data to rlog-normalized data before we calculate a distance metric. The data used for this example is available at the GEO (Accession GSE80128, titled Evidence for two protein coding transcripts at the Igf2as locus consisting of 8 samples from mouse skeletal muscle tissue; 4 wild-type [WT] and 4 \\(\\Delta\\)DMR1-U2 [KO] variants). gse80128 &lt;- read.table(&#39;./data/GSE80128_P035_DESeq_KO-WT_pathway.txt&#39;, sep = &#39;\\t&#39;, header = TRUE) # Only use the actual count data as this file contains a lot more descriptive columns counts &lt;- as.matrix(gse80128[, 2:9]) rownames(counts) &lt;- gse80128$geneID Table 1; Raw count data for GSE80128 KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 Igf2as 121 104 100 122 1113 1868 1490 1282 St8sia5 77 25 52 65 147 206 199 222 Apln 222 85 353 224 638 1133 993 802 Prnd 400 79 527 208 1069 1600 1431 1306 Camk2b 1573 1066 1457 1457 2447 3220 3630 3435 Slc2a5 17 22 33 24 61 127 122 94 If you look at Table 1 you immediately see a huge difference between the two groups for each of the genes. The first gene (Igf2as) shows &gt; 10 more expression in the wild-type group compared to the KO group, but it is unknown if this is only caused by the experiment itself by just looking at these numbers. One very simple and quick inspection is looking at the total number of mapped reads per sample as the sequencing-depth might vary across samples. Table 2; Mapped reads per sample (millions) KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 19 18.24 16.78 20.06 16.08 25.96 26.91 24.24 The numbers shown in Table 2 immediately clearify that it is not just the experimental condition that might have caused this large difference between sample counts but the sequencing depth shows a substantial difference too. One more reason that we will normalize the data with DESeq2’s rlog method. Check the Packages tab in Rstudio to see if you have the DESeq2 package installed and load it with the library command. If you are missing this package, install and load the library using the code below: # Install -only if- the library is not available source(&#39;http://bioconductor.org/biocLite.R&#39;) biocLite(&#39;DESeq2&#39;) # Load the library library(&#39;DESeq2&#39;) To use the rlog function from the DESeq2 library we must contruct a DESeqDataSet object consisting of the count data combined with sample annotation. Since we only want to use it (for now) for performing a log-transformation we use the most basic form with the sample names as annotation (the colData argument): # Put the sample names in a separate vector coldata &lt;- colnames(counts) # DESeq2 will now construct a SummarizedExperiment object and combine this # into a &#39;DESeqDataSet&#39; object. The &#39;design&#39; argument usually indicates the # experimental design using the sample names as a &#39;factor&#39;, for now we use just &#39;~ 1&#39; (ddsMat &lt;- DESeqDataSetFromMatrix(countData = counts, colData = data.frame(samples=coldata), design = ~ 1)) ## class: DESeqDataSet ## dim: 23336 8 ## metadata(1): version ## assays(1): counts ## rownames(23336): Igf2as St8sia5 ... Zscan4f Zscan5b ## rowData names(0): ## colnames(8): KO1A KO1B ... WT2 WT3 ## colData names(1): samples We now have a proper DESeqDataSet object as you can see above, containing 23336 rows and 8 columns (genes and samples) with the gene symbols as rownames. Usually this object would hold more data, but as this is only a requirement to perform the rlog transformation it is good enough for now. Next step is performing this transformation (results in a Large DESeqTransform object) and retrieving the actual data from this with the assay function as this object too contains a lot of meta-data. The table below shows the updated values which are now comparable across genes whereas the raw count data was harder to compare. # Perform normalization rld.dds &lt;- rlog(ddsMat) # &#39;Extract&#39; normalized values rld &lt;- assay(rld.dds) Table 3; RLog transformed count data KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 Igf2as 8.04 7.999 8.042 8.024 9.927 9.931 9.687 9.663 St8sia5 6.624 6.282 6.532 6.535 7.123 7.016 6.977 7.134 Apln 8.34 7.879 8.78 8.323 9.349 9.404 9.261 9.184 Prnd 8.986 8.138 9.325 8.531 10.03 9.924 9.796 9.829 Camk2b 10.82 10.54 10.89 10.73 11.44 11.2 11.29 11.36 Slc2a5 5.494 5.552 5.682 5.548 5.942 6.069 6.034 5.961 3.4.2 Distance Calculation We now have normalized data that we can use for distance calculation. We need to transpose the matrix or data frame of values using t(), because the dist function expects the different samples as rows and the genes as columns. Note that the output matrix is symmetric. # Calculate basic distance metric (using euclidean distance, see &#39;?dist&#39;) sampledists &lt;- dist( t( rld )) Table 4; Sample distances (Euclidean method) KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 KO1A 0 21.48 13.96 16.92 20.14 15.87 15.47 16.94 KO1B 21.48 0 22.46 19.17 28.1 27.65 25.16 24.85 KO2 13.96 22.46 0 17.7 17.15 13.82 14.93 15.11 KO3 16.92 19.17 17.7 0 19.66 21.38 19.24 19.02 WT1A 20.14 28.1 17.15 19.66 0 13.83 14.62 15.82 WT1B 15.87 27.65 13.82 21.38 13.83 0 12.08 14.61 WT2 15.47 25.16 14.93 19.24 14.62 12.08 0 13.24 WT3 16.94 24.85 15.11 19.02 15.82 14.61 13.24 0 3.4.3 Sample Distances using a Heatmap If you have both (raw-)count data and an other normalized format (TPM, RPKM, etc.), follow the above procedure for your count data and create a heatmap for both formats to see if this makes any difference. The reason for this is that while the RNA-Seq method exists for over 10 years, there are still ongoing discussions on the subject of data processing, especially regarding subjects like which data format to use for which data analysis. The following code block creates a heatmap using the pheatmap library which offers on of the many available heatmap functions. The resulting heatmap shows an interesting comparison across all samples. Where all of the wild-type samples cluster nicely together, one of the knockout samples deviates from all other samples and might be classified as an outlier in this case. Since we have 4 samples per category, we retain statistical power if we eventually were to remove this sample. # We use the &#39;pheatmap&#39; library (install with install.packages(&#39;pheatmap&#39;)) library(pheatmap) # Convert the &#39;dist&#39; object into a matrix for creating a heatmap sampleDistMatrix &lt;- as.matrix(sampledists) # Give the matrix row and column names rownames(sampleDistMatrix) &lt;- coldata colnames(sampleDistMatrix) &lt;- coldata # The annotation is an extra layer that will be plotted above the heatmap columns # indicating the cell type annotation &lt;- data.frame(Variant = factor(c(1, 1, 1, 1, 2, 2, 2, 2), labels = c(&#39;KO&#39;, &#39;WT&#39;))) # Set the rownames of the annotation dataframe to the sample names (required) rownames(annotation) &lt;- coldata pheatmap(sampleDistMatrix, show_colnames = FALSE, annotation = annotation, clustering_distance_rows = sampledists, clustering_distance_cols = sampledists) 3.4.4 Sample Distances using MDS The following code example shows how to perform Multidimensional scaling that displays the previously calculated distances in a 2D-plot. With an experiment like this with two groups of samples, we hope to see two clusters forming separating these two groups, however as we’ve seen in the heatmap, sample KO1B showed a large deviation which we will also note using MDS. All figures below are plotted using ggplot2, a more advanced method of plotting in R. While these plots are preferred over base-R plotting, it is always sufficient to use just that as it can be very challenging to alter the example code shown in this section. The data objects plotted are always shown and they usually contain simple X- and Y-coordinates. # Perform MDS using the &#39;cmdscale&#39; function. The resulting data can simply be # plotted using basic R plotting, here we will use &#39;ggplot2&#39; mdsData &lt;- data.frame(cmdscale(sampleDistMatrix)) pander(mdsData, caption = &#39;MDS coordinates used for plotting the distances&#39;) MDS coordinates used for plotting the distances X1 X2 KO1A -1.997 6.976 KO1B -17.98 0.3748 KO2 0.9181 4.117 KO3 -7.214 -7.154 WT1A 8.118 -7.903 WT1B 8.517 3.032 WT2 5.407 1.073 WT3 4.23 -0.5169 # Separate the annotation factor (as the variable name is used as label) (Variant &lt;- annotation$Variant) ## [1] KO KO KO KO WT WT WT WT ## Levels: KO WT library(ggplot2) ggplot(mdsData, aes(X1, X2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Euclidean Distance&#39;) As a demonstration of a different distance metric (instead of the default euclidean used in the dist function) the following code shows how to calculate a maybe more fitting distance called the Poisson Distance. This library was specifically designed to handle read count data. Note that again, the code below is only usable for count data as this function requires: &gt; A n-by-p data matrix with observations on the rows, and p features on the columns. The (i,j) element of x is the number of reads in observation i that mapped to feature (e.g. gene or exon) j`. library(&#39;PoiClaClu&#39;) # Use the raw (not r-log transformed) counts dds &lt;- assay(ddsMat) poisd &lt;- PoissonDistance( t(dds) ) # Extract the matrix with distances samplePoisDistMatrix &lt;- as.matrix(poisd$dd) # Calculate the MDS and get the X- and Y-coordinates (mdsPoisData &lt;- data.frame(cmdscale(samplePoisDistMatrix))) ## X1 X2 ## 1 -1703.5304 4797.3631 ## 2 -13785.4488 554.1001 ## 3 -247.7059 1076.6632 ## 4 -5286.3101 -4215.8790 ## 5 5481.0361 -4477.7972 ## 6 7416.4224 2436.2925 ## 7 4517.6335 969.1246 ## 8 3607.9032 -1139.8673 # Create the plot using ggplot ggplot(mdsPoisData, aes(X1, X2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Poisson Distance&#39;) Comparing both figures shows similar results, we clearly see a separation of both sample sets, however within the knockout (KO) group there is a large spread and if we were to apply a proper clustering technique, we would see clusters forming containing mixed samples. Once we have a set of genes identified as being differentially expressed we can repeat this step with the expectation of a more clear clustering. An alternative method of showing sample relations which is often preferred over MDS (but a bit harder to perform with pre-normalized data formats) is Principal Component Analysis (PCA). The code below shows PCA on our DESeqTransform object rld.dds. # Calculate PCA data data &lt;- plotPCA(rld.dds, intgroup = c(&#39;samples&#39;), returnData = TRUE) # Calculate the percentage of each principal component # Only used in the axis-labels percentVar &lt;- round(100 * attr(data, &#39;percentVar&#39;)) # Separate the annotation factor (as the variable name is used as label) Variant &lt;- annotation$Variant # Create the plot using ggplot ggplot(data, aes(PC1, PC2, color=Variant, label = coldata)) + geom_text(size=4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) Or on FPKM/RPKM data (only perform if you want to compare normalization techniques and have FPKM data. Output is not shown): # PCA done with the &#39;prcomp&#39; function in base-R. Replace &#39;rld&#39; with a matrix # containing the normalized data (genes as rows, samples as columns) pca.data &lt;- prcomp( t(rld) ) # Extract coordinates for PC1 and PC2 components &lt;- data.frame(mds.labels, pca.data$x[, 1:2]) pander(components) # Plot using ggplot2 ggplot(data = components, aes(PC1, PC2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) References "],
["chapter-4.html", " Discovering Differentialy Expressed Genes (DEGs) 4.1 Pre-processing 4.2 The Fold Change Value 4.3 Using Manual Methods (t-test &amp; ANOVA) 4.4 Multiple Testing Correction 4.5 Using Bioconductor", " Discovering Differentialy Expressed Genes (DEGs) The first ‘real’ analysis step we will do is finding genes that show a difference in expression between sample groups; the differentially expressed genes. The concept might sound rather simple; calculate the ratios for all genes between samples to determine the fold-change (FC) denoting the factor of change in expression between groups. Then, filter out only those genes that actually show a difference. While this does give a list of genes showing different behaviour across samples, we need to focus on genes that do not only show a difference, but are also statistically significant! To determine whether or not a gene can be classified as a significant DEG we are going to use - at least - two techniques: We will start with manually performing statistical test(s) and compare these results from those given by using one or more Bioconductor libraries (mostly edgeR and DeSEQ2) 4.1 Pre-processing Given the results of the exploratory data analysis performed in chapter 3, you might have concluded that there are one or more samples that show (very) deviating expression patterns compared to samples from the same group. If you have more then enough (&gt; 3) samples in a group, you might opt to remove a sample to reduce the noise, as the statistical tests are very sensitive to this. Since we are performing all analysis steps programatically it is also very easy to test for DEGs with and without the sample(s) in question and see if the removal results in lower p-values (= higher significance). If this is the case you can continue on without those sample(s). As always, be sure to properly document this, including the reason why you chose to remove them. Another step we need to take - and this might be guided by your article - is to filter out (partially) inactive genes. Most datasets available contains a lot of 0-measurements, transcripts where no reads have mapped. In an experiment with two groups, three replicates each, if three out of those six samples have 0-reads mapped, it is often adviced to remove the gene completely. But this can be very subjective to the experiment, it might be expected (thus important) when comparing different tissues or knockout experiments. Also, genes with a (very) low read count can give a very high (artificial) FC value (see the lefthand side of an MA-plot). Comparing two samples where one has a value of 2 and the other 11, this reads as an up-regulated gene by a factor of 5.5 while it might actually just be noise! Assignment: Search through your article for any advice on how to filter out zero values or low count genes. If there is nothing stated on this subject, think of your own tactic (or search the literature/ online!). It is perfectly fine to discuss with your peers. describe what you will be doing for this aspect; if you do not filter your data, clearly explain why not (most likely because the article stated a proper reason) Perform the filtering on your dataset. For this you will most likely need to use one of the apply functions, combined with maybe the which, all and any functions. Manually verify that the rows removed were correctly filtered. Properly document how many genes have been filtered out! 4.2 The Fold Change Value The FC is usually given as the calculated log2 of the case/control ratio. For example, gene A has an average expression of 30 mapped reads in the control group and 88 reads in the experiment group, the ratio case/control is 2.93. Values &gt; 1 indicate increased expression in the experiment in relation to the control and values between 0 and 1 indicate lower expression. The log2(2.93) is 1.55. If the counts were reversed, the ratio would have been 30/88, which is 0.34. The previously calculated value of 2.93 means a 3-fold up regulation while the 0.34 value means 3-fold down regulation but you can see the range of numbers is very different. Comparing log2 values this would be 1.55 and -1.55 which compares much better. While it is very easy to calculate the FC for all genes at once, a simple FC value doesn’t mean much, yet! We still need to use the power that lie within the replicates we have for each sample group. Using these replicates, we want to determine if the observed FC is not just biological noise or a sequencing error. Assignment: Create a histogram of your log2-FC values for all genes. Apply the following steps to do this: average the replicates for each group and add this as a new column to your data set, calculate the ratios; simply divide one group (averaged-column) by another, usually experiment/control, perform a log2-transformation (now you have the log2-FC values), plot the data using the hist(logFC_column, breaks=60) function (change the breaks argument if needed), Add two vertical lines at -1 and 1 (using abline(v=...)) to indicate some significance (2-fold change). If a calculated FC shows a large change in expression between groups this means nothing if the variation within a group is very high. For this reason we use some form of statistical test that checks both the variation in each group and the difference between groups of samples. In the simplest form this usually comes down to using a t-test: “It can be used to determine if two sets of data are significantly different from each other” It does however depend on your experimental setup when it comes down to deciding the proper statistical test to perform and for that it is best to look at your experiment article to see what the authors used for method. The output of finding DEG’s always includes - but is not limited to - a list of p-values, usually one p-value per gene. This value indicates whether that gene is a statistically-significant differentially expressed gene (SDEG) and to find these genes of interest all we need to do is get all genes with a p-value below our threshold (i.e. 0.05). Because we will get at least two p-values per gene (one found using manually statistical testing and one through the use of a library) we can make multiple selections, i.e. compare methods, find genes that have a low p-value in both methods and visualize these results in a venn-diagram (see section visualization). 4.3 Using Manual Methods (t-test &amp; ANOVA) Performing the statistical tests yourself consists of performing either a t-test or an ANOVA based analysis. Both of these methods have multiple forms and the one to choose fully depends on what question you’d like to ask. Again, refer to your experimental setup and any hints found in the article or formulate your own question and base the decision on that. Manually performing a t-test or ANOVA on gene expression opposed to using one of the specialized libraries can have advantages since every gene is processed individually, while edgeR and DESeq2 look at all genes and this might ‘smooth’ the results which is not always wanted. We will find out later if and how this affects your data set. If your data only consists of the read counts there is an extra normalisation step to perform (only for the manual methods explained in this section, keep the count data stored as well!). One generally accepted method of normalizing count data is to calculate the fragments per million mapped fragments (FPM) value and then transform this with log2. Opposed to FPKM and RPKM this does not include the gene-length in its calculation (which you most likely don’t have) but as said before we apply the test per gene and do not need to compare multiple genes. If your dataset also includes FPKM or RPKM (pre-normalized), you are allowed to use this data too. Always clearly document if you did so! Once you performed a manual statistical text, be sure to read the section on Multiple Testing Correction further on in this chapter. # Perform FPM normalization using DESeq2 &#39;fpm&#39; and perform log2 transformation # The &#39;ddsMat&#39; object is the &#39;DESeqDataSet&#39; created in chapter 4. # NOTE: only for count data counts.fpm &lt;- log2( fpm(ddsMat, robust = TRUE) ) Next is selecting the test to perform, the following links show diagrams that can be used once the experimental design is known: What test to use links: Institute for Digital Research and Education Biochemia - image at the bottom of the article PracticallyScience 4.3.1 Students T-test In order to test if one or more genes are significantly differentially expressed between two conditions one can perform a t-test. The t-test will test the null hypothesis that there is no difference between the mean of the two populations. Usually, if the p-value is below the significance threshold chosen (also called alpha-value, usually set at \\(\\alpha\\) 0.05) you reject the null hypothesis and conclude that there is a significant difference between the means. Note that it is necessary to perform a single t-test for every gene (row) of your data set, including all replicates for the involved group. This means that you need to either use one of the apply() set of functions or find a different method (there is one specifically for this problem) to make sure that you test each gene separately. From the output of the t-test you only need to keep the calculated p-value; one value per gene (row) in your data set. In R, you can append this as a column to your dataframe/ matrix containing the normalized count values. ## Loading data set with count data (71 samples, 21000+ genes) gse74329 &lt;- read.table(&#39;data/gse74329.txt&#39;, sep = &#39;\\t&#39;, header = TRUE, row.names = 1) case &lt;- 14:18 control &lt;- 1:3 ## Executing single t-test for single gene (row 1) test &lt;- t.test(gse74329[5000, case], gse74329[5000, control]) Data for t-test, RU_42 (case) vs RU_0 (control) RU_0_1 RU_0_2 RU_0_3 RU_42_1 RU_42_2 RU_42_3 RU_42_4 RU_42_5 ENSBTAG00000007712 27 32 37 18 21 21 18 18 The following table shows the numeric values from performing a t-test. Here we see that this gene is identified as a DEG because its \\(\\alpha\\) &lt; 0.05. Since the t-test compares the mean of two groups, the difference between these two values (19.2 for the ‘case’ group and 32 for the ‘control’ group) are deemed significant. Do note that ‘noise’ within a group of replicates has a lot of effect on the p-value. Welch Two Sample t-test: gse74329[5000, case] and gse74329[5000, control] Test statistic df P value Alternative hypothesis mean of x mean of y -4.297 2.263 0.04007 * two.sided 19.2 32 Again, we are only interested in the p-value for each gene, store this in either the original data frame or a new one where it is coupled to the gene identifier (name, symbol, etc.). t.test tutorials: Quick-R @ statmethods Whitehead Institute: based on microarray analysis, but it’s still expression data and the basics are the same. 4.3.2 Analysis of Variance (ANOVA) If you have decided that you need to perform an ANOVA analysis the experimental design as well as the question to ask become much more important. ANOVA tutorials Quick-R @ statmethods R-bloggers 4.4 Multiple Testing Correction Read the text about Multiple comparisons at the biostathandbook and the help of the p.adjust R function. Your article is a possible source to see which correction method they applied and this is not mentioned there it is probably best to use the fdr method (also called the Benjamini Hochberg method). An expected side-effect of correcting for multiple-testing is the lower number of genes with a resulting p-value &lt; 0.05 (caused by hopefuly removing false-positives). In some cases though, you end up with 0 genes that have a low p-value and thus you have no DEGs. 4.5 Using Bioconductor This section demonstrates the use of two packages to perform DEG-analysis on count data and one library for processed FPKM/RPKM data. There are many packages available on Bioconductor for similar analysis, such as DSS, EBSeq and BaySeq but here we will focus on edgeR and DESeq2 for processing count-based data and briefly take a look at limma for processing FPKM/RPKM data. Both edgeR and DESeq2 apply their own normalization methods (described in the sections below) therefore they only work on the count data. You can choose one of these two packages to use, or use both since it could increase statistical power and they are not very hard to use once you understand how to model the data as we’ll show next. 4.5.1 The Design (matrix) For all of these packages you need to properly specify how your samples are grouped. We have seen examples of this using an R factor object with the heatmap, MDS and PCA visualizations to tell which groups of samples we have and to which group each sample belongs. Reading the documentation for the below packages shows that this is an important part of performing DE-analysis. For example, the following code is shown in de edgeR documentation on page 8 where two sample groups are defined (numbered 1 and 2), placed in a factor object and used as input in the model.matrix function. group &lt;- factor(c(1,1,2,2), labels = c(&quot;Control&quot;, &quot;Case&quot;)) (design &lt;- model.matrix( ~ group)) ## (Intercept) groupCase ## 1 1 0 ## 2 1 0 ## 3 1 1 ## 4 1 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$group ## [1] &quot;contr.treatment&quot; As mentioned before we need to think about the question we want to aks; which difference do we want to know? With two sample groups as used here the question is rather easy; ‘which genes show an effect between case/ control samples?’. With more then two sample groups however the question becomes more difficult. If we have an experiment comparing influence of three kinds of drugs (thus three groups) combined with effect over time, do we then want to focus on the influence of the drugs or the time? Both are valid questions but they define the way of how to create the design matrix. Documentation on this subject is plenty, however it often contains overwhelming information. This page contains some valuable details (you can safely start reading at Choice of design), including the following text which is based on the same example used in the edgeR documentation: “For the examples we cover here, we use linear models to make comparisons between different groups. Hence, the design matrices that we ultimately work with will have at least two columns: an intercept column, which consists of a column of 1’s, and a second column, which specifies which samples are in a second group. In this case, two coefficients are fit in the linear model: the intercept, which represents the population average of the first group, and a second coefficient, which represents the difference between the population averages of the second group and the first group. The latter is typically the coefficient we are interested in when we are performing statistical tests: we want to know if their is a difference between the two groups.” If you have more then two sample groups and you want to change the question (i.e. test the influence of a different group), read the section about Releveling the factor. 4.5.2 DESeq2 We have used the DESeq2 library before and for DEG analysis we could re-use the DESeqDataSet object but it is adviced to create a new object with more data added, e.g. the sample annotation we retrieved from the GEO and the proper design formula instead of the ~ 1 we used before. There is no need to normalize the data using the previously used rlog function because the DESeq2 library will normalize the count data for you as follows: “DESeq computes a scaling factor for a given sample by computing the median of the ratio, for each gene, of its read count over its geometric mean across all samples. It then uses the assumption that most genes are not DE and uses this median of ratios to obtain the scaling factor associated with this sample.” For our example data with just two groups we use the design ~ groups where groups is a simple factor with two levels (KO and WT). One note of importance though is that the first level in the factor is taken as the reference, or for expression analysis, the control group. In the example factor the KO level is the first which is the experiment group. Therefore we need to relevel the data (check the help and documentation if this is unclear): (groups &lt;- factor(c(1, 1, 1, 2, 2, 2, 2), labels = c(&#39;KO&#39;, &#39;WT&#39;))) ## [1] KO KO KO WT WT WT WT ## Levels: KO WT (groups &lt;- relevel(groups, &#39;WT&#39;)) ## [1] KO KO KO WT WT WT WT ## Levels: WT KO Once you have a proper DESeqDataSet all you need to do is run the DESeq function on this object. Then, using the results function you can extract the DEGs as a DESeqResults object. Applying the summary function on this object shows the number of up and down regulated DEGs: ## ## out of 20098 with nonzero total read count ## adjusted p-value &lt; 0.05 ## LFC &gt; 0 (up) : 234, 1.2% ## LFC &lt; 0 (down) : 289, 1.4% ## outliers [1] : 0, 0% ## low counts [2] : 5023, 25% ## (mean count &lt; 12) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results The output of the results function contains the following columns for each gene: Column Type Description baseMean intermediate mean of normalized counts for all samples log2FoldChange results log2 fold change (MLE): groups KO vs WT lfcSE results standard error: groups KO vs WT stat results Wald statistic: groups KO vs WT pvalue results Wald test p-value: groups KO vs WT padj results BH adjusted p-values How you call the results function depends heavily on your experiment. As you can see from the output of the summary function, there are no details given about which comparison is shown (and also, a p-value of 0.1 is used instead of 0.05). Depending on the design used to create the DESeqDataSet with, one or more comparisons can be made (applying the DESeq function calculates all and you filter with the results function). Read the help for the results function carefully; especially regarding the contrast argument where you define the comparison to retrieve. The following code is used for our example experiment: res &lt;- results(dds, contrast = c(&quot;groups&quot;, &quot;KO&quot;, &quot;WT&quot;), alpha = 0.05) Links Analyzing RNA-seq data with DESeq2 A very comprehensive guide to analyzing RNA-Seq data using DESeq2 (part of this document has been used in this material too!). It is adviced to read the first few sections of this guide and take a good look at the index of the document because there are many interesting sections that might be of help later. Publication, an accompanying article showing differences in performance compared to other methods and packages. 4.5.3 edgeR One of the most mature libraries for RNA-Seq data analysis is the edgeR library available on Bioconductor. There is a very complete (sometimes a bit complex) manual available of which you need to read Chapter 2 with a focus on 2.1 to 2.7, 2.9 and - if you have a more complex design - 2.10. Section 1.4 (Quickstart) shows a code example on the steps needed to do DEG analysis using count data using the two glm methods (quasi-likelihood F-tests and likelihood ratio tests). All the steps shown there are identical for the non-glm method up to calculating the fit object which can be replaced by performing the exactTest function as shown in sectino 2.9.2. The edgeR library will normalize the count data for you as follows: “The trimmed means of M values (TMM) from Robinson and Oshlack, which is implemented in edgeR, computes a scaling factor between two experiments by using the weighted average of the subset of genes after excluding genes that exhibit high average read counts and genes that have large differences in expression.” Running edgeR requires the raw count data together with the grouping-factor packaged in a DGEList object (with the DGEList() function). Furthermore, a proper model.matrix object (see the section on design) is needed as input for the estimateDisp function. The exact steps to take (there are more variations then with DeSEQ) must be searched in the documentation linked above. Gene annotation has been added to the DGEList object that is used to run edgeR with the genes parameter. This data has been taken from the AnnotationDbi package as shown in chapter 5. Once the analysis is done you can retrieve the actual results with the topTags function: Most significant genes given by edgeR logFC logCPM PValue FDR Igf2as -3.389 5.284 6.222e-108 9.969e-104 Angptl4 1.063 5.884 2.846e-24 2.28e-20 St8sia5 -1.318 2.696 8.637e-15 3.04e-11 Mrgprb1 0.8404 4.129 8.826e-15 3.04e-11 Acot2 1.113 5.531 9.487e-15 3.04e-11 Camk2b -0.823 6.836 2.576e-14 6.879e-11 Frzb -1.258 4.477 4.54e-14 1.039e-10 Cd36 0.7175 9.023 1.288e-13 2.58e-10 Slc2a5 -1.725 1.688 4.355e-13 7.753e-10 Chi3l3 2.104 0.3187 5.666e-13 9.078e-10 The package also contains a few plotting methods that you can use at intermediate steps during the analysis. For instance, after calculating the normalization factors (calcNormFactors), you can perform multi-dimensional scaling with the plotMDS function: plotMDS(y) Or the dispersion after running the estimateDisp function with the plotBCV function: plotBCV(y) Or the log-fold changes for all genes, once we have the output of the exactTest function (output et is an DGEExact object) with the plotSmear function. The abline shows a log-FC threshold: deGenes &lt;- decideTestsDGE(et, p=0.001) deGenes &lt;- rownames(et)[as.logical(deGenes)] plotSmear(et, de.tags=deGenes) abline(h=c(-1, 1), col=2) Links Differential Expression Analysis using edgeR tutorial Another tutorial hosted on GitHub "]
]
