[
["index.html", "Analysis of Gene Expression Preface Capstone Project", " Analysis of Gene Expression Marcel Kempenaar 2018-04-23 Preface Capstone Project This project aims to teach the whole process of analysing a dataset containing gene expression data measured with the RNA-sequencing technique. The tools we will use to perform the analysis will be the statistical programming language R with its many libraries and the software package RStudio to interact with R. Rstudio is a so called Integrated Development Environment (IDE) which makes programming in R much easier. This course is organized in a number of chapters, each with the goal of understanding and performing one of the analysis steps to gain knowledge about both the biology of gene activity and the bioinformatics approach to answering a biological question given a data set. Demonstrating the understanding of these concepts is done through completing a number of small assignments and constant reflecting on results from the various analysis steps. "],
["chapter-1.html", " Introduction 1.1 Capstone Project - Analysis of Gene Expression 1.2 Project Deliverables 1.3 Project Schedule 1.4 Grading 1.5 Project Proposal 1.6 Lab Journal 1.7 Article", " Introduction 1.1 Capstone Project - Analysis of Gene Expression The use of RNA-Sequencing techniques for measuring gene expression is relatively new and replaces microarrays, though in some cases microarrays are still used. Gene expression data gives valuable insights into the workings of cells in certain conditions. Especially when comparing for instance healthy and diseased samples it can become clear which genes are causal or under influence of a specific condition. Finding the genes of interest (genes showing differing expression accross conditions, called the Differentially Expressed Genes (DEGs)) is the goal of this project. While there is no golden standard for analyzing RNA-sequencing datasets as there are many tools (all manufacturers of sequencing equiptment also deliver software packages) we will use R combined with proven libraries for processing, visualizing and analyzing publically available datasets. While in some cases you are allowed to use the actual raw data that is available, it is highly recommended to use the pre-processed data which often is a table with a count value for each gene. This count is the number of reads that was mapped to that gene which corresponds to the relative number of transcripts (mRNA sequences) of that gene present in the cell at the time of sampling. 1.2 Project Deliverables The end products of this course consist of two deliverables; a PDF output file from an RMarkdown ‘lab journal’ where you have logged all steps performed to get to the end result and a final report in the form of a short article. This chapter briefly describes the requirements and contents of these products and ends with instructions on how to use an RMarkdown template for writing the article. 1.3 Project Schedule The aim is to keep to the below schedule during this course. Use the first two weeks to see if you need to focus more on one of the points below (depending on your dataset) and discuss changes to the planning with your teacher. Find a public experiment of interest [week one] Using online resources (sections 2.1, 2.2) Data Gathering and Literature Research [week three] Make a final project choice Retrieve the accompanying publication Write and present a short project proposal (section 2.3) Starting with Exploratory Data Analysis (chapter 3) Data Analysis [weeks four and five] Finalizing Exploratory Data Analysis Discovering Differentially Expressed Genes (chapter 4) Data Annotation (optional; appendix a2) Techniques used: R with bioconductor, the EdgeR and/ or DESeq2 packages, RMarkdown Result Analysis [week six] Analyzing and Visualizing your results (chapter 5) Techniques used: clustering, pathway analysis, gene-enrichment analysis Finalizing analysis and start writing the final report (article) [weeks 7 and 8] 1.4 Grading The final grade consists of a weighted average of the work done for weeks 1 &amp; 2 (resulting in the project proposal) (15%), the lab journal containing all performed steps, their code and outputs (50%) and the article report (25%) and finally your work attitude (10%). A grade &gt;= 5.5 will give a total of 8 EC. The rest of this chapter explains the expected contents for the three graded products (each of these elements is explained in greater detail in other chapters). 1.5 Project Proposal As you are free to choose from thousands of public data sets on a large variety of biological subjects, it is important to demonstrate a good understanding of the experiment, biology and the available data. This is demonstrated by presenting a short project proposal where you briefly explain the subject of the chosen research article, the experimental setup (how many samples were used, etc.) and what types of data are available for you to use. 1.6 Lab Journal As you know from previous projects and most likely from working in the laboratory, it is essential to keep a proper lab journal detailing every step you have done during the experiment. Here, the log is to be kept in an R markdown file, showing which steps have been taken in the analysis of the data set. This markdown should be knitted into a single PDF-file once the project is completed thus containing text detailling the steps and any decisions you’ve made, R-code (always visible!) and their resulting output/ images. As a general advice; do not wait with knitting this whole document until the project is done! 1.7 Article The final report is written in article form which is a bit different from a usual report, mainly in its size. The article-report has a maximum number of pages of 4 including all images and references (no appendices!). Contents for this article should be extracted from the lab journal combined with part introduction and part conclusion/ discussion. The sections below describe a template that is available for writing this report and example report(s) will be made available for inspiration. Refer to this chapter again once you start writing the report. 1.7.1 Installing the Article Template The templates are available in an R package and contains both RMarkdown and (another layer on top of the actual markup language, yes, another language…) files. RStudio can use templates for a number of documents, including article-templates. These templates can be installed from a package called rticles by running the install.packages function (note: might already be installed): install.packages(&quot;rticles&quot;, type = &quot;source&quot;) 1.7.2 Using the Template Now that you have the templates package (rticles) installed you can download and use the template project (ZIP-file) available from the course website. Download this file to your project folder, extract its contents and open the report-template.Rmd file contained within the folder in RStudio. Verify that everything is setup correctly by hitting the Knit button at the top, this should create a PDF version of the report. Note that - somehow - the resulting PDF file is named RJwrapper.pdf instead of the expected report-template.pdf. If everything checks out ok you can rename the file to your liking and start editing. This template is based on the R Journal Submission template that you can also find in RStudio in the New file -&gt; R Markdown -&gt; From Template menu. Articles published in the R-journal are based on this template which you can browse for inspiration at the R Journal Website. The available template shows an example of segments/ chapters and briefly describes what each section could or should contain. If you want to write your report in the Dutch language, you can create a new file from the template and change the segment names and content to Dutch. There will however still be some headings added by the template in English which is fine with me, but if you want to keep everything Dutch you can either edit the RJournal.sty file manually (not recommended) or start a new file by yourself and add some nice headings and page options. The top part of the template (between the dashes ---) contains some settings that you need to change such as title, authors and abstract. Compare for instance a newly created article from this template with the one offered from the project-repository website. "],
["chapter-2.html", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest 2.2 The NCBI Gene Expression Omnibus 2.3 Creating a Project Proposal", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest This chapter describes a method of finding public datasets of interest. A dataset in the context of this course refers to all data belonging to a certain gene-expression experiment, usually consisting of a number of sequencing-samples combined with meta-data describing the experiment. There are a number of very large databases online that offer access to thousands of - published - experiments and here we will focus on searching and downloading gene expression data from high-throughput (NGS) sequencing techniques (RNA-Seq). With thousands of freely available datasets it is possible to start performing research without the need of actually performing your own lab-experiment. For all common conditions, organisms and tissues you can download samples and compare them over multiple studies to find novel relations between genes and conditions or to verify experiments performed in your laboratory. For instance, the power of public datasets was demonstrated jointly by three of our alumni in an article called Calling genotypes from public RNA-sequencing data enables identification of genetic variants that affect gene-expression levels.(al. 2015). It’s method section begins with the sentence “We downloaded the raw reads for all available human RNA-seq datasets …” of which the amount of data and work will become clear later on. If you are interested in expression Quantitative Trait Loci (eQTLs) or Allele-Specific Expression (ASE) analysis, please read this very interesting paper. The following weeks you will be investigating one or more public RNA-sequencing datasets yourself and the rest of this document focuses on databases containing such public data sets and - especially - how to find a dataset or experiment you would like to use. But before we start diving into large and complex databases potentially containing thousands of experiments with millions of samples and terabytes of data, we need to get a rough idea on what we would like to do once we have found something of interest in order to know what we are looking for in the first place. While there is no definitive guide or protocol that can be followed for processing and analysing a gene-expression dataset, the following goals can be considered: Re-do (part of) the analysis described in the accompanying publication. As all datasets - except for the ones that were added very recently - are accompanied by a publication, you can gather a lot of information regarding experimental design and results from just this paper. Most often, the researchers already performed the most interesting research on this dataset and it is therefore a good exercise to try and reproduce their results. Challenges here are getting a proper understanding of the techniques used by the researchers (from the article) and translating these to your own analysis steps. Often though, not all details are clear from the article which requires your own interpretation of their results and performing analysis steps to arrive at the same conclusion. Alternatively you can come up with your own ideas and (biological) question(s) that you could try and answer given a dataset instead of redoing the published work. This is of course more challenging and not suitable for all datasets. You might notice that some publications only focus on a small set of genes instead of the whole transcriptome. For instance, when you perform an experiment on yeast where you want to measure the activity of genes involved in alcohol fermentation, researchers might only look at genes from the Glycolysis pathway. This leaves the other 6000+ yeast genes for you to explore and possibly come up with novel relations of gene expression and experimental setup. If the analysis approach explained in the paper is not focusing on a pre-selected list of genes, depending on the experiment you might come up with comparisons not done in the original research. For instance, in an experiment where the maternal age (at birth) is correlated to autism in their offspring, the paternal age is also known but not addressed in the publication. This could be a subject of further research (spoiler alert; no clear conclusion can be drawn from including this extra factor…). Another type of project is to evaluate different methods of either normalisation or statistical analysis to either confirm the published results or find novel genes involved in the experiment. This can be viewed more as a technical research subject which is ok to pursue, however the final report should mainly concern the biological impact of your findings. This means that when the accompanying publication shows very conclusive results, it will probably come down to acknowledging their results (therefore, you extend on point 1 from this list). Because with a very conclusive result you hope to find the same results and if this is not the case it might become hard to formulate a good conclusion in the end. However, if the publication isn’t very specific and only states a conclusion such as ‘Benzene exposure shows increased risk of leukemia’ followed by a list of a few genes that might be involved, you could try to see if you can find other genes that might be involved by changing the analysis approach. Although this last project goal has many risks involved and will need a very sound project proposal, it might result in the most interesting project. As shown before with the linked article, it is possible to combine data from multiple experiments. You could for instance find two very related experiments (i.e. researching the effect of a certain drug) both measuring expression in different tissue types (i.e. liver and brain tissue). After analyzing both experiments, you could present a set of genes that show an effect in both tissues and - more interestingly - genes showing an effect in only one tissue. The list above is a guideline to be kept in mind while browsing for suitable data sets and it will be extended in week three. Also, don’t worry if some of the terms above are unclear, getting a good understanding on the used terminology is part of these first few weeks 2.2 The NCBI Gene Expression Omnibus The easiest method of exploring and finding interesting datasets is by simply using your web-browser to access a data-repository. You could also download a few gigabytes of SQL database and query that if you’d like, but I would advice against that. One such repository is the NCBI Gene Expression Omnibus (GEO). This repository was primarily used to store microarray datasets and describe those experiments, linking to raw data, processed data and an accompanying publication. There are however many more sources of data browsable in the GEO, and for this project we will limit our searches to “Expression profiling by high throughput sequencing” for which almost 17.000 experiments are listed. The GEO Summary page shows all available data sources, click on the right Expression profiling link to get to the full table of relevant experiments. Besides finding an experiment that you are interested in, there are some further requirements that you need to account for when browsing: The experiment must be published and the publication must be fully available, free or otherwise through the Hanze University library. For each sample group, a minimum of three replicates must be present. If one or more groups have less then three replicates, that group cannot be used (the rest of the data might still be usable). The available data must contain at least the count data. Data is offered in multiple formats, always including the RAW data (reads), but also in further processed data such as FPKM, RPKM and TPM (see this interesting blog-post discussing the use of these data). The tools and R analysis libraries that we will use for the downstream analysis rely on unnormalized or unprocessed data which is the count data. These counts simply represent the number of reads mapped to a transcript. For manual analysis, these need to be normalized across samples (which has been done to get the FPKM, RPKM or TPM values) but will be done by the analysis software using the count data (unbiased). Once you’ve found an experiment, write a mail to m.kempenaar@pl.hanze.nl where you link your experiment. Once you receive a go-ahead you can start by reading the accompanying article and writing a project proposal. 2.3 Creating a Project Proposal In week three you are asked to present a short project proposal where you: Briefly explain the subject, methods, results and conclusion of the chosen project, describe the experimental design of the chosen experiment; how many samples and in how many groups are they divided (i.e. how many replicates per group)? Show what your plan is with the dataset, given the choices listed in the section about Finding Public Data. Note that whatever choice you make, the first steps will most likely be the same leading to finding the Differentially Expressed Genes (DEGs). It is very difficult to plan your complete project based on what you read in the article and it is not expected to get a planning for the full remaining 6 weeks. You can however specify a bit more given what you’ve read in the article, such as trying to reproduce a certain figure or repeat a certain analysis step. Summarize your project proposal on max. 1 A4 and, combined with the slides, are the final deliverables for this chapter (submission through the accompanying BlackBoard course). References "],
["chapter-3.html", " Exploratory Data Analysis 3.1 Exploring the Available Project Data 3.2 Loading data into R 3.3 Example Data 3.4 Visualizing using summary, boxplot, density plot, scatterplot &amp; MA-plot 3.5 Visualizing using heatmap, MDS &amp; PCA 3.6 Clean Data", " Exploratory Data Analysis We will be performing some exploratory data analysis with the goal of getting to grips with your chosen data set to properly identify a strategy for the actual analysis steps. During this exploration we will also keep an eye on the quality of the data. Even though the downloadable data is ‘processed’, there might be samples present that deviate too much from the other group of samples (a so called outlier). Creating basic visualizations of the data will give the necessary insight before we continue. We first start by downloading and loading in the actual count data. 3.1 Exploring the Available Project Data Whether you’ve found a dataset through the SRA or GEO, we want to get the data into R to start working with it. For now, we will only download the count data which is most likely stored in a TXT or XLS(X) file format: Data formats: TXT: simple text file containing a minimum of two columns (either tab or comma separated containing i.e. gene / transcript identifier and one of the above mentioned data types). However it can also contain up to 10 data columns either including more information regarding the gene/ transcript (i.e. gene ID, name, symbol, chromosome, start/ stop position, length, etc.) or more numerical columns (i.e. raw read count, normalized readcount, FPKM, etc.). XLS(X): Microsoft Excel file containing the same data columns as mentioned in the TXT files. Data types: Read Count (simple raw count of mapped reads to a certain gene or transcript). FPKM (Fragments Per Kilobase Million, F for Fragment, paired-end sequencing), RPKM (Reads Per Kilobase Million, R for Read, single-end sequencing), Note: if you want to use this, make sure that the raw data is actually single-end (should be stated in the article) TPM (Transcripts Per Kilobase Million, T for Transcript, technique independent), Layouts Either one or more files per sample with one of the above data types or One file containing the data types for all samples (with the samples as data columns in the file) Please watch the video and read the page found at the RNA-Seq blog regarding the meaning and calculation of the above mentioned expression data formats or a more technical document found at The farrago blog page. On GEO you can see what data might be availalbe in the Supplementary column, as shown below: Finding an experiment on GEO with a TXT file as supplementary data This overview on GEO contains many links which are not direct links to the items for that dataset, but can be used as filter for browsing the results. If you want to actually download the data, click on the GSE identifier (first column) which brings you to an overview for this experiment containing a lot of information about the experiment (subject, research institute, publication, design, etc.) and links to each individual sample (GSM identifier). Following the link to a sample shows information on how this sample was retrieved with often many (lab) protocols used. Sometimes there is a segment regarding “Data Processing” that refers to techniques and software used for the full analysis and might contain something like: … Differential expression testing between sample groups was performed in Rstudio (v. 1.0.36) using DESeq2 (v.1.14.1) … Back on the experiment overview page you’ll see a (variable) number of links to data files belonging to this experiment, see . For now we are only interested in the count-data which is stored in the TXT-file (see the column File type/resource) called GSE97406_RAW.tar. This file contains all the data that we need, even though it is only 220Kb in size where the experiment started with about 5Gb of read-data for a small bacteria: Finding the supplementary data in a GEO record As a bioinformatician we love to compress all the files so for this particular example, we download the tar-archive file, extract it to find a folder with another archive file for each sample. After extracting these files we end up with 12 TXT-files with just two columns; a gene identifier and a semi-raw count value (~4500 rows of data per sample): Table 3.1: Contents of a TXT-file for sample GSM2563998 Gene ID Count Value aaaD 0 aaaE 0 aaeR 50 aaeX 0 aas 118 abgB 21 abgR 56 abgT 0 abrB 11 accA 453 accB 2492 accC 1197 Other experiments combine their samples in a single file where each column represents a sample. If you do get an experiment with one file per sample, we can programmatically (yes, even in R) load this data in batch. 3.2 Loading data into R R works best with data in simple text formats. If your project data is offered as an Excel file it is therefore advised to open it in Excel (or OpenOffice Calc) and save/ export the file as a tab-separated text file, alternatively you can search for how to import an Excel file into R. Once you have one or more column based text files they can be read into R simply by using the read.table() function. Follow the following steps to read in the data and start the exploratory data analysis. The resulting document should be semi-treated as a lab journal where you log the process from loading the data to the final analysis steps. Open RStudio Create a new R Markdown document Give it a proper title and select the PDF format Give the document some structure; e.g. create a segment (using single hash #) called Exploratory Data Analysis for this week’s work. Whenever you add code to your document make sure that it is both readable (keep the maximum line length &lt; 100 if possible) and there is sufficient documentation either by text around the code ‘chunks’ or by using comments in the code chunk. Read in the data file(s) Use the read.table function and carefully set its arguments. Open the file in a text editor first to check its contents; does it have a header? can we set the row.names? Are all columns needed? etc. Note: if the data set consists of separate files (i.e. one per sample) or for general tips on reading in data, see the Appendix A: Batch Loading Expression Data chapter. For the remainder of the document, try to show either the contents, structure or - in this case - dimensions of relevant R objects Show the first five lines of the loaded data set. Including tables in a markdown document can be done using the pander function from the pander(Daróczi and Tsegelskyi 2017) R-library. Give the dimensions (with dim() and the structure (with str()) of the loaded data set. Check the output of the str function to see if all columns are of the expected R data type (e.g. values, factors, character, etc.) Examine the samples included in the experiment and create as many R character objects as needed to store the classification. For instance, if you have eight samples divided into case/ control columns you create an object called case in which you store the column indices of the respective columns in the data set and an object called control with the remaining four data column indices. These are for later use. For some datasets the order in which the samples are listed in your loaded dataset is different from the order that is shown on the GEO website. Most often, the names are different too or they are lacking any description and all you have are the GSM IDs. When creating these variables such as control that need to point to all control samples, you need to make sure that you have the right columns from your dataframe. Go to Appendix A2; annotation if the order is unclear or you just have a large number of samples as there might be supporting data available that can help make sense of your sample layout. If you want to include including external images to your log and to better control properties such as height and width for individual images you can use the following code (requires the import of the png and grid libraries). Note: the code below is an example and you need to replace the ImageToInclude.png file reference to an actual image.): # Use the following to the code chunk header to control figure height and width: # {r, fig.height=5, fig.width=4, echo=FALSE} library(png) library(grid) img &lt;- readPNG(&quot;ImageToInclude.png&quot;) grid.raster(img) 3.3 Example Data This section lists all (publically available) datasets used in this chapter. Each chapter contains this section if new datasets are used there. Note that for all examples, your data will be different from the examples and one of the challenges during this course will be translating the examples to your own data. Keep in mind that simple copy-pasting of most code will fail for that reason. Most examples therefore will print the input data for comparison to your own data. From the section Normalization onwards, the experiment with identifier GSE101942 is used for visualizing the normalized raw count values (the earlier sections explore the unnormalized data). This experiment is titled “Transcriptome analysis of genetically matched human induced pluripotent stem cells disomic or trisomic for chromosome 21” and the experimental setup is described as follows: “12 total polyA selected samples. 6 IPSC samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples. 6 IPSC derived neuronal samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples.” The following code shows how to load the two available files containing the raw-count data (one file per 6 samples), stored locally in the data/gse101942/ folder. Some simple ‘cleanup’ steps are performed which aren’t required by helpful for demonstration purposes (i.e., renaming samples to identify groups). GSE101942_IPSC_rawCounts.txt: raw count data for the induced pluripotent stem cells (iPSCs), both trisomic and disomic GSE101942_Neuron_rawCounts.txt: raw count data for the IPSC-derived Neurons, both trisomic and disomic. These cells were treated to remove chromosome 21 from the iPSCs, as described by the treatment protocol as: “Targeted removal of CHR 21 in IPSC using TKNEO transgene”. # Load the two sets of 6 samples ipsc &lt;- read.table(&#39;./data/gse101942/GSE101942_IPSC_rawCounts.txt&#39;, header = TRUE) neuron &lt;- read.table(&#39;./data/gse101942/GSE101942_Neuron_rawCounts.txt&#39;, header=TRUE) # Merge by rowname counts &lt;- merge(ipsc, neuron, by = 0, all.x = TRUE, all.y = TRUE, sort = FALSE) # Change all NA&#39;s introduced by the merge to zeros counts[is.na(counts)] &lt;- 0 print(names(counts)) ## [1] &quot;Row.names&quot; &quot;c244A&quot; &quot;c244B&quot; &quot;c243&quot; &quot;c2A&quot; ## [6] &quot;c2B&quot; &quot;c2C&quot; &quot;C3_1&quot; &quot;C3_2&quot; &quot;C3_4&quot; ## [11] &quot;C2_2_1&quot; &quot;C2_2_2&quot; &quot;C2_2_3&quot; # Set the row names to the gene IDs stored in the &#39;Row.names&#39; column row.names(counts) &lt;- counts$Row.names # Remove the gene ID column counts &lt;- counts[-1] # Rename samples to include their group name names(counts) &lt;- c(paste0(&#39;di_IPSC_r&#39;, 1:3), # Disomic IPSC, replicates 1-3 paste0(&#39;tri_IPSC_r&#39;, 1:3), # Trisomic IPSC, replicates 1-3 paste0(&#39;di_NEUR_r&#39;, 1:3), # Disomic Neuron, replicates 1-3 paste0(&#39;tri_NEUR_r&#39;, 1:3)) # Trisomic Neuron, replicates 1-3 print(names(counts)) ## [1] &quot;di_IPSC_r1&quot; &quot;di_IPSC_r2&quot; &quot;di_IPSC_r3&quot; &quot;tri_IPSC_r1&quot; &quot;tri_IPSC_r2&quot; ## [6] &quot;tri_IPSC_r3&quot; &quot;di_NEUR_r1&quot; &quot;di_NEUR_r2&quot; &quot;di_NEUR_r3&quot; &quot;tri_NEUR_r1&quot; ## [11] &quot;tri_NEUR_r2&quot; &quot;tri_NEUR_r3&quot; See Table 1 in the Normalization section for the resulting contents in the counts dataframe. 3.4 Visualizing using summary, boxplot, density plot, scatterplot &amp; MA-plot This segment describes some of the basic steps and visualizations that can be performed during Exploratory Data Analysis. In this part we will work with the unnormalized count data. In part this data is normalized before analysing it further. As mentioned above, the focus of EDA is to get an overview/ perform a bit of Quality Control of the data set and while this often requires visualizing the data, these figures do not need to be very pretty. Simple figures are perfectly fine in this stage. Try to create these figures for your own data (and keep them in your log) and add a small description for each figure pointing out anything that is different from what you expect. Also note that these steps are just a selection. Furthermore, make sure that for every visualization you make, add proper axis-labels containing the measurement units (important!). As you might be able to see, all values are log-transformed using the log2 function because very often the numerical values have a very high range which will ‘hide’ the details on the plots. See the section about the Fold Change value in chapter 4 for further details. It is fine to use non-log-transformed (simply the raw-)data, otherwise use for instance boxplot(log2(dataset)) for plotting. Instead of using the basic R-plotting library (i.e. plot, boxplot, hist, etc.) you can also opt for using the (challenging) ggplot2 library that is also used for the boxplot, scatterplot and MA-plot figures in the following sections. While constructing a ggplot2 plot feels like learning yet another language, there are many resources available online that you can follow. 3.4.1 Statistics Even the most basic statistics can give some insight into the data such as performing a 6-number-statistic on the data columns using the summary() function. Note: you can also use the pander function to pretty-print a summary from a markdown document. What do you notice if you look at the numbers produced by executing this function on the complete data set? 3.4.2 Boxplots A visual representation of these values can be shown in a boxplot. Boxplots are very easy to create from an R data frame object by just passing in the data columns. The following boxplot shows the data for an experiment with a separate boxplot for each sample. This allows a quick overview for spotting irregularities (i.e. checking if the replicates within a sample-group show similar expression profiles). Of course, if we consider the amount of data in this single plot, it can only hint at any problems, we need to look in much more detail when doing any form of quality control. Creating a boxplot from a dataframe is easy, but as we saw with using the summary function; the data has a large range with the maximum and average values being very far apart. This will create a lot of outliers in the plot which will be interesting later on, but for the boxplot we can either: * hide them completely using the outliers = FALSE argument to boxplot() (do say so in the figure description!), or * perform a log2() transformation as you can see below. Boxplot comparing basic statistics for all genes across multiple samples 3.4.3 Density Plots Another form of visualizing the same data is using a density plot. This method shows a distribution of the (log2-transformed) count data for all samples and allows for easy spotting of problems. While this plot is more commonly used in analysing microarrays, it is still useful for comparing the complete dataset. The code and figure below show an example distribution for 12 samples. A few things to note about this figure is that there is a huge peak at exactly -3.321928 which can be ignored because this is value is calculated from log2(0.1). This value is very prevalent in the dataset and consists of all 0-values (inactive genes) that had 0.1 added as a so called pseudo count, see the box in the scatterplot section. Therefore, we added a vertical line to indicate the left-part is of little interest. Note: a lot of the code below is extra, for a simple inspection using only the line plotDensity(log2(data + 0.1)) is enough, the rest is extra example code. ## The affy library has a density plotting function library(affy) ## Create a list of 4 colors to use which are the same used throughout this chapter library(scales) myColors &lt;- hue_pal()(4) ## Plot the log2-transformed data with a 0.1 pseudocount plotDensity(log2(counts + 0.1), col=rep(myColors, each=3), lty=c(1:ncol(counts)), xlab=&#39;Log2(count)&#39;, main=&#39;Expression Distribution&#39;) ## Add a legend and vertical line legend(&#39;topright&#39;, names(counts), lty=c(1:ncol(counts)), col=rep(myColors, each=3)) abline(v=-1.5, lwd=1, col=&#39;red&#39;, lty=2) Density plot comparing count distribution for 12 samples 3.4.4 Scatterplots Scatterplots are good for spotting correlations between two sets of numerical data. Data that is similar will show up as a diagonal line in a scatter plot and the more they differ, the bigger the spread of points relative to the diagonal. Downside is that a scatterplot can only contain data from two samples (columns in your data set) and it might not always be useful to plot all samples from the condition group to all samples from the control group. Other possibilities include each sample against the average of its group (should be fairly diagonal), the averages of both groups in one scatterplot (this probably shows a fair amount of variation depending on the experiment). A scatterplot can be made simply using the plot() functions with the type argument set to p (points). To show the correlation more clearly you can add a linear regression line using abline(lm(y ~ x, col='red', lwd=2). If the regression line points more downwards from the diagonal, the values in sample x are higher and vice versa. Try to create the plot on non-log2 transformed data first and you’ll see that due to the range of the values most data is in a single black blob at the lower left corner (most of the probably 20.000+ data points) with just a few in the right side of the plot. This is not very informative, so place a simple log2() function call around the two samples and plot again. This introduces another problem once the linear regression line is added since the lm() function will complain when trying to calculate its coefficients etc. on data that contains negative infinity (-Inf) ‘numbers’. These are caused by all the 0-values in the data since log2(0) == -Inf. To circumvent this issue, we can add a pseudo count to the data by simply adding the value 1 to all count values since the log2(1) == 0 and lm won’t complain anymore; lm(log2(data[sample1] + 1) ~ log2(data[sample2] + 1)). There are more situations other than the lm() function where adding a pseudo count value to the complete dataset can be useful. Always add a pseudocount in-place, meaning within the plotting code instead of overwriting your original dataset with a pseudo count added as not all steps require this. In some cases (see the density plot section) it can be useful to clearly separate the 0-values from the rest in which case a 0 &lt; pseudo count &lt; 1 value, such as 0.1 can be used as this generates a negative value (-3.3) that is far away from the rest of the data. A simple scatterplot showing the correlation between two samples or experiments (see the text above). This image shows the values log2 transformed which in most cases gives a clearer picture. 3.4.5 MA-plot An MA-plot is a scatterplot comparing the average expression value per gene (A) against the log-fold-change (M; a log2 value indicating expression change across samples which is taken as the log2(sample1 / sample2))). The points in this plot should be centered around the horizontal 0-value with an even spread both above and below the line. If there are (far) more points above or below the 0 on the y-axis, there is a bias in the data as can be shown below. An MA-plot showing the average count values (A, x-axis) vs. the log2 fold-change (M, y-axis) with a very clear bias (data not centered around y=0 line). An MA-plot can be very helpful (though it has the same constraints mentioned with the scatterplot) to see if we need to perform data normalization. There are various online resources such as Wikipedia available on how to create this plot. Carefully remember how you should work with logarithms as these rules might be (are) confusing. An MA-plot showing the average count values (A, x-axis) vs. the log2 fold-change (M, y-axis). 3.5 Visualizing using heatmap, MDS &amp; PCA This section adds a few Exploratory Data Analysis techniques where we will measure and look at distances between samples. Measuring distances between two data objects (samples in our case) is a common task in cluster analysis to compare similarity (low distance indicates similar data). In this case we will calculate the distances between our samples and visualize them in a heatmap and using multidimensional scaling (MDS) techniques. 3.5.1 Normalization In the previous section we used the raw count data. You might have one or more samples that have different values (i.e. shifted) compared to other samples. While we need the raw count data to use R packages such as edgeR (Chen et al. 2018) and DeSEQ2 (Love, Anders, and Huber 2017), calculating sample distances (used in the visualizations in this section) should be done on some form of normalized data. This data can either be RPKM/FPKM/TPM/CPM or rlog-transformed (raw-)read counts. A proper method of transforming raw read count data is using the rlog method from the DESeq2 R Bioconductor library which is shown below. The following code examples shows how to use this library to normalize the count data from the GSE101942 experiment to rlog-normalized data before we calculate a distance metric. If you look at Table 1 you immediately see a huge difference between the groups for each of the genes. The gene (A1BG) shows &gt;100 more expression in the tri_NEUR group compared to the di_IPSC group, but within most groups there is a very high variation too indicating that it might not be the actual expression that is different. One very simple and quick method of inspection is looking at the total number of mapped reads per sample (the sum of each sample/ column), as the sequencing-depth might vary across samples. di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 tri_IPSC_r1 tri_IPSC_r2 tri_IPSC_r3 108 108 81 112 134 83 Table 2; Mapped reads per sample (millions) (continued below) di_NEUR_r1 di_NEUR_r2 di_NEUR_r3 tri_NEUR_r1 tri_NEUR_r2 tri_NEUR_r3 70 94 114 108 48 140 The numbers shown in Table 2 and the barplot above immediately clearify that it is not just the experimental condition that might have caused this large difference between sample counts but the sequencing depth shows a substantial difference too. Both the minimum and maximum sequencing depth are within the trisomic neuron group (purple in the barplot). The average expression of the A1BG gene within this group is 820, but with a minimum of 461 and a maximum of 1010 there is a lot of variation. This is one more reason tellins us that we will have to normalize the data. The following code chunks show how to do this with DESeq2’s rlog method. Check the Packages tab in Rstudio to see if you have the DESeq2 package installed and load it with the library command. To use the rlog function from the DESeq2 library we must contruct a DESeqDataSet-object consisting of the count data combined with sample annotation. Since we only want to use it (for now) for performing an rlog-transformation we use the most basic form with a very simple design and the sample names as annotation (the colData argument): # Load the library library(&#39;DESeq2&#39;) # DESeq2 will construct a SummarizedExperiment object and combine this # into a &#39;DESeqDataSet&#39; object. The &#39;design&#39; argument usually indicates the # experimental design using the condition(s) names as a &#39;factor&#39;, for now we use just &#39;~ 1&#39; (ddsMat &lt;- DESeqDataSetFromMatrix(countData = counts, colData = data.frame(samples = names(counts)), design = ~ 1)) ## class: DESeqDataSet ## dim: 56640 12 ## metadata(1): version ## assays(1): counts ## rownames(56640): 5S_rRNA 7SK ... C1orf220 C2orf15 ## rowData names(0): ## colnames(12): di_IPSC_r1 di_IPSC_r2 ... tri_NEUR_r2 tri_NEUR_r3 ## colData names(1): samples We now have a proper DESeqDataSet object as you can see above, containing 56640 rows and 12 columns (genes and samples) with the gene symbols as rownames. Usually this object would hold more data, but as this is only a requirement to perform the rlog transformation it is good enough for now. Next step is performing this transformation (this can take a while depending on the size of the experiment and results in a Large DESeqTransform object) and retrieving the actual data from this with the assay function as this object too contains a lot of meta-data. The table below shows the updated values which are now comparable across genes whereas the raw count data was harder to compare. # Perform normalization rld.dds &lt;- rlog(ddsMat) # &#39;Extract&#39; normalized values rld &lt;- assay(rld.dds) di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 tri_IPSC_r1 5S_rRNA 2.548 2.932 2.767 3.813 7SK 6.272 5.973 5.466 6.387 A1BG 5.29 4.837 5.275 5.172 A1BG-AS1 8.467 8.457 8.255 8.547 A1CF -0.1184 -1.2 -1.127 0.366 A2M 4.393 4.254 4.326 5.752 Table 2; RLog transformed count data (continued below) tri_IPSC_r2 tri_IPSC_r3 di_NEUR_r1 di_NEUR_r2 5S_rRNA 3.315 3.748 3.995 3.382 7SK 6.495 5.716 4.976 4.707 A1BG 5.752 5.715 8.668 8.877 A1BG-AS1 8.385 8.433 9.256 8.916 A1CF -1.325 -1.147 -1.274 -0.7715 A2M 3.97 3.891 5.877 8.589 Table continues below di_NEUR_r3 tri_NEUR_r1 tri_NEUR_r2 tri_NEUR_r3 5S_rRNA 0.8281 3.737 3.43 0.7798 7SK 5.467 4.776 4.725 4.774 A1BG 8.475 8.835 8.788 8.534 A1BG-AS1 9.297 8.745 9.177 9.223 A1CF -0.5163 -1.384 -1.157 -1.405 A2M 7.703 7.94 7.283 5.456 Note that when again looking at the A1BG gene, the normalized expression values show much less variation accross the samples and within each sample group. Therefore, we assume that the large difference in expression we observed earlier might be non-existent (there might still be a significant difference though!). 3.5.2 Distance Calculation We now have normalized data that we can use for distance calculation. This is a standard procedure for many data analysis tasks as it calculates a distance metric for each combination of samples that we will use to check for variation within the sample groups. We first need to transpose the matrix (rld) of normalized values using the t-function, because the dist function expects the different samples as rows and the genes as columns. Note that the output matrix is symmetric. Table 3 below shows the calculated distances within the disomic IPSC group where the distance between samples varies from 100 to 140. The maximum distance between any samples is 380 as will be demonstrated with the visualisations in the next sections. # Calculate basic distance metric (using euclidean distance, see &#39;?dist&#39;) sampledists &lt;- dist( t( rld )) # Produces a square matrix-like object of distance values dim(as.matrix(sampledists)) ## [1] 12 12 di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 di_IPSC_r1 0 106.4 142.3 di_IPSC_r2 106.4 0 133.1 di_IPSC_r3 142.3 133.1 0 Table 3; Sample distances for the trisomic IPSCs (Euclidean method) 3.5.3 Sample Distances using a Heatmap If you have both (raw-)count data and an other normalized format (TPM, RPKM, etc.), you can optionally follow the above procedure for your count data and create a heatmap for both formats to see if this makes any difference. The reason for this is that while the RNA-Seq method exists for over 10 years, there are still ongoing discussions on the subject of data processing, especially regarding subjects like which data format to use for which data analysis. The following code block creates a heatmap using the pheatmap library which offers on of the many available heatmap functions. The resulting heatmap shows an interesting comparison across all samples where the order of samples is purely determined using the distance and is often different from the order in the dataset. Ideally, we want the sample groups to be clustered together as we see with this data. Using the annotation dataframe (you can inspect the contents of it yourself) we identify the samples based on both the cell type and the ploidy. The clustering shown in the heatmap clearly separates the data based on the cell type and the differences between the ploidy seems to be minimal. Looking further still, the differences within a single group are minimal too meaning that we are not - yet - inclined to remove any outlier-samples. Since we have only have 3 samples per category, we would also lose statistical power if we eventually were to remove one or more samples (also, always check the article to see if they did remove any samples prior to the data analysis). # We use the &#39;pheatmap&#39; library (install with install.packages(&#39;pheatmap&#39;)) library(pheatmap) coldata &lt;- names(counts) # Convert the &#39;dist&#39; object into a matrix for creating a heatmap sampleDistMatrix &lt;- as.matrix(sampledists) # Give the matrix row and column names #rownames(sampleDistMatrix) &lt;- coldata #colnames(sampleDistMatrix) &lt;- coldata # The annotation is an extra layer that will be plotted above the heatmap columns # indicating the cell type annotation &lt;- data.frame(Cell = factor(rep(1:2, each = 6), labels = c(&quot;IPSC&quot;, &quot;Neuron&quot;)), Ploidy = factor(rep(rep(1:2, each = 3), 2), labels = c(&quot;disomic&quot;, &quot;trisomic&quot;))) # Set the rownames of the annotation dataframe to the sample names (required) rownames(annotation) &lt;- coldata pheatmap(sampleDistMatrix, show_colnames = FALSE, annotation_col = annotation, clustering_distance_rows = sampledists, clustering_distance_cols = sampledists, main = &quot;Euclidean Sample Distances&quot;) 3.5.4 Multi-Dimensional Scaling The following code example shows how to perform Multi-Dimensional Scaling (MDS) that displays the previously calculated distances in a 2D-plot. With an experiment like this with two groups of samples, we hope to see two clearly separated clusters formed, however as we’ve seen in the heatmap, sample KO1B showed a large deviation which we will also see (confirm) this using MDS. All figures below are plotted using ggplot2, a more advanced method of plotting in R. While these plots are preferred over base-R plotting, it is always sufficient to use just that as it can be very challenging to alter the example code shown in this section. The data objects plotted are always shown and they usually contain simple X- and Y-coordinates. # Perform MDS using the &#39;cmdscale&#39; function. The resulting data can simply be # plotted using basic R plotting, here we will use &#39;ggplot2&#39; sampleDistMatrix &lt;- as.matrix(sampledists) coldata &lt;- names(counts) mdsData &lt;- data.frame(cmdscale(sampleDistMatrix)) names(mdsData) &lt;- c(&#39;x_coord&#39;, &#39;y_coord&#39;) x_coord y_coord di_IPSC_r1 -170 -17.72 di_IPSC_r2 -165.4 -8.865 di_IPSC_r3 -159.2 3.551 tri_IPSC_r1 -154.4 3.329 tri_IPSC_r2 -158.3 7.434 tri_IPSC_r3 -153.9 17.22 di_NEUR_r1 157.1 -40.19 di_NEUR_r2 154.1 -11.08 di_NEUR_r3 162.8 -78.34 tri_NEUR_r1 152 67 tri_NEUR_r2 159.8 -57.89 tri_NEUR_r3 175.4 115.6 Table 4: MDS coordinates used for plotting the distances # Separate the annotation factor (as the variable name is used as label) groups &lt;- factor(rep(1:4, each=3), labels = c(&quot;di_IPSC&quot;, &quot;tri_IPSC&quot;, &quot;di_NEUR&quot;, &quot;tri_NEUR&quot;)) # Load the ggplot2 library library(ggplot2) ggplot(mdsData, aes(x_coord, y_coord, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Euclidean Distance&#39;) + labs(x = &quot;Euclidean distance&quot;, y = &quot;Euclidean distance&quot;) + theme_bw() As a demonstration of a different distance metric (instead of the default euclidean used in the dist function) the following code shows how to calculate a maybe more fitting distance called the Poisson Distance. This library was specifically designed to handle read count data. Note that again, the code below is only usable for count data as this function requires: &gt; An n-by-p data matrix with observations on the rows, and p features on the columns. The (i,j) element of x is the number of reads in observation i that mapped to feature (e.g. gene or exon) j`. library(&#39;PoiClaClu&#39;) # Use the raw (not r-log transformed!) counts dds &lt;- assay(ddsMat) poisd &lt;- PoissonDistance( t(dds) ) # Extract the matrix with distances samplePoisDistMatrix &lt;- as.matrix(poisd$dd) # Calculate the MDS and get the X- and Y-coordinates mdsPoisData &lt;- data.frame(cmdscale(samplePoisDistMatrix)) # And set some better readable names for columns and rows names(mdsPoisData) &lt;- c(&#39;x_coord&#39;, &#39;y_coord&#39;) row.names(mdsPoisData) &lt;- row.names(mdsData) x_coord y_coord di_IPSC_r1 -21963 -3867 di_IPSC_r2 -22000 -97.21 di_IPSC_r3 -19007 -171.1 tri_IPSC_r1 -19889 1223 tri_IPSC_r2 -21693 1384 tri_IPSC_r3 -18354 2443 di_NEUR_r1 17162 -1041 di_NEUR_r2 17466 17.54 di_NEUR_r3 22619 -11120 tri_NEUR_r1 19313 6855 tri_NEUR_r2 16192 -978.6 tri_NEUR_r3 30154 5353 Table 5: MDS coordinates used for plotting the distances using Poisson Distance # Create the plot using ggplot ggplot(mdsPoisData, aes(x_coord, y_coord, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Poisson Distance&#39;) + labs(x = &quot;Euclidean distance&quot;, y = &quot;Euclidean distance&quot;) + theme_bw() Comparing both figures show similar results, we clearly see a separation on cell type and a less clear separation on ploidy. The sample di_NEUR_r3 shows a large difference in the non-normalized MDS plot, though in the normalized plot it is close to the other replicates. Once we have a set of genes identified as being differentially expressed we can repeat this step with the expectation of a more clear clustering. 3.5.5 Principal Component Analysis An alternative method of showing sample relations which is often preferred over MDS (but a bit harder to perform with pre-normalized data formats) is Principal Component Analysis (PCA). The code below shows PCA on our DESeqTransform object rld.dds. library(BiocGenerics) # Calculate PCA data data &lt;- plotPCA(rld.dds, intgroup = c(&#39;samples&#39;), returnData = TRUE) # Calculate the percentage of each principal component # Only used in the axis-labels percentVar &lt;- round(100 * attr(data, &#39;percentVar&#39;)) # Create the plot using ggplot ggplot(data, aes(PC1, PC2, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + theme_bw() Or on FPKM/RPKM data (only perform if you want to compare normalization techniques and have FPKM data. Output is not shown): # PCA done with the &#39;prcomp&#39; function in base-R. Replace &#39;rld&#39; with a matrix # containing the normalized data (genes as rows, samples as columns) pca.data &lt;- prcomp( t(rld) ) # Extract coordinates for PC1 and PC2 components &lt;- data.frame(pca.data$x[, 1:2]) # Plot using ggplot2 ggplot(data = components, aes(PC1, PC2, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) 3.6 Clean Data Conclude this chapter by cleaning your dataset if the visualizations show the necessity for this. Cleaning in this case means removing complete samples if they can be classified as an outlier within its group. As with most tasks during this course, there is no clear advice on when to decide to remove one or more samples. The only clear rule is that each group must retain at least three samples. If one of the three samples visibly deviates from the other replicates, even after normalization, it will stay in your data set and it should be mentioned in your analysis report/ final article that noise may be introduced by this sample. Note: Once you are satisfied with the data and/ or reported on your findings, we do not return to any of these steps in the next chapters. If there are mentions of creating a heatmap for instance, this means a heatmap of expression data instead of the sample distances we’ve shown in this chapter. References "],
["a1-batch-data-loading.html", "A Appendix A: Batch Loading Expression Data in R A.1 Decompressing A.2 Determining Data Format A.3 Loading Data A.4 References", " A Appendix A: Batch Loading Expression Data in R This code example shows how to batch-load multiple files containing expression (count) data for a single sample. The data for this example can be found on GEO with ID GSE109798. Downloading the data for this experiment from GEO gives us a single .tar file called GSE109798_RAW.tar. Extracting this archive file nets us a folder with the following files: file.names &lt;- list.files(&#39;data/GSE109798_RAW/&#39;) Files GSM2970149_4T1E274.isoforms.results.txt GSM2970150_4T1E266.isoforms.results.txt GSM2970151_4T1E247D.isoforms.results.txt GSM2970152_4T1P2247A.isoforms.results.txt GSM2970153_4T1P2247G.isoforms.results.txt GSM2970154_4T1P2247F.isoforms.results.txt GSM2970155_HCC1806E224B.isoforms.results.txt GSM2970156_HCC1806E224A.isoforms.results.txt GSM2970157_HCC1806E224C.isoforms.results.txt GSM2970158_HCC1806P2232A.isoforms.results.txt GSM2970159_HCC1806P2232B.isoforms.results.txt GSM2970160_HCC1806P2230.isoforms.results.txt A.1 Decompressing The file extension of all these files is .txt.gz which means that all files are compressed using gzip and need to be unpacked before they can be loaded. The easiest method is using the system gunzip command on all files which can be done from within R by applying the gunzip command using the system function on each file. ## Change directory to where the files are stored setwd(&#39;data/GSE109798_RAW/&#39;) sapply(file.names, FUN = function(file.name) { system(paste(&quot;gunzip&quot;, file.name)) }) Now we can update the file.names variable since each file name has changed. file.names &lt;- list.files(&#39;data/GSE109798_RAW/&#39;) A.2 Determining Data Format Next, we can inspect what the contents are of these files, assuming that they all have the same layout/ column names etc. to decide what we need to use for our analysis. ## Call the system &#39;head&#39; tool to &#39;peek&#39; inside the file system(paste0(&quot;head &quot;, &quot;data/GSE109798_RAW/&quot;, file.names[1])) transcript_id gene_id length effective_length expected_count TPM FPKM IsoPct uc007aet.1 1 3608 3608.00 1.82 0.44 0.24 100.00 uc007aeu.1 1 3634 3634.00 0.00 0.00 0.00 0.00 uc011whv.1 10 26 26.00 0.00 0.00 0.00 0.00 uc007amd.1 100 1823 1823.00 0.00 0.00 0.00 0.00 uc007ame.1 100 4355 4355.00 1.32 0.26 0.15 100.00 uc007dac.1 1000 1403 1403.00 2.00 1.25 0.70 100.00 uc008ajp.1 10000 1078 1078.00 0.00 0.00 0.00 0.00 uc012ajs.1 10000 1753 1753.00 0.00 0.00 0.00 0.00 uc008ajq.1 10001 2046 2046.00 0.00 0.00 0.00 0.00 These files contain (much) more then just a count value for each gene as we can see columns such as (transcript) length, TPM, FPKM, etc. Also, the count-column is called expected_count which raises a few questions as well. The expected count value is usable as it contains more information - compared to the raw count - then we actually require. The expected part results from multimapped reads where a single read mapped to multiple positions in the genome. As each transcript originates only from one location, this multimapped read is usually discarded. With the expected count though, instead of discarding the read completely it is estimated where it originates from and this is added as a fraction to the count value. So the value of 1.32 that we see on line 5 in the example above means an true count of 1 (uniquely mapped read) and the .32 (the estimated part) results from an algorithm and can mean multiple things. As mentioned before, we require integer count data for use with packages such as DESeq2 and edgeR and there are two methods to convert the expected count to raw count data: + round the value to the nearest integer (widely accepted method and is well within the expected sampling variation), or + discard the fraction part by using for example the floor() function. A.3 Loading Data From all these columns we want to keep the transcript_id and expected_count columns and ignore the rest (we might be interested in this data later on though). As we need to lead each file separately we can define a function that reads in the data, keeping the columns of interest and returning a dataframe with this data. Note that the first line of each file is used as a header, but check before setting the header argument to TRUE, sometimes the expression data starts at line 1. The file name is then also used to name the column in the dataframe so that we know which column is which sample. This is done by splitting the file name (using strsplit) using the dot (‘.’) keeping the first part (i.e. ‘GSM2970156_HCC1806E224A’) and discarding the second part (‘isoforms.results.txt’). The strsplit function however always returns a list, in this case containing a vector with the 5 splitted elements: ## String splitting in R ## (the fixed = TRUE is required as the dot is a special character, see &#39;?strsplit&#39;) strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE) ## [[1]] ## [1] &quot;GSM2970155_HCC1806E224B&quot; &quot;isoforms&quot; ## [3] &quot;results&quot; &quot;txt&quot; ## [5] &quot;gz&quot; ## Keeping the sample identifier strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE)[[1]][1] ## [1] &quot;GSM2970155_HCC1806E224B&quot; ## Function for reading in files read_sample &lt;- function(file.name) { ## Extract the sample name for naming the column sample.name &lt;- strsplit(file.name, &quot;.&quot;, fixed = TRUE)[[1]][1] ## Read the data, use a header but not row.names sample &lt;- read.table(file.name, header = TRUE, sep=&quot;\\t&quot;, row.names = NULL) ## Rename the &#39;expected_count&#39; column names(sample)[5] &lt;- sample.name ## Return a subset containing the &#39;transcript_id&#39; and (renamed) expected_count column return(subset(sample, select=c(1, 5))) } Applying the read_sample function to all file names gives us a set of data frames that we can merge together using the merge function. We merge the data based on the transcript id defined with the by = 0 (or by = 'row.names') argument. We start by reading in just one file which is the ‘base’ dataframe to which we will merge the other files. During processing it seemed that this data set is divided into two groups which is also listed on the GEO website for this project: GPL11154 Illumina HiSeq 2000 (Homo sapiens) GPL13112 Illumina HiSeq 2000 (Mus musculus) where the first 6 files are from human source and the last 6 from the mouse. Therefore, the following code only shows how to read the first 6 samples and merge these into a single dataframe. Repeating this process for the other 6 files would result into another dataframe for those samples. setwd(&#39;data/GSE109798_RAW/&#39;) ## Read the FIRST sample dataset &lt;- read_sample(file.names[1]) ## Read first sample group (6) for (file.name in file.names[2:6]) { sample &lt;- read_sample(file.name) # Merge new sample to existing dataset, using &#39;transcript_id&#39; column dataset &lt;- merge(dataset, sample, by = 1) } # Set the row.names to the &#39;transcript_id&#39; column row.names(dataset) &lt;- dataset$transcript_id # Remove the &#39;transcript_id&#39; column dataset &lt;- dataset[,-1] pander(head(dataset)) GSM2970149_4T1E274 GSM2970150_4T1E266 GSM2970151_4T1E247D uc007aet.1 1.82 0 0 uc007aeu.1 0 1 0.24 uc007aev.1 0 0 0 uc007aew.1 0 0 0.97 uc007aex.2 0 0 0 uc007aey.1 0 0 0 Table continues below GSM2970152_4T1P2247A GSM2970153_4T1P2247G uc007aet.1 0 0 uc007aeu.1 0.13 0 uc007aev.1 0 0 uc007aew.1 0 1 uc007aex.2 0 0 uc007aey.1 0 0 Table continues below GSM2970154_4T1P2247F uc007aet.1 0 uc007aeu.1 0 uc007aev.1 0 uc007aew.1 0 uc007aex.2 0 uc007aey.1 0 The dataset variable now contains all data for the first 6 samples in this experiment. It is advisable to compare the number of rows in this data set with the number of rows in a single sample. It is not guaranteed that all samples have exactly the same number of genes/transcripts present (i.e., 0-values might have been discarded) which results in a final data set that has as many rows as the smallest sample. See the help of merge if this is the case because the all argument can be used to introduce extra rows for missing data. A.4 References "]
]
