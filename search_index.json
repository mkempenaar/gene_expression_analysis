[
["index.html", "Analysis of Gene Expression Preface", " Analysis of Gene Expression Marcel Kempenaar 2018-02-11 Preface This project aims to teach the whole process of analysing a dataset containing gene expression data measured with RNA-sequencing technique. The tools we will use to perform the analysis will be the statistical programming language R with its many libraries and the software package RStudio to interact with R. Instead of following the usual weekly deliverables and final assignment, this course is setup around performing your own research, based on published data and given a general guideline with a set of defined end-products. Furthermore, this whole course is done individually. Cooperating and discussing however with your fellow students and helping each other is desired/ expected. "],
["chapter-1.html", " Introduction 1.1 Theme 7 - Analysis of Gene Expression Project 1.2 Project Deliverables 1.3 Project Schedule 1.4 Grading 1.5 Article", " Introduction 1.1 Theme 7 - Analysis of Gene Expression Project The use of RNA-Sequencing techniques for measuring gene expression is relatively new and replaces microarrays, though in some cases microarrays are still used. Gene expression data gives valuable insights into the workings of cells in certain conditions. Especially when comparing for instance healthy and diseased samples it can become clear which genes are causal or under influence of a specific condition. Finding the genes of interest (genes showing differing expression accross conditions, called the Differentially Expressed Genes (DEGs)) is the goal of this project. While there is no golden standard for analyzing RNA-sequencing datasets as there are many tools (all manufacturers of sequencing equiptment also deliver software packages) we will use proven R libraries for processing, visualizing and analyzing publically available datasets. While in some cases you are allowed to use the actual raw data that is available, it is highly recommended to use the pre-processed data which often is a table with a count value for each gene. This count is the number of reads that was mapped to that gene which corresponds to the relative number of transcripts (mRNA sequences) of that gene present in the cell at the time of sampling. 1.2 Project Deliverables The end products of this course consist of three deliverables; a PDF output file from an RMarkdown ‘lab journal’ where you have logged all steps performed to get to the end result, a final report in the form of a short article and a poster to be presented - if selected - during the Life Science and Technology Poster session at the end of this quarter. 1.3 Project Schedule The aim is to keep to the below schedule during this course. Use the first two weeks to see if you need to focus more on one of the points below and discuss changes to the planning with your teacher. Find a public experiment of interest [week one] Using online resources Data Gathering and Literature Research [week two] Make a final project choice Retrieve the accompanying publication Download and inspect the supplementary files Write and present a short project proposal Data Analysis [weeks three and four] Exploratory Data Analysis Data Annotation Discovering Differentially Expressed Genes (DEGs) Multiple Testing Techniques used: R with bioconductor, the EdgeR and/ or DESeq2 packages, RMarkdown Result Analysis [weeks five and six] Analyzing and Visualizing your results Techniques used: clustering, pathway analysis, gene-enrichment analysis Finalizing analysis and start writing the final report and design a poster [weeks 7 and 8] 1.4 Grading The final grade consists of a weighted average of the work done for weeks 1 &amp; 2 (resulting in the project proposal) (15%), the lab journal containing all performed steps, their code and outputs (50%), the article report and poster (25%) and finally your work attitude (10%). A grade higher then a 5.5 will give a total of 5 EC. The rest of this chapter explains the expected contents for the three graded products (each of these elements is explained in greater detail in other chapters). 1.4.1 Project Proposal As you are free to choose from thousands of public data sets on a large variety of biological subjects, it is important to demonstrate a good understanding of the experiment, biology and the available data. This is demonstrated by presenting a short project proposal where you briefly explain the subject of the chosen research article, the experimental setup (how many samples were used, etc.) and what types of data are available for you to use. 1.4.2 Keeping a Log (a lab journal) As you know from previous projects and (maybe from) working in the laboratory, it is essential to keep a proper lab journal detailing every step you have done during the experiment. The log is to be kept in an R markdown file, showing which steps have been taken in the analysis of the data set. This markdown should be knitted into a single PDF-file once the project is completed thus containing text detailling the steps and any decisions you’ve made, R-code (always visible!) and their resulting output/ images. As a general advice; do not wait with knitting this whole document until the project is done! 1.5 Article The final report is written in article form which is a bit different from a usual report, mainly in its size. The article-report has a maximum number of pages of 4 including all images and references (no appendices!). Contents for this article should be extracted from the lab journal combined with part introduction and part conclusion/ discussion. The sections below describe a template that is available for writing this report and example report(s) will be made available for inspiration. Refer to this chapter again once you start writing the report. 1.5.1 Installing RStudio Templates The templates are available in an R package and contains both RMarkdown and (another layer on top of the actual markup language, yes, another language…) files. RStudio can use templates for a number of documents, including article-templates. These templates can be installed from a package called rticles by running the install.packages function (note: might already be installed): install.packages(&quot;rticles&quot;, type = &quot;source&quot;) 1.5.2 Using the template Now that you have the templates package (rticles) installed you can download and use the template project (ZIP-file) available from https://github.com/mkempenaar/BFVH15CAPSTONE/blob/BFVH3TH7_2018_dev/article/report-template.zip. Download this file to your project folder, extract its contents and open the report-template.Rmd file contained within the folder in RStudio. Verify that everything is setup correctly by hitting the Knit button at the top, this should create a PDF version of the report. Note that - somehow - the resulting PDF file is named RJwrapper.pdf instead of the expected report-template.pdf. If everything checks out ok you can rename the file to your liking and start editing. This template is based on the R Journal Submission template that you can also find in RStudio in the New file -&gt; R Markdown -&gt; From Template menu. Articles published in the R-journal are based on this template which you can browse for inspiration at the [R Journal Website]((https://journal.r-project.org/archive/2015-2/). The available template shows an example of segments/ chapters and briefly describes what each section could or should contain. If you want to write your report in the Dutch language, you can create a new file from the template and change the segment names and content to Dutch. There will however still be some headings added by the template in English which is fine with me, but if you want to keep everything Dutch you can either edit the RJournal.sty file manually (not recommended) or start a new file by yourself and add some nice headings and page options. The top part of the template (between the dashes ---) contains some settings that you need to change such as title, author and abstract. Compare for instance a newly created article from this template with the one offered from the project-repository website. "],
["chapter-2.html", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest 2.2 The NCBI Gene Expression Omnibus 2.3 Creating a Project Proposal", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest This chapter describes a method of finding public datasets of interest. A dataset in the context of this course refers to all data belonging to a certain gene-expression experiment, usually consisting of a number of sequencing-samples combined with meta-data describing the experiment. There are a number of very large databases online that offer access to thousands of - published - experiments and here we will focus on searching and downloading gene expression data from high-throughput (NGS) sequencing techniques (RNA-Seq). With thousands of freely available datasets it is possible to start performing research without the need of actually performing your own lab-experiment. For all common conditions, organisms and tissues you can download samples and compare them over multiple studies to find novel relations between genes and conditions or to verify experiments performed in your laboratory. For instance, the power of public datasets was demonstrated jointly by three of our alumni in an article called Calling genotypes from public RNA-sequencing data enables identification of genetic variants that affect gene-expression levels.(al. 2015). It’s method section begins with the sentence “We downloaded the raw reads for all available human RNA-seq datasets …” of which the amount of data and work will become clear later on. If you are interested in expression Quantitative Trait Loci (eQTL) or Allele-Specific Expression (ASE) analysis, please read this very interesting paper. The following weeks you will be investigating one or more public RNA-sequencing datasets yourself and the rest of this document focuses on databases containing such public data sets and - especially - how to find a dataset or experiment you would like to use. But before we start diving into large and complex databases potentially containing thousands of experiments with millions of samples and terabytes of data, we need to get a rough idea on what we would like to do once we have found something of interest in order to know what we are looking for in the first place. While there is no definitive guide or protocol that can be followed for processing and analysing a gene-expression dataset, the following goals can be considered: Re-do (part of) the analysis described in the accompanying publication. As all datasets - except for the ones that were added very recently - are accompanied by a publication, you can gather a lot of information regarding experimental design and results from just this paper. Most often, the researchers already performed the most interesting research on this dataset and it is therefore a good exercise to try and reproduce their results. Alternatively you can come up with your own ideas and (biological) question(s) that you could try and answer given a dataset instead of redoing the published work. This is of course more challenging and not suitable for all datasets. You might notice that some publications only focus on a small set of genes instead of the whole transcriptome. For instance, when you perform an experiment on yeast where you want to measure the activity of genes involved in alcohol fermentation, researchers might only look at genes from the Glycolysis pathway. This leaves the other 6000+ yeast genes for you to explore and possibly come up with novel relations of gene expression and experimental setup. If the analysis approach explained in the paper is not focusing on a pre-selected list of genes, depending on the experiment you might come up with comparisons not done in the original research. For instance, in an experiment where the maternal age (at birth) is correlated to autism in their offspring, the paternal age is also known but not addressed in the publication. This could be a subject of further research (spoiler alert; no clear conclusion can be drawn from including this extra factor…). Another type of project is to evaluate different methods of either normalisation or statistical analysis to either confirm the published results or find novel genes involved in the experiment. This can be viewed more as a technical research subject which is ok to pursue, however the final report should mainly concern the biological impact of your findings. This means that when the accompanying publication shows very conclusive results, it will probably come down to acknowledging their results (therefore, you extend on point 1 from this list). Because with a very conclusive result you hope to find the same results and if this is not the case it might become hard to formulate a good conclusion in the end. However, if the publication isn’t very specific and only states a conclusion such as ‘Benzene exposure shows increased risk of leukemia’ followed by a list of a few genes that might be involved, you could try to see if you can find other genes that might be involved by changing the analysis approach. Although this last project goal has many risks involved and will need a very sound project proposal, it might result in the most interesting project. As shown before with the linked article, it is possible to combine data from multiple experiments. You could for instance find two very related experiments (i.e. researching the effect of a certain drug) both measuring expression in different tissue types (i.e. liver and brain tissue). After analyzing both experiments, you could present a set of genes that show an effect in both tissues and - more interestingly - genes showing an effect in only one tissue. The list above is a guideline to be kept in mind while browsing for suitable data sets and it will be extended in week three. Also, don’t worry if some of the terms above are unclear, getting a good understanding on the used terminology is part of these first few weeks 2.2 The NCBI Gene Expression Omnibus The easiest method of exploring and finding interesting datasets is by simply using your web-browser to access a data-repository. You could also download a few gigabytes of SQL database and query that if you’d like, but I would advice against that. One such repository is the NCBI Gene Expression Omnibus (GEO). This repository is primarily used to store microarray datasets and describe those experiments, linking to raw data, processed data and an accompanying publication. There are however many more sources of data browsable in the GEO, and for this project we will limit our searches to “Expression profiling by high throughput sequencing” for which almost 17.000 experiments are listed. The GEO Summary page shows all available data sources, click on the right Expression profiling link to get to the full table of relevant experiments. Besides finding an experiment that you are interested in, there are some further requirements that you need to account for when browsing: The experiment must be published and the publication must be fully available, free or otherwise through the Hanze University library. For each sample group, a minimum of three replicates must be present. If one or more groups have less then three replicates, that group cannot be used (the rest of the data might still be usable). The available data must contain at least the count data. Data is offered in multiple formats, always including the RAW data (reads), but also in further processed data such as FPKM, RPKM and TPM (see this interesting blog-post discussing the use of these data). The tools and R analysis libraries that we will use for the downstream analysis rely on unnormalized or unprocessed data which is the count data. These counts simply represent the number of reads mapped to a transcript. For manual analysis, these need to be normalized across samples (which has been done to get the FPKM, RPKM or TPM values) but will be done by the analysis software using the count data (unbiased). Once you’ve found an experiment, write a mail to m.kempenaar@pl.hanze.nl where you link your experiment. Once you receive a go-ahead you can start by reading the accompanying article and writing a project proposal. 2.3 Creating a Project Proposal In week two you are asked to present a short project proposal where you: Briefly explain the subject, methods, results and conclusion of the chosen project, describe the experimental design of the chosen experiment; how many samples and in how many groups are they divided (i.e. how many replicates per group)? Show what your plan is with the dataset, given the choices listed in the section about Finding Public Data. Note that whatever choice you make, the first steps will most likely be the same leading to finding the Differentially Expressed Genes (DEGs). It is very difficult to plan your complete project based on what you read in the article and it is not expected to get a planning for the full remaining 6 weeks. You can however specify a bit more given what you’ve read in the article, such as trying to reproduce a certain figure or repeat a certain analysis step. Summarize your project proposal on max. 1 A4 and, combined with the slides, are the final deliverables for this chapter. References "],
["chapter-3.html", " Exploratory Data Analysis 3.1 Exploring the Available Project Data 3.2 Loading data into R", " Exploratory Data Analysis We will be performing some exploratory data analysis with the goal of getting to grips with your chosen data set to properly identify a strategy for the actual analysis steps. During this exploration we will also keep an eye on the quality of the data. Even though the downloadable data is ‘processed’, there might be samples present that deviate too much from the other group of samples (a so called outlier). Another problem that might surface with your data is a sample-swap, where two samples are labeled incorrectly. There is a great deal of administration required when researchers want to publish their data, and such mistakes can happen. Creating basic visualizations of the data will give the necessary insight before we continue. We first start by downloading and loading in the actual count data. 3.1 Exploring the Available Project Data Whether you’ve found a dataset through NCBI’s Short Read Archive or the Gene Expression Omnibus, we want to get the data into R to start working with it. For now, we will only download the count data which is most likely stored in a TXT or XLS(X) file format: Data formats: TXT: simple text file containing a minimum of two columns (either tab or comma separated containing i.e. gene / transcript identifier and one of the above mentioned data types). However it can also contain up to 10 data columns either including more information regarding the gene/ transcript (i.e. gene ID, name, symbol, chromosome, start/ stop position, length, etc.) or more numerical columns (i.e. raw read count, normalized readcount, FPKM, etc.). XLS(X): Microsoft Excel file containing the same data columns as mentioned in the TXT files. Data types: Read Count (simple raw count of mapped reads to a certain gene or transcript). FPKM (Fragments Per Kilobase Million, F for Fragment, paired-end sequencing), RPKM (Reads Per Kilobase Million, R for Read, single-end sequencing), Note: if you want to use this, make sure that the raw data is actually single-end (should be stated in the article) TPM (Transcripts Per Kilobase Million, T for Transcript, technique independent), Layouts Either one or more files per sample with one of the above data types or One file containing the data types for all samples (with the samples as data columns in the file) Please watch the video and read the page found at the RNA-Seq blog regarding the meaning and calculation of the above mentioned expression data formats or a more technical document found at The farrago blog page. On GEO you can see what data might be availalbe in the Supplementary column, as shown below: (#fig:GEO_RNA-Seq)Finding an experiment on GEO with a TXT file as supplementary data This overview on GEO contains many links which are not direct links to the items for that dataset, but can be used as filter for browsing the results. If you want to actually download the data, click on the GSE identifier (first column) which brings you to an overview for this experiment containing a lot of information about the experiment (subject, research institute, publication, design, etc.) and links to each individual sample (GSM identifier). Following the link to a sample shows information on how this sample was retrieved with often many (lab) protocols used. Sometimes there is a segment regarding “Data Processing” that refers to techniques and software used for the full analysis and might contain something like: … Differential expression testing between sample groups was performed in Rstudio (v. 1.0.36) using DESeq2 (v.1.14.1) … Back on the experiment overview page you’ll see a (variable) number of links to data files belonging to this experiment, see . For now we are only interested in the count-data which is stored in the TXT-file (see the column File type/resource) called GSE97406_RAW.tar. This file contains all the data that we need, even though it is only 220Kb in size where the experiment started with about 5Gb of read-data for a small bacteria: As a bioinformatician we love to compress all the files so for this particular example, we download the tar-archive file, extract it to find a folder with another archive file for each sample. After extracting these files we end up with 12 TXT-files with just two columns; a gene identifier and a semi-raw count value (~4500 rows of data per sample): Table 3.1: Contents of a TXT-file for sample GSM2563998 Gene ID Count Value aaaD 0 aaaE 0 aaeA 3 aaeB 3 aaeR 50 aaeX 0 aas 118 aat 43 abgA 6 abgB 21 abgR 56 abgT 0 abrB 11 accA 453 accB 2492 accC 1197 Other experiments combine their samples in a single file where each column represents a sample. If you do get an experiment with one file per sample, it’s a small task to combine these (just a few lines of R-code). 3.2 Loading data into R R works best with data in simple text formats. If your project data is offered as an Excel file it is therefore advised to open it in Excel (or OpenOffice Calc) and save/ export the file as a tab-separated text file, alternatively you can search for how to import an Excel file into R. Once you have one or more column based text files they can be read into R simply by using the read.table() function. Follow the following steps to read in the data and start the exploratory data analysis. The resulting document should be semi-treated as a lab journal where you log the process from loading the data to the final analysis steps. Open RStudio Create a new R Markdown document Give it a proper title and select the PDF format Give the document some structure; e.g. create a segment called Exploratory Data Analysis for this week’s work. Whenever you add code to your document make sure that it is both readable (keep the maximum line length &lt; 100 if possible) and there is sufficient documentation either by text around the code ‘chunks’ or by using comments in the code chunk. Read in the data file(s) For the remainder of the document, try to show either the contents, structure or - in this case - dimensions of relevant R objects Show the first five lines of the loaded data set. Including tables in a markdown document can be done using the pander function from the pander(Daróczi and Tsegelskyi 2017) R-library. Give the dimensions (with dim() and the structure (with str()) of the loaded data set. Check the output of the str function to see if all columns are of the expected R data type (e.g. values, factors, character, etc.) Examine the samples included in the experiment and create as many R character objects as needed to store the classification. For instance, if you have eight samples divided into case/ control columns you create an object called case in which you store the column indices of the respective columns in the data set and an object called control with the remaining four data column indices. These are for later use. If you want to include including external images to your log and to better control properties such as height and width for individual images you can use the following code (requires the import of the png and grid libraries). Note: the code below is an example and you need to replace the ImageToInclude.png file reference to an actual image.): # Use the following to the code chunk header to control figure height and width: # {r, fig.height=5, fig.width=4, echo=FALSE} library(png) library(grid) img &lt;- readPNG(&quot;ImageToInclude.png&quot;) grid.raster(img) References "],
["eda-part-i.html", "EDA - Part I 3.3 Visualizing using summary, boxplot, scatterplot &amp; MA-plot", " EDA - Part I 3.3 Visualizing using summary, boxplot, scatterplot &amp; MA-plot This segment describes some of the basic steps and visualizations that can be performed during Exploratory Data Analysis. In this part we work with the unnormalized count data. In part this data is normalized before analyzing it further. As mentioned above, the focus of EDA is to get an overview of the data set and while this often requires visualizing the data, these figures do not need to be very pretty. Simple figures are perfectly fine in this stage. Try to create these figures for your own data (and keep them in your log). Also note that these steps are just a selection. Furthermore, make sure that for every visualization you make, add proper axis-labels containing the measurement units (important!). 3.3.1 Statistics Even the most basic statistics can give some insight into the data such as performing a 6-number-statistic on the data columns using the summary() function. 3.3.2 Boxplots A visual representation of these values can be shown in a boxplot. Boxplots are very easy to create from an R data frame object by just passing in the data columns. The following boxplot shows the data for the complete experiment with a separate boxplot for each sample. Boxplot comparing basic statistics for all genes across multiple samples 3.3.3 Scatterplots Scatterplots are good for spotting correlations between two sets of numerical data. Data that is similar will show up as a diagonal line in a scatter plot and the more they differ, the bigger the spread of points relative to the diagonal. Downside is that a scatterplot can only contain data from two columns and it might not always be useful to plot all samples from the condition group to all samples from the control group. Other possibilities include each sample against the average of its group (should be fairly diagonal), the averages of both groups in one scatterplot (this probably shows a fair amount of variation depending on the experiment). A scatterplot can be made simply using the plot() functions with the type argument set to p (points). To show the correlation more clearly you can add a linear regression line using abline(lm(y ~ x, col='red', lwd=2). If the regression line points more downwards from the diagonal, the values in sample x are higher and vice versa. A simple scatterplot showing the correlation between two samples or experiments (see the text above). This image shows the values log2 transformed which in most cases gives a clearer picture. 3.3.4 MA-plots An MA-plot is a scatterplot comparing the average expression value per gene (A) against the log-fold-change (M; a log2 value indicating expression change across samples which is taken as the log2(sample1 / sample2))). The points in this plot should be centered around the horizontal 0-value with an even spread both above and below the line. If there are (far) more points above or below the 0 on the y-axis, there is a bias in the data. An MA-plot can be very helpful (it has the same constraints mentioned with the scatterplot) but might not be easy/ possible to create as this is mostly done on normalized count data other than FPKM or RPKM. If you have raw or normalized count data in your data set you can search various online resources on how to create this plot. An MA-plot showing the average normalized count values (A, x-axis) vs. the log2 fold-change (M, y-axis). "],
["eda-part-ii.html", "EDA - Part II 3.4 Visualizing using heatmap, MDS &amp; PCA", " EDA - Part II 3.4 Visualizing using heatmap, MDS &amp; PCA This section adds a few Exploratory Data Analysis techniques where we will measure and look at distances between samples. Measuring distances between two data objects (samples in our case) is a common task in cluster analysis to compare similarity (low distance indicates similar data). In this case we will calculate the distances between our samples and visualize them in a heatmap and using multidimensional scaling (MDS) techniques. 3.4.1 Normalization In the previous section we used the raw count data. You might have one or more samples that have different values (i.e. shifted) compared to other samples. While we need the raw count data to use R packages such as edgeR (Chen et al. 2018) and DeSEQ2 (Love, Anders, and Huber 2017), calculating sample distances should be done on some form of normalized data. This data can either be RPKM/FPKM/TPM/CPM or log-transformed (raw-)read counts. A proper method of transforming raw read count data is using the rlog method from the DESeq2 R Bioconductor library which is shown below. The following code examples shows how to use this library to normalize the count data to rlog-normalized data before we calculate a distance metric. The data used for this example is available at the GEO (Accession GSE80128, titled Evidence for two protein coding transcripts at the Igf2as locus consisting of 8 samples from mouse skeletal muscle tissue; 4 wild-type [WT] and 4 \\(\\Delta\\)DMR1-U2 [KO] variants). gse80128 &lt;- read.table(&#39;./data/GSE80128_P035_DESeq_KO-WT_pathway.txt&#39;, sep = &#39;\\t&#39;, header = TRUE) # Only use the actual count data as this file contains a lot more descriptive columns counts &lt;- as.matrix(gse80128[, 2:9]) rownames(counts) &lt;- gse80128$geneID Table 1; Raw count data for GSE80128 KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 Igf2as 121 104 100 122 1113 1868 1490 1282 St8sia5 77 25 52 65 147 206 199 222 Apln 222 85 353 224 638 1133 993 802 Prnd 400 79 527 208 1069 1600 1431 1306 Camk2b 1573 1066 1457 1457 2447 3220 3630 3435 Slc2a5 17 22 33 24 61 127 122 94 If you look at Table 1 you immediately see a huge difference between the two groups for each of the genes. The first gene (Igf2as) shows &gt; 10 more expression in the wild-type group compared to the KO group, but it is unknown if this is only caused by the experiment itself by just looking at these numbers. One very simple and quick inspection is looking at the total number of mapped reads per sample as the sequencing-depth might vary across samples. # Show the number of mapped reads per sample in millions pander( colSums(counts) / 1e6, caption = &#39;Mapped reads per sample (millions)&#39;) KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 19 18.24 16.78 20.06 16.08 25.96 26.91 24.24 The numbers shown in Table 2 immediately clearify that it is not just the experimental condition that might have caused this large difference between sample counts but the sequencing depth shows a substantial difference too. One more reason that we will normalize the data with DESeq2’s rlog method. Check the Packages tab in Rstudio to see if you have the DESeq2 package installed and load it with the library command. If you are missing this package, install and load the library using the code below: # Install if the library is not available source(&#39;http://bioconductor.org/biocLite.R&#39;) biocLite(&#39;DESeq2&#39;) # Load the library library(&#39;DESeq2&#39;) To use the rlog function from the DESeq2 library we must contruct a DESeqDataSet object consisting of the count data combined with sample annotation. Since we only want to use it (for now) for performing a log-transformation we use the most basic form with the sample names as annotation (the colData argument): # Put the sample names in a separate vector coldata &lt;- colnames(counts) # DESeq2 will now construct a SummarizedExperiment object and combine this # into a &#39;DESeqDataSet&#39; object. The &#39;design&#39; argument usually indicates the # experimental design using the sample names as a &#39;factor&#39;, for now we use just &#39;~ 1&#39; (ddsMat &lt;- DESeqDataSetFromMatrix(countData = counts, colData = data.frame(samples=coldata), design = ~ 1)) ## class: DESeqDataSet ## dim: 23336 8 ## metadata(1): version ## assays(1): counts ## rownames(23336): Igf2as St8sia5 ... Zscan4f Zscan5b ## rowData names(0): ## colnames(8): KO1A KO1B ... WT2 WT3 ## colData names(1): samples We now have a proper DESeqDataSet object as you can see above, containing 23336 rows and 8 columns (genes and samples) with the gene symbols as rownames. Usually this object would hold more data, but as this is only a requirement to perform the rlog transformation it is good enough for now. Next step is performing this transformation (results in a Large DESeqTransform object) and retrieving the actual data from this with the assay function as this object too contains a lot of meta-data. The table below shows the updated values which are now comparable across genes whereas the raw count data was harder to compare. # Perform normalization rld.dds &lt;- rlog(ddsMat) # &#39;Extract&#39; normalized values rld &lt;- assay(rld.dds) Table 3; RLog transformed count data KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 Igf2as 8.04 7.999 8.042 8.024 9.927 9.931 9.687 9.663 St8sia5 6.624 6.282 6.532 6.535 7.123 7.016 6.977 7.134 Apln 8.34 7.879 8.78 8.323 9.349 9.404 9.261 9.184 Prnd 8.986 8.138 9.325 8.531 10.03 9.924 9.796 9.829 Camk2b 10.82 10.54 10.89 10.73 11.44 11.2 11.29 11.36 Slc2a5 5.494 5.552 5.682 5.548 5.942 6.069 6.034 5.961 3.4.2 Distance Calculation We now have normalized data that we can use for distance calculation. We need to transpose the matrix or data frame of values using t(), because the dist function expects the different samples as rows and the genes as columns. Note that the output matrix is symmetric. # Calculate basic distance metric (using euclidean distance, see &#39;?dist&#39;) sampledists &lt;- dist( t( rld )) Table 4; Sample distances (Euclidean method) KO1A KO1B KO2 KO3 WT1A WT1B WT2 WT3 KO1A 0 21.48 13.96 16.92 20.14 15.87 15.47 16.94 KO1B 21.48 0 22.46 19.17 28.1 27.65 25.16 24.85 KO2 13.96 22.46 0 17.7 17.15 13.82 14.93 15.11 KO3 16.92 19.17 17.7 0 19.66 21.38 19.24 19.02 WT1A 20.14 28.1 17.15 19.66 0 13.83 14.62 15.82 WT1B 15.87 27.65 13.82 21.38 13.83 0 12.08 14.61 WT2 15.47 25.16 14.93 19.24 14.62 12.08 0 13.24 WT3 16.94 24.85 15.11 19.02 15.82 14.61 13.24 0 3.4.3 Sample Distances using a Heatmap If you have both (raw-)count data and an other normalized format (TPM, RPKM, etc.), follow the above procedure for your count data and create a heatmap for both formats to see if this makes any difference. The reason for this is that while the RNA-Seq method exists for over 10 years, there are still ongoing discussions on the subject of data processing, especially regarding subjects like which data format to use for which data analysis. The following code block creates a heatmap using the pheatmap library which offers on of the many available heatmap functions. The resulting heatmap shows an interesting comparison across all samples. Where all of the wild-type samples cluster nicely together, one of the knockout samples deviates from all other samples and might be classified as an outlier in this case. Since we have 4 samples per category, we retain statistical power if we eventually were to remove this sample. # We use the &#39;pheatmap&#39; library (install with install.packages(&#39;pheatmap&#39;)) library(pheatmap) # Convert the &#39;dist&#39; object into a matrix for creating a heatmap sampleDistMatrix &lt;- as.matrix(sampledists) # Give the matrix row and column names rownames(sampleDistMatrix) &lt;- coldata colnames(sampleDistMatrix) &lt;- coldata # The annotation is an extra layer that will be plotted above the heatmap columns # indicating the cell type annotation &lt;- data.frame(Variant = factor(c(1, 1, 1, 1, 2, 2, 2, 2), labels = c(&#39;KO&#39;, &#39;WT&#39;))) # Set the rownames of the annotation dataframe to the sample names (required) rownames(annotation) &lt;- coldata pheatmap(sampleDistMatrix, show_colnames = FALSE, annotation = annotation, clustering_distance_rows = sampledists, clustering_distance_cols = sampledists) 3.4.4 Sample Distances using MDS The following code example shows how to perform Multidimensional scaling that displays the previously calculated distances in a 2D-plot. With an experiment like this with two groups of samples, we hope to see two clusters forming separating these two groups, however as we’ve seen in the heatmap, sample KO1B showed a large deviation which we will also note using MDS. Note: all figures below are plotted using ggplot2, a more advanced method of plotting in R. While these plots are preferred over base-R plotting, it is always sufficient to use just that as it can be very challenging to alter the example code shown in this section. The data objects plotted are always shown and they usually contain simple X- and Y-coordinates. # Perform MDS using the &#39;cmdscale&#39; function. The resulting data can simply be # plotted using basic R plotting, here we will use &#39;ggplot2&#39; mdsData &lt;- data.frame(cmdscale(sampleDistMatrix)) pander(mdsData, caption = &#39;MDS coordinates used for plotting the distances&#39;) MDS coordinates used for plotting the distances X1 X2 KO1A -1.997 6.976 KO1B -17.98 0.3748 KO2 0.9181 4.117 KO3 -7.214 -7.154 WT1A 8.118 -7.903 WT1B 8.517 3.032 WT2 5.407 1.073 WT3 4.23 -0.5169 # Separate the annotation factor (as the variable name is used as label) (Variant &lt;- annotation$Variant) ## [1] KO KO KO KO WT WT WT WT ## Levels: KO WT library(ggplot2) ggplot(mdsData, aes(X1, X2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Euclidean Distance&#39;) As a demonstration of a different distance metric (instead of the default euclidean used in the dist function) the following code shows how to calculate a maybe more fitting distance called the Poisson Distance. This library was specifically designed to handle read count data. Note that again, the code below is only usable for count data as this function requires: &gt; A n-by-p data matrix with observations on the rows, and p features on the columns. The (i,j) element of x is the number of reads in observation i that mapped to feature (e.g. gene or exon) j`. library(&#39;PoiClaClu&#39;) # Use the raw (not r-log transformed) counts dds &lt;- assay(ddsMat) poisd &lt;- PoissonDistance( t(dds) ) # Extract the matrix with distances samplePoisDistMatrix &lt;- as.matrix(poisd$dd) # Calculate the MDS and get the X- and Y-coordinates (mdsPoisData &lt;- data.frame(cmdscale(samplePoisDistMatrix))) ## X1 X2 ## 1 -1703.5304 4797.3631 ## 2 -13785.4488 554.1001 ## 3 -247.7059 1076.6632 ## 4 -5286.3101 -4215.8790 ## 5 5481.0361 -4477.7972 ## 6 7416.4224 2436.2925 ## 7 4517.6335 969.1246 ## 8 3607.9032 -1139.8673 # Create the plot using ggplot ggplot(mdsPoisData, aes(X1, X2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Poisson Distance&#39;) Comparing both figures shows similar results, we clearly see a separation of both sample sets, however within the knockout (KO) group there is a large spread and if we were to apply a proper clustering technique, we would see clusters forming containing mixed samples. Once we have a set of genes identified as being differentially expressed we can repeat this step with the expectation of a more clear clustering. An alternative method of showing sample relations which is often preferred over MDS (but a bit harder to perform with pre-normalized data formats) is Principal Component Analysis (PCA). The code below shows PCA on our DESeqTransform object rld.dds. # Calculate PCA data data &lt;- plotPCA(rld.dds, intgroup = c(&#39;samples&#39;), returnData = TRUE) # Calculate the percentage of each principal component # Only used in the axis-labels percentVar &lt;- round(100 * attr(data, &#39;percentVar&#39;)) # Separate the annotation factor (as the variable name is used as label) Variant &lt;- annotation$Variant # Create the plot using ggplot ggplot(data, aes(PC1, PC2, color=Variant, label = coldata)) + geom_text(size=4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) Or on FPKM/RPKM data (only perform if you want to compare normalization techniques and have FPKM data. Output is not shown): # PCA done with the &#39;prcomp&#39; function in base-R. Replace &#39;rld&#39; with a matrix # containing the normalized data (genes as rows, samples as columns) pca.data &lt;- prcomp( t(rld) ) # Extract coordinates for PC1 and PC2 components &lt;- data.frame(mds.labels, pca.data$x[, 1:2]) pander(components) # Plot using ggplot2 ggplot(data = components, aes(PC1, PC2, color = Variant, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) "]
]
