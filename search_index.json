[
["index.html", "Analysis of Gene Expression Preface Capstone Project", " Analysis of Gene Expression Marcel Kempenaar 2018-05-27 Preface Capstone Project This project aims to teach the whole process of analysing a dataset containing gene expression data measured with the RNA-sequencing technique. The tools we will use to perform the analysis will be the statistical programming language R with its many libraries and the software package RStudio to interact with R. Rstudio is a so called Integrated Development Environment (IDE) which makes programming in R much easier. This course is organized in a number of chapters, each with the goal of understanding and performing one of the analysis steps to gain knowledge about both the biology of gene activity and the bioinformatics approach to answering a biological question given a data set. Demonstrating the understanding of these concepts is done through completing a number of small assignments and constant reflecting on results from the various analysis steps. "],
["chapter-1.html", " Introduction 1.1 Capstone Project - Analysis of Gene Expression 1.2 Project Deliverables 1.3 Project Schedule 1.4 Grading 1.5 Project Proposal 1.6 Lab Journal 1.7 Article", " Introduction 1.1 Capstone Project - Analysis of Gene Expression The use of RNA-Sequencing techniques for measuring gene expression is relatively new and replaces microarrays, though in some cases microarrays are still used. Gene expression data gives valuable insights into the workings of cells in certain conditions. Especially when comparing for instance healthy and diseased samples it can become clear which genes are causal or under influence of a specific condition. Finding the genes of interest (genes showing differing expression accross conditions, called the Differentially Expressed Genes (DEGs)) is the goal of this project. While there is no golden standard for analyzing RNA-sequencing datasets as there are many tools (all manufacturers of sequencing equiptment also deliver software packages) we will use R combined with proven libraries for processing, visualizing and analyzing publically available datasets. While in some cases you are allowed to use the actual raw data that is available, it is highly recommended to use the pre-processed data which often is a table with a count value for each gene. This count is the number of reads that was mapped to that gene which corresponds to the relative number of transcripts (mRNA sequences) of that gene present in the cell at the time of sampling. 1.2 Project Deliverables The end products of this course consist of two deliverables; a PDF output file from an RMarkdown ‘lab journal’ where you have logged all steps performed to get to the end result and a final report in the form of a short article. This chapter briefly describes the requirements and contents of these products and ends with instructions on how to use an RMarkdown template for writing the article. 1.3 Project Schedule The aim is to keep to the below schedule during this course. Use the first two weeks to see if you need to focus more on one of the points below (depending on your dataset) and discuss changes to the planning with your teacher. Find a public experiment of interest [week one] Using online resources (sections 2.1, 2.2) Data Gathering and Literature Research [week three] Make a final project choice Retrieve the accompanying publication Write and present a short project proposal (section 2.3) Starting with Exploratory Data Analysis (chapter 3) Data Analysis [weeks four and five] Finalizing Exploratory Data Analysis Discovering Differentially Expressed Genes (chapter 4) Data Annotation (optional; appendix a2) Techniques used: R with bioconductor, the EdgeR and/ or DESeq2 packages, RMarkdown Result Analysis [week six] Analyzing and Visualizing your results (chapter 5) Techniques used: clustering, pathway analysis, gene-enrichment analysis Finalizing analysis and start writing the final report (article) [weeks 7 and 8] 1.4 Grading The final grade consists of a weighted average of the work done for weeks 1 &amp; 2 (resulting in the project proposal) (15%), the lab journal containing all performed steps, their code and outputs (50%) and the article report (25%) and finally your work attitude (10%). A grade &gt;= 5.5 will give a total of 8 EC. The rest of this chapter explains the expected contents for the three graded products (each of these elements is explained in greater detail in other chapters). 1.5 Project Proposal As you are free to choose from thousands of public data sets on a large variety of biological subjects, it is important to demonstrate a good understanding of the experiment, biology and the available data. This is demonstrated by presenting a short project proposal where you briefly explain the subject of the chosen research article, the experimental setup (how many samples were used, etc.) and what types of data are available for you to use. 1.6 Lab Journal As you know from previous projects and most likely from working in the laboratory, it is essential to keep a proper lab journal detailing every step you have done during the experiment. Here, the log is to be kept in an R markdown file, showing which steps have been taken in the analysis of the data set. This markdown should be knitted into a single PDF-file once the project is completed thus containing text detailling the steps and any decisions you’ve made, R-code (always visible!) and their resulting output/ images. As a general advice; do not wait with knitting this whole document until the project is done! 1.7 Article The final report is written in article form which is a bit different from a usual report, mainly in its size. The article-report has a maximum number of pages of 4 including all images and references (no appendices!). Contents for this article should be extracted from the lab journal combined with part introduction and part conclusion/ discussion. The sections below describe a template that is available for writing this report and example report(s) will be made available for inspiration. Refer to this chapter again once you start writing the report. 1.7.1 Installing the Article Template The templates are available in an R package and contains both RMarkdown and (another layer on top of the actual markup language, yes, another language…) files. RStudio can use templates for a number of documents, including article-templates. These templates can be installed from a package called rticles by running the install.packages function (note: might already be installed): install.packages(&quot;rticles&quot;, type = &quot;source&quot;) 1.7.2 Using the Template Now that you have the templates package (rticles) installed you can download and use the template project (ZIP-file) available from the course website. Download this file to your project folder, extract its contents and open the report-template.Rmd file contained within the folder in RStudio. Verify that everything is setup correctly by hitting the Knit button at the top, this should create a PDF version of the report. Note that - somehow - the resulting PDF file is named RJwrapper.pdf instead of the expected report-template.pdf. If everything checks out ok you can rename the file to your liking and start editing. This template is based on the R Journal Submission template that you can also find in RStudio in the New file -&gt; R Markdown -&gt; From Template menu. Articles published in the R-journal are based on this template which you can browse for inspiration at the R Journal Website. The available template shows an example of segments/ chapters and briefly describes what each section could or should contain. If you want to write your report in the Dutch language, you can create a new file from the template and change the segment names and content to Dutch. There will however still be some headings added by the template in English which is fine with me, but if you want to keep everything Dutch you can either edit the RJournal.sty file manually (not recommended) or start a new file by yourself and add some nice headings and page options. The top part of the template (between the dashes ---) contains some settings that you need to change such as title, authors and abstract. Compare for instance a newly created article from this template with the one offered from the project-repository website. "],
["chapter-2.html", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest 2.2 The NCBI Gene Expression Omnibus 2.3 Creating a Project Proposal", " Discovering Data Sets 2.1 Finding Public Data Sets of Interest This chapter describes a method of finding public datasets of interest. A dataset in the context of this course refers to all data belonging to a certain gene-expression experiment, usually consisting of a number of sequencing-samples combined with meta-data describing the experiment. There are a number of very large databases online that offer access to thousands of - published - experiments and here we will focus on searching and downloading gene expression data from high-throughput (NGS) sequencing techniques (RNA-Seq). With thousands of freely available datasets it is possible to start performing research without the need of actually performing your own lab-experiment. For all common conditions, organisms and tissues you can download samples and compare them over multiple studies to find novel relations between genes and conditions or to verify experiments performed in your laboratory. For instance, the power of public datasets was demonstrated jointly by three of our alumni in an article called Calling genotypes from public RNA-sequencing data enables identification of genetic variants that affect gene-expression levels.(al. 2015). It’s method section begins with the sentence “We downloaded the raw reads for all available human RNA-seq datasets …” of which the amount of data and work will become clear later on. If you are interested in expression Quantitative Trait Loci (eQTLs) or Allele-Specific Expression (ASE) analysis, please read this very interesting paper. The following weeks you will be investigating one or more public RNA-sequencing datasets yourself and the rest of this document focuses on databases containing such public data sets and - especially - how to find a dataset or experiment you would like to use. But before we start diving into large and complex databases potentially containing thousands of experiments with millions of samples and terabytes of data, we need to get a rough idea on what we would like to do once we have found something of interest in order to know what we are looking for in the first place. While there is no definitive guide or protocol that can be followed for processing and analysing a gene-expression dataset, the following goals can be considered: Re-do (part of) the analysis described in the accompanying publication. As all datasets - except for the ones that were added very recently - are accompanied by a publication, you can gather a lot of information regarding experimental design and results from just this paper. Most often, the researchers already performed the most interesting research on this dataset and it is therefore a good exercise to try and reproduce their results. Challenges here are getting a proper understanding of the techniques used by the researchers (from the article) and translating these to your own analysis steps. Often though, not all details are clear from the article which requires your own interpretation of their results and performing analysis steps to arrive at the same conclusion. Alternatively you can come up with your own ideas and (biological) question(s) that you could try and answer given a dataset instead of redoing the published work. This is of course more challenging and not suitable for all datasets. You might notice that some publications only focus on a small set of genes instead of the whole transcriptome. For instance, when you perform an experiment on yeast where you want to measure the activity of genes involved in alcohol fermentation, researchers might only look at genes from the Glycolysis pathway. This leaves the other 6000+ yeast genes for you to explore and possibly come up with novel relations of gene expression and experimental setup. If the analysis approach explained in the paper is not focusing on a pre-selected list of genes, depending on the experiment you might come up with comparisons not done in the original research. For instance, in an experiment where the maternal age (at birth) is correlated to autism in their offspring, the paternal age is also known but not addressed in the publication. This could be a subject of further research (spoiler alert; no clear conclusion can be drawn from including this extra factor…). Another type of project is to evaluate different methods of either normalisation or statistical analysis to either confirm the published results or find novel genes involved in the experiment. This can be viewed more as a technical research subject which is ok to pursue, however the final report should mainly concern the biological impact of your findings. This means that when the accompanying publication shows very conclusive results, it will probably come down to acknowledging their results (therefore, you extend on point 1 from this list). Because with a very conclusive result you hope to find the same results and if this is not the case it might become hard to formulate a good conclusion in the end. However, if the publication isn’t very specific and only states a conclusion such as ‘Benzene exposure shows increased risk of leukemia’ followed by a list of a few genes that might be involved, you could try to see if you can find other genes that might be involved by changing the analysis approach. Although this last project goal has many risks involved and will need a very sound project proposal, it might result in the most interesting project. As shown before with the linked article, it is possible to combine data from multiple experiments. You could for instance find two very related experiments (i.e. researching the effect of a certain drug) both measuring expression in different tissue types (i.e. liver and brain tissue). After analyzing both experiments, you could present a set of genes that show an effect in both tissues and - more interestingly - genes showing an effect in only one tissue. The list above is a guideline to be kept in mind while browsing for suitable data sets and it will be extended in week three. Also, don’t worry if some of the terms above are unclear, getting a good understanding on the used terminology is part of these first few weeks 2.2 The NCBI Gene Expression Omnibus The easiest method of exploring and finding interesting datasets is by simply using your web-browser to access a data-repository. You could also download a few gigabytes of SQL database and query that if you’d like, but I would advice against that. One such repository is the NCBI Gene Expression Omnibus (GEO). This repository was primarily used to store microarray datasets and describe those experiments, linking to raw data, processed data and an accompanying publication. There are however many more sources of data browsable in the GEO, and for this project we will limit our searches to “Expression profiling by high throughput sequencing” for which almost 17.000 experiments are listed. The GEO Summary page shows all available data sources, click on the right Expression profiling link to get to the full table of relevant experiments. Besides finding an experiment that you are interested in, there are some further requirements that you need to account for when browsing: The experiment must be published and the publication must be fully available, free or otherwise through the Hanze University library. For each sample group, a minimum of three replicates must be present. If one or more groups have less then three replicates, that group cannot be used (the rest of the data might still be usable). The available data must contain at least the count data. Data is offered in multiple formats, always including the RAW data (reads), but also in further processed data such as FPKM, RPKM and TPM (see this interesting blog-post discussing the use of these data). The tools and R analysis libraries that we will use for the downstream analysis rely on unnormalized or unprocessed data which is the count data. These counts simply represent the number of reads mapped to a transcript. For manual analysis, these need to be normalized across samples (which has been done to get the FPKM, RPKM or TPM values) but will be done by the analysis software using the count data (unbiased). Once you’ve found an experiment, write a mail to m.kempenaar@pl.hanze.nl where you link your experiment. Once you receive a go-ahead you can start by reading the accompanying article and writing a project proposal. 2.3 Creating a Project Proposal In week three you are asked to present a short project proposal where you: Briefly explain the subject, methods, results and conclusion of the chosen project, describe the experimental design of the chosen experiment; how many samples and in how many groups are they divided (i.e. how many replicates per group)? Show what your plan is with the dataset, given the choices listed in the section about Finding Public Data. Note that whatever choice you make, the first steps will most likely be the same leading to finding the Differentially Expressed Genes (DEGs). It is very difficult to plan your complete project based on what you read in the article and it is not expected to get a planning for the full remaining 6 weeks. You can however specify a bit more given what you’ve read in the article, such as trying to reproduce a certain figure or repeat a certain analysis step. Summarize your project proposal on max. 1 A4 and, combined with the slides, are the final deliverables for this chapter (submission through the accompanying BlackBoard course). References "],
["chapter-3.html", " Exploratory Data Analysis 3.1 Exploring the Available Project Data 3.2 Loading data into R 3.3 Example Data 3.4 Visualizing using summary, boxplot, density plot, scatterplot &amp; MA-plot 3.5 Visualizing using heatmap, MDS &amp; PCA 3.6 Cleaning Data", " Exploratory Data Analysis We will be performing some exploratory data analysis with the goal of getting to grips with your chosen data set to properly identify a strategy for the actual analysis steps. During this exploration we will also keep an eye on the quality of the data. Even though the downloadable data is ‘processed’, there might be samples present that deviate too much from the other group of samples (a so called outlier). Creating basic visualizations of the data will give the necessary insight before we continue. We first start by downloading and loading in the actual count data. 3.1 Exploring the Available Project Data Whether you’ve found a dataset through the SRA or GEO, we want to get the data into R to start working with it. For now, we will only download the count data which is most likely stored in a TXT or XLS(X) file format: Data formats: TXT: simple text file containing a minimum of two columns (either tab or comma separated containing i.e. gene / transcript identifier and one of the above mentioned data types). However it can also contain up to 10 data columns either including more information regarding the gene/ transcript (i.e. gene ID, name, symbol, chromosome, start/ stop position, length, etc.) or more numerical columns (i.e. raw read count, normalized readcount, FPKM, etc.). XLS(X): Microsoft Excel file containing the same data columns as mentioned in the TXT files. Data types: Read Count (simple raw count of mapped reads to a certain gene or transcript). FPKM (Fragments Per Kilobase Million, F for Fragment, paired-end sequencing), RPKM (Reads Per Kilobase Million, R for Read, single-end sequencing), Note: if you want to use this, make sure that the raw data is actually single-end (should be stated in the article) TPM (Transcripts Per Kilobase Million, T for Transcript, technique independent), Layouts Either one or more files per sample with one of the above data types or One file containing the data types for all samples (with the samples as data columns in the file) Please watch the video and read the page found at the RNA-Seq blog regarding the meaning and calculation of the above mentioned expression data formats or a more technical document found at The farrago blog page. On GEO you can see what data might be availalbe in the Supplementary column, as shown below: Finding an experiment on GEO with a TXT file as supplementary data This overview on GEO contains many links which are not direct links to the items for that dataset, but can be used as filter for browsing the results. If you want to actually download the data, click on the GSE identifier (first column) which brings you to an overview for this experiment containing a lot of information about the experiment (subject, research institute, publication, design, etc.) and links to each individual sample (GSM identifier). Following the link to a sample shows information on how this sample was retrieved with often many (lab) protocols used. Sometimes there is a segment regarding “Data Processing” that refers to techniques and software used for the full analysis and might contain something like: … Differential expression testing between sample groups was performed in Rstudio (v. 1.0.36) using DESeq2 (v.1.14.1) … Back on the experiment overview page you’ll see a (variable) number of links to data files belonging to this experiment, see . For now we are only interested in the count-data which is stored in the TXT-file (see the column File type/resource) called GSE97406_RAW.tar. This file contains all the data that we need, even though it is only 220Kb in size where the experiment started with about 5Gb of read-data for a small bacteria: Finding the supplementary data in a GEO record As a bioinformatician we love to compress all the files so for this particular example, we download the tar-archive file, extract it to find a folder with another archive file for each sample. After extracting these files we end up with 12 TXT-files with just two columns; a gene identifier and a semi-raw count value (~4500 rows of data per sample): Table 3.1: Contents of a TXT-file for sample GSM2563998 Gene ID Count Value aaaD 0 aaaE 0 aaeR 50 aaeX 0 aas 118 abgB 21 abgR 56 abgT 0 abrB 11 accA 453 accB 2492 accC 1197 Other experiments combine their samples in a single file where each column represents a sample. If you do get an experiment with one file per sample, we can programmatically (yes, even in R) load this data in batch. 3.2 Loading data into R R works best with data in simple text formats. If your project data is offered as an Excel file it is therefore advised to open it in Excel (or OpenOffice Calc) and save/ export the file as a tab-separated text file, alternatively you can search for how to import an Excel file into R. Once you have one or more column based text files they can be read into R simply by using the read.table() function. Follow the following steps to read in the data and start the exploratory data analysis. The resulting document should be semi-treated as a lab journal where you log the process from loading the data to the final analysis steps. Open RStudio Create a new R Markdown document Give it a proper title and select the PDF format Give the document some structure; e.g. create a segment (using single hash #) called Exploratory Data Analysis for this week’s work. Whenever you add code to your document make sure that it is both readable (keep the maximum line length &lt; 100 if possible) and there is sufficient documentation either by text around the code ‘chunks’ or by using comments in the code chunk. Read in the data file(s) Use the read.table function and carefully set its arguments. Open the file in a text editor first to check its contents; does it have a header? can we set the row.names? Are all columns needed? etc. Note: if the data set consists of separate files (i.e. one per sample) or for general tips on reading in data, see the Appendix A: Batch Loading Expression Data chapter. For the remainder of the document, try to show either the contents, structure or - in this case - dimensions of relevant R objects Show the first five lines of the loaded data set. Including tables in a markdown document can be done using the pander function from the pander(Daróczi and Tsegelskyi 2017) R-library. Give the dimensions (with dim() and the structure (with str()) of the loaded data set. Check the output of the str function to see if all columns are of the expected R data type (e.g. values, factors, character, etc.) Examine the samples included in the experiment and create as many R character objects as needed to store the classification. Note: this is an important item and is often overlooked. See the last 4 lines in the code chunk in the section Example Data below. For instance, if you have eight samples divided into case/ control columns you create an object called case in which you store the column indices (only numbers!) of the respective columns in the data set and an object called control with the remaining four data column indices. These are for later use. These variables allows us to repeatedly access the same data during the rest of this course, i.e. the code boxplot(counts[case]) immediately shows that we are plotting the case samples from the counts data which is more clear than reading `boxplot() For some datasets the order in which the samples are listed in your loaded dataset is different from the order that is shown on the GEO website. Most often, the names are different too or they are lacking any description and all you have are the GSM IDs. When creating these variables such as control that need to point to all control samples, you need to make sure that you have the right columns from your dataframe. Go to Appendix A2; annotation if the order is unclear or you just have a large number of samples as there might be supporting data available that can help make sense of your sample layout. If you want to include including external images to your log and to better control properties such as height and width for individual images you can use the following code (requires the import of the png and grid libraries). Note: the code below is an example and you need to replace the ImageToInclude.png file reference to an actual image.): # Add the following to the code chunk header to control figure height and width: # {r, fig.height=5, fig.width=4, echo=FALSE} library(png) library(grid) img &lt;- readPNG(&quot;ImageToInclude.png&quot;) grid.raster(img) 3.3 Example Data This section lists all (publically available) dataset(s) used in this chapter. Each chapter contains this section if new datasets are used there. Note that for all examples, your data will be different from the examples and one of the challenges during this course will be translating the examples to your own data. Keep in mind that simple copy-pasting of most code will fail for that reason. Most examples therefore will print the input data for comparison to your own data. From the section Normalization onwards, the experiment with identifier GSE101942 is used for visualizing the normalized raw count values (the earlier sections explore the unnormalized data). This experiment is titled “Transcriptome analysis of genetically matched human induced pluripotent stem cells disomic or trisomic for chromosome 21” and the experimental setup is described as follows: “12 total polyA selected samples. 6 IPSC samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples. 6 IPSC derived neuronal samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples.” The following code shows how to load the two available files containing the raw-count data (one file per 6 samples), stored locally in the data/gse101942/ folder. Some simple ‘cleanup’ steps are performed which aren’t required but helpful for demonstration purposes (i.e., renaming samples to identify groups). GSE101942_IPSC_rawCounts.txt: raw count data for the induced pluripotent stem cells (iPSCs), both trisomic and disomic GSE101942_Neuron_rawCounts.txt: raw count data for the IPSC-derived Neurons, both trisomic and disomic. These cells were treated to remove chromosome 21 from the iPSCs, as described by the treatment protocol as: “Targeted removal of CHR 21 in IPSC using TKNEO transgene”. # Load the two sets of 6 samples ipsc.data &lt;- read.table(&#39;./data/gse101942/GSE101942_IPSC_rawCounts.txt&#39;, header = TRUE) neuron.data &lt;- read.table(&#39;./data/gse101942/GSE101942_Neuron_rawCounts.txt&#39;, header=TRUE) # Merge by rowname (by = 0, see the help of &#39;merge&#39;) counts &lt;- merge(ipsc.data, neuron.data, by = 0, all.x = TRUE, all.y = TRUE, sort = FALSE) # Change all NA&#39;s introduced by the merge to zeros counts[is.na(counts)] &lt;- 0 # Show column names coming from the input files print(names(counts)) ## [1] &quot;Row.names&quot; &quot;c244A&quot; &quot;c244B&quot; &quot;c243&quot; &quot;c2A&quot; ## [6] &quot;c2B&quot; &quot;c2C&quot; &quot;C3_1&quot; &quot;C3_2&quot; &quot;C3_4&quot; ## [11] &quot;C2_2_1&quot; &quot;C2_2_2&quot; &quot;C2_2_3&quot; # Set the row names to the gene IDs stored in the &#39;Row.names&#39; column row.names(counts) &lt;- counts$Row.names # Remove the gene ID column counts &lt;- counts[-1] # Rename samples to include their group name names(counts) &lt;- c(paste0(&#39;di_IPSC_r&#39;, 1:3), # Disomic IPSC, replicates 1-3 paste0(&#39;tri_IPSC_r&#39;, 1:3), # Trisomic IPSC, replicates 1-3 paste0(&#39;di_NEUR_r&#39;, 1:3), # Disomic Neuron, replicates 1-3 paste0(&#39;tri_NEUR_r&#39;, 1:3)) # Trisomic Neuron, replicates 1-3 # Show results of renaming the samples print(names(counts)) ## [1] &quot;di_IPSC_r1&quot; &quot;di_IPSC_r2&quot; &quot;di_IPSC_r3&quot; &quot;tri_IPSC_r1&quot; &quot;tri_IPSC_r2&quot; ## [6] &quot;tri_IPSC_r3&quot; &quot;di_NEUR_r1&quot; &quot;di_NEUR_r2&quot; &quot;di_NEUR_r3&quot; &quot;tri_NEUR_r1&quot; ## [11] &quot;tri_NEUR_r2&quot; &quot;tri_NEUR_r3&quot; # Assign column -indices- to variables for later use (chapter 4) di_IPSC &lt;- 1:3 tri_IPSC &lt;- 4:6 di_NEUR &lt;- 7:9 tri_NEUR &lt;- 10:12 See Table 1 in the Normalization section for the resulting contents in the counts dataframe. 3.4 Visualizing using summary, boxplot, density plot, scatterplot &amp; MA-plot This segment describes some of the basic steps and visualizations that can be performed during Exploratory Data Analysis. In this part we will work with the unnormalized count data. In part this data is normalized before analysing it further. As mentioned above, the focus of EDA is to get an overview/ perform a bit of Quality Control of the data set and while this often requires visualizing the data, these figures do not need to be very pretty. Simple figures are perfectly fine in this stage. Try to create these figures for your own data (and keep them in your log) and add a small description for each figure pointing out anything that is different from what you expect. Also note that these steps are just a selection. Furthermore, make sure that for every visualization you make, add proper axis-labels containing the measurement units (important!). As you might be able to see, all values are log-transformed using the log2 function because very often the numerical values have a very high range which will ‘hide’ the details on the plots. See the section about the Fold Change value in chapter 4 for further details. It is fine to use non-log-transformed (simply the raw-)data, otherwise use for instance boxplot(log2(dataset)) for plotting. Instead of using the basic R-plotting library (i.e. plot, boxplot, hist, etc.) you can also opt for using the (challenging) ggplot2 library that is also used for the boxplot, scatterplot and MA-plot figures in the following sections. While constructing a ggplot2 plot feels like learning yet another language, there are many resources available online that you can follow. 3.4.1 Statistics Even the most basic statistics can give some insight into the data such as performing a 6-number-statistic on the data columns using the summary() function. Note: you can also use the pander function to pretty-print a summary from a markdown document. What do you notice if you look at the numbers produced by executing this function on the complete data set? 3.4.2 Boxplots A visual representation of these values can be shown in a boxplot. Boxplots are very easy to create from an R data frame object by just passing in the data columns. The following boxplot shows the data for an experiment with a separate boxplot for each sample. This allows a quick overview for spotting irregularities (i.e. checking if the replicates within a sample-group show similar expression profiles). Of course, if we consider the amount of data in this single plot, it can only hint at any problems, we need to look in much more detail when doing any form of quality control. Creating a boxplot from a dataframe is easy, but as we saw with using the summary function; the data has a large range with the maximum and average values being very far apart. This will create a lot of outliers in the plot which will be interesting later on, but for the boxplot we can either: * hide them completely using the outliers = FALSE argument to boxplot() (do say so in the figure description!), or * perform a log2() transformation as you can see below. Boxplot comparing basic statistics for all genes across multiple samples 3.4.3 Density Plots Another form of visualizing the same data is using a density plot. This method shows a distribution of the (log2-transformed) count data for all samples and allows for easy spotting of problems. While this plot is more commonly used in analysing microarrays, it is still useful for comparing the complete dataset. The code and figure below show an example distribution for 12 samples. A few things to note about this figure is that there is a huge peak at exactly -3.321928 which can be ignored because this is value is calculated from log2(0.1). This value is very prevalent in the dataset and consists of all 0-values (inactive genes) that had 0.1 added as a so called pseudo count, see the box in the scatterplot section. Therefore, we added a vertical line to indicate the left-part is of little interest. Note: a lot of the code below is extra, for a simple inspection using only the line plotDensity(log2(data + 0.1)) is enough, the rest is extra example code. ## The affy library has a density plotting function library(affy) ## Create a list of 4 colors to use which are the same used throughout this chapter library(scales) myColors &lt;- hue_pal()(4) ## Plot the log2-transformed data with a 0.1 pseudocount plotDensity(log2(counts + 0.1), col=rep(myColors, each=3), lty=c(1:ncol(counts)), xlab=&#39;Log2(count)&#39;, main=&#39;Expression Distribution&#39;) ## Add a legend and vertical line legend(&#39;topright&#39;, names(counts), lty=c(1:ncol(counts)), col=rep(myColors, each=3)) abline(v=-1.5, lwd=1, col=&#39;red&#39;, lty=2) Density plot comparing count distribution for 12 samples 3.4.4 Scatterplots Scatterplots are good for spotting correlations between two sets of numerical data. Data that is similar will show up as a diagonal line in a scatter plot and the more they differ, the bigger the spread of points relative to the diagonal. Downside is that a scatterplot can only contain data from two samples (columns in your data set) and it might not always be useful to plot all samples from the condition group to all samples from the control group. Other possibilities include each sample against the average of its group (should be fairly diagonal), the averages of both groups in one scatterplot (this probably shows a fair amount of variation depending on the experiment). A scatterplot can be made simply using the plot() functions with the type argument set to p (points). To show the correlation more clearly you can add a linear regression line using abline(lm(y ~ x, col='red', lwd=2). If the regression line points more downwards from the diagonal, the values in sample x are higher and vice versa. Try to create the plot on non-log2 transformed data first and you’ll see that due to the range of the values most data is in a single black blob at the lower left corner (most of the probably 20.000+ data points) with just a few in the right side of the plot. This is not very informative, so place a simple log2() function call around the two samples and plot again. This introduces another problem once the linear regression line is added since the lm() function will complain when trying to calculate its coefficients etc. on data that contains negative infinity (-Inf) ‘numbers’. These are caused by all the 0-values in the data since log2(0) == -Inf. To circumvent this issue, we can add a pseudo count to the data by simply adding the value 1 to all count values since the log2(1) == 0 and lm won’t complain anymore; lm(log2(data[sample1] + 1) ~ log2(data[sample2] + 1)). There are more situations other than the lm() function where adding a pseudo count value to the complete dataset can be useful. Always add a pseudocount in-place, meaning within the plotting code instead of overwriting your original dataset with a pseudo count added as not all steps require this. In some cases (see the density plot section) it can be useful to clearly separate the 0-values from the rest in which case a 0 &lt; pseudo count &lt; 1 value, such as 0.1 can be used as this generates a negative value (-3.3) that is far away from the rest of the data. A simple scatterplot showing the correlation between two samples or experiments (see the text above). This image shows the values log2 transformed which in most cases gives a clearer picture. 3.4.5 MA-plot An MA-plot is a scatterplot comparing the average expression value per gene (A) against the log-fold-change (M; a log2 value indicating expression change across samples which is taken as the log2(sample1 / sample2))). The points in this plot should be centered around the horizontal 0-value with an even spread both above and below the line. If there are (far) more points above or below the 0 on the y-axis, there is a bias in the data as can be shown below. An MA-plot showing the average count values (A, x-axis) vs. the log2 fold-change (M, y-axis) with a very clear bias (data not centered around y=0 line). An MA-plot can be very helpful (though it has the same constraints mentioned with the scatterplot) to see if we need to perform data normalization. There are various online resources such as Wikipedia available on how to create this plot. Carefully remember how you should work with logarithms as these rules might be (are) confusing. An MA-plot showing the average count values (A, x-axis) vs. the log2 fold-change (M, y-axis). 3.5 Visualizing using heatmap, MDS &amp; PCA This section adds a few Exploratory Data Analysis techniques where we will measure and look at distances between samples. Measuring distances between two data objects (samples in our case) is a common task in cluster analysis to compare similarity (low distance indicates similar data). In this case we will calculate the distances between our samples and visualize them in a heatmap and using multidimensional scaling (MDS) techniques. 3.5.1 Normalization In the previous section we used the raw count data. You might have one or more samples that have different values (i.e. shifted) compared to other samples. While we need the raw count data to use R packages such as edgeR (Chen et al. 2018) and DESeq2 (Love, Anders, and Huber 2017), calculating sample distances (used in the visualizations in this section) should be done on some form of normalized data. This data can either be RPKM/FPKM/TPM/CPM or rlog-transformed (raw-)read counts. A proper method of transforming raw read count data is using the rlog method from the DESeq2 R Bioconductor library which is shown below. The following code examples shows how to use this library to normalize the count data from the GSE101942 experiment to rlog-normalized data before we calculate a distance metric. If you look at Table 1 you immediately see a huge difference between the groups for each of the genes. The gene (A1BG) shows &gt;100 more expression in the tri_NEUR group compared to the di_IPSC group, but within most groups there is a very high variation too indicating that it might not be the actual expression that is different. One very simple and quick method of inspection is looking at the total number of mapped reads per sample (the sum of each sample/ column), as the sequencing-depth might vary across samples. di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 tri_IPSC_r1 tri_IPSC_r2 tri_IPSC_r3 108 108 81 112 134 83 Table 1; Mapped reads per sample (millions) (continued below) di_NEUR_r1 di_NEUR_r2 di_NEUR_r3 tri_NEUR_r1 tri_NEUR_r2 tri_NEUR_r3 70 94 114 108 48 140 The numbers shown in Table 2 and the barplot above immediately clearify that it is not just the experimental condition that might have caused this large difference between sample counts but the sequencing depth shows a substantial difference too. Both the minimum and maximum sequencing depth are within the trisomic neuron group (purple in the barplot). The average expression of the A1BG gene within this group is 820, but with a minimum of 461 and a maximum of 1010 there is a lot of variation. This is one more reason tellins us that we will have to normalize the data. The following code chunks show how to do this with DESeq2’s rlog method. Check the Packages tab in Rstudio to see if you have the DESeq2 package installed and load it with the library command. To use the rlog function from the DESeq2 library we must contruct a DESeqDataSet-object consisting of the count data combined with sample annotation. Since we only want to use it (for now) for performing an rlog-transformation we use the most basic form with a very simple design and the sample names as annotation (the colData argument): # Load the library library(&#39;DESeq2&#39;) # DESeq2 will construct a SummarizedExperiment object and combine this # into a &#39;DESeqDataSet&#39; object. The &#39;design&#39; argument usually indicates the # experimental design using the condition(s) names as a &#39;factor&#39;, for now we use just &#39;~ 1&#39; (ddsMat &lt;- DESeqDataSetFromMatrix(countData = counts, colData = data.frame(samples = names(counts)), design = ~ 1)) ## class: DESeqDataSet ## dim: 56640 12 ## metadata(1): version ## assays(1): counts ## rownames(56640): 5S_rRNA 7SK ... C1orf220 C2orf15 ## rowData names(0): ## colnames(12): di_IPSC_r1 di_IPSC_r2 ... tri_NEUR_r2 tri_NEUR_r3 ## colData names(1): samples We now have a proper DESeqDataSet object as you can see above, containing 56640 rows and 12 columns (genes and samples) with the gene symbols as rownames. Usually this object would hold more data, but as this is only a requirement to perform the rlog transformation it is good enough for now. Next step is performing this transformation (this can take a while depending on the size of the experiment and results in a Large DESeqTransform object) and retrieving the actual data from this with the assay function as this object too contains a lot of meta-data. The table below shows the updated values which are now comparable across genes whereas the raw count data was harder to compare. # Perform normalization rld.dds &lt;- rlog(ddsMat) # &#39;Extract&#39; normalized values rld &lt;- assay(rld.dds) Quitting from lines 458-459 (gene_expression.Rmd) Error in head(rld[ipsc.di]) : object ‘ipsc.di’ not found Calls: … withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval -&gt; pander -&gt; head In addition: Warning messages: 1: In yaml.load(readLines(con), error.label = error.label, …) : R expressions in yaml.load will not be auto-evaluated by default in the near future 2: In yaml.load(readLines(con), error.label = error.label, …) : R expressions in yaml.load will not be auto-evaluated by default in the near future Note that when again looking at the A1BG gene, the normalized expression values show much less variation accross the samples (not shown, but also within each sample group). Therefore, we assume that the large difference in expression we observed earlier might be non-existent (there might still be a significant difference though!). 3.5.2 Distance Calculation We now have normalized data that we can use for distance calculation. This is a standard procedure for many data analysis tasks as it calculates a distance metric for each combination of samples that we will use to check for variation within the sample groups. We first need to transpose the matrix (rld) of normalized values using the t-function, because the dist function expects the different samples as rows and the genes as columns. Note that the output matrix is symmetric. Table 3 below shows the calculated distances within the disomic IPSC group where the distance between samples varies from 100 to 140. The maximum distance between any samples is 380 as will be demonstrated with the visualisations in the next sections. # Calculate basic distance metric (using euclidean distance, see &#39;?dist&#39;) sampledists &lt;- dist( t( rld )) Quitting from lines 473-475 (gene_expression.Rmd) Error in pander(.dist.mat[ipsc.di, 1:3], caption = “Table 3; Sample distances for the disomic IPSCs (Euclidean method)”) : object ‘ipsc.di’ not found Calls: … withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval -&gt; pander 3.5.3 Sample Distances using a Heatmap If you have both (raw-)count data and an other normalized format (TPM, RPKM, etc.), you can optionally follow the above procedure for your count data and create a heatmap for both formats to see if this makes any difference. The reason for this is that while the RNA-Seq method exists for over 10 years, there are still ongoing discussions on the subject of data processing, especially regarding subjects like which data format to use for which data analysis. The following code block creates a heatmap using the pheatmap library which offers on of the many available heatmap functions. Using the annotation dataframe (you can inspect the contents of it yourself) we identify the samples based on both the cell type and the ploidy. The clustering shown in the heatmap clearly separates the data based on the cell type and the differences between the ploidy seems to be minimal. Looking further still, the differences within a single group are minimal too meaning that we are not - yet - inclined to remove any outlier-samples. Since we have only have 3 samples per category, we would also lose statistical power if we eventually were to remove one or more samples (also, always check the article to see if they did remove any samples prior to the data analysis). # We use the &#39;pheatmap&#39; library (install with install.packages(&#39;pheatmap&#39;)) library(pheatmap) # Convert the &#39;dist&#39; object into a matrix for creating a heatmap sampleDistMatrix &lt;- as.matrix(sampledists) # The annotation is an extra layer that will be plotted above the heatmap columns # indicating the cell type annotation &lt;- data.frame(Cell = factor(rep(1:2, each = 6), labels = c(&quot;IPSC&quot;, &quot;Neuron&quot;)), Ploidy = factor(rep(rep(1:2, each = 3), 2), labels = c(&quot;disomic&quot;, &quot;trisomic&quot;))) # Set the rownames of the annotation dataframe to the sample names (required) rownames(annotation) &lt;- names(counts) pheatmap(sampleDistMatrix, show_colnames = FALSE, annotation_col = annotation, clustering_distance_rows = sampledists, clustering_distance_cols = sampledists, main = &quot;Euclidean Sample Distances&quot;) The resulting heatmap shows an interesting comparison across all samples where the order of samples is purely determined using the distance and is often different from the order in the dataset. Ideally, we want the sample groups to be clustered together as we see with this data. This particular heatmap clearly shows a large difference in overal expression between the induced pluripotent stem cells (iPSCs) and their differentiated cortical neurons while the difference - within cell type - between disomic and trisomic is much lower. 3.5.4 Multi-Dimensional Scaling The following code example shows how to perform Multi-Dimensional Scaling (MDS) that displays the previously calculated distances in a 2D-plot. With an experiment like this with two groups of samples, we hope to see two clearly separated clusters formed, however as we’ve seen in the heatmap, sample KO1B showed a large deviation which we will also see (confirm) this using MDS. All figures below are plotted using ggplot2, a more advanced method of plotting in R. While these plots are preferred over base-R plotting, it is always sufficient to use just that as it can be very challenging to alter the example code shown in this section. The data objects plotted are always shown and they usually contain simple X- and Y-coordinates. # Perform MDS using the &#39;cmdscale&#39; function. The resulting data can simply be # plotted using basic R plotting, here we will use &#39;ggplot2&#39; sampleDistMatrix &lt;- as.matrix(sampledists) coldata &lt;- names(counts) mdsData &lt;- data.frame(cmdscale(sampleDistMatrix)) names(mdsData) &lt;- c(&#39;x_coord&#39;, &#39;y_coord&#39;) x_coord y_coord di_IPSC_r1 -170 -17.73 di_IPSC_r2 -165.4 -8.864 di_IPSC_r3 -159.2 3.549 tri_IPSC_r1 -154.4 3.331 tri_IPSC_r2 -158.3 7.435 tri_IPSC_r3 -153.9 17.22 di_NEUR_r1 157.1 -40.19 di_NEUR_r2 154.1 -11.08 di_NEUR_r3 162.8 -78.36 tri_NEUR_r1 152 67 tri_NEUR_r2 159.8 -57.87 tri_NEUR_r3 175.4 115.5 Table 4: MDS coordinates used for plotting the distances # Separate the annotation factor (as the variable name is used as label) groups &lt;- factor(rep(1:4, each=3), labels = c(&quot;di_IPSC&quot;, &quot;tri_IPSC&quot;, &quot;di_NEUR&quot;, &quot;tri_NEUR&quot;)) # Load the ggplot2 library library(ggplot2) ggplot(mdsData, aes(x_coord, y_coord, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Euclidean Distance&#39;) + labs(x = &quot;Euclidean distance&quot;, y = &quot;Euclidean distance&quot;) + theme_bw() As a demonstration of a different distance metric (instead of the default euclidean used in the dist function) the following code shows how to calculate a maybe more fitting distance called the Poisson Distance. This library was specifically designed to handle read count data. Note that again, the code below is only usable for count data as this function requires: &gt; An n-by-p data matrix with observations on the rows, and p features on the columns. The (i,j) element of x is the number of reads in observation i that mapped to feature (e.g. gene or exon) j`. library(&#39;PoiClaClu&#39;) # Use the raw (not r-log transformed!) counts dds &lt;- assay(ddsMat) poisd &lt;- PoissonDistance( t(dds) ) # Extract the matrix with distances samplePoisDistMatrix &lt;- as.matrix(poisd$dd) # Calculate the MDS and get the X- and Y-coordinates mdsPoisData &lt;- data.frame(cmdscale(samplePoisDistMatrix)) # And set some better readable names for columns and rows names(mdsPoisData) &lt;- c(&#39;x_coord&#39;, &#39;y_coord&#39;) row.names(mdsPoisData) &lt;- row.names(mdsData) x_coord y_coord di_IPSC_r1 -21963 -3867 di_IPSC_r2 -22000 -97.21 di_IPSC_r3 -19007 -171.1 tri_IPSC_r1 -19889 1223 tri_IPSC_r2 -21693 1384 tri_IPSC_r3 -18354 2443 di_NEUR_r1 17162 -1041 di_NEUR_r2 17466 17.54 di_NEUR_r3 22619 -11120 tri_NEUR_r1 19313 6855 tri_NEUR_r2 16192 -978.6 tri_NEUR_r3 30154 5353 Table 5: MDS coordinates used for plotting the distances using Poisson Distance # Create the plot using ggplot ggplot(mdsPoisData, aes(x_coord, y_coord, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling - Poisson Distance&#39;) + labs(x = &quot;Euclidean distance&quot;, y = &quot;Euclidean distance&quot;) + theme_bw() Comparing both figures show similar results, we clearly see a separation on cell type and a less clear separation on ploidy. The sample di_NEUR_r3 shows a large difference in the non-normalized MDS plot, though in the normalized plot it is close to the other replicates. Once we have a set of genes identified as being differentially expressed we can repeat this step with the expectation of a more clear clustering. 3.5.5 Principal Component Analysis An alternative method of showing sample relations which is often preferred over MDS (but a bit harder to perform with pre-normalized data formats) is Principal Component Analysis (PCA). The code below shows PCA on our DESeqTransform object rld.dds. The resulting plot shows a very clear separation between cell types and within the cell types a separation between disomic and trisomic. library(BiocGenerics) # Calculate PCA data data &lt;- plotPCA(rld.dds, intgroup = c(&#39;samples&#39;), returnData = TRUE) # Calculate the percentage of each principal component # Only used in the axis-labels percentVar &lt;- round(100 * attr(data, &#39;percentVar&#39;)) # Create the plot using ggplot ggplot(data, aes(PC1, PC2, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + theme_bw() Or on FPKM/RPKM data (only perform if you want to compare normalization techniques and have FPKM data. Output is not shown): # PCA done with the &#39;prcomp&#39; function in base-R. Replace &#39;rld&#39; with a matrix # containing the normalized data (genes as rows, samples as columns) pca.data &lt;- prcomp( t(rld) ) # Extract coordinates for PC1 and PC2 components &lt;- data.frame(pca.data$x[, 1:2]) # Plot using ggplot2 ggplot(data = components, aes(PC1, PC2, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&quot;Principal Component Analysis for RNA-Seq data&quot;) 3.6 Cleaning Data Conclude this chapter by cleaning your dataset if the visualizations show the necessity for this. Cleaning in this case means removing complete samples if they can be classified as an outlier within its group. As with most tasks during this course, there is no clear advice on when to decide to remove one or more samples. The only clear rule is that each group must retain at least three samples. If one of the three samples visibly deviates from the other replicates, even after normalization, it will stay in your data set and it should be mentioned in your analysis report/ final article that noise may be introduced by this sample. Note: Once you are satisfied with the data and reported on your findings, we do not return to any of these steps in the next chapters. If there are mentions of creating a heatmap for instance, this means a heatmap of expression data instead of the sample distances we’ve shown in this chapter. References "],
["chapter-4.html", " Discovering Differentialy Expressed Genes (DEGs) 4.1 Pre-processing 4.2 The Fold Change Value 4.3 Using Manual Methods (t-test &amp; ANOVA) 4.4 Multiple Testing Correction 4.5 Using Bioconductor Packages", " Discovering Differentialy Expressed Genes (DEGs) The first and most important ‘real’ analysis step we will do is finding genes that show a difference in expression between sample groups; the differentially expressed genes (DEGs). The concept might sound rather simple; calculate the ratios for all genes between samples to determine the fold-change (FC) denoting the factor of change in expression between groups. Then, filter out only those genes that actually show a difference. While this does give a list of genes showing different behaviour across samples, we need to focus on genes that do not only show a difference, but are also statistically significant! To determine whether or not a gene can be classified as a significant DEG we are going to use - at least - two techniques: We will start with manually performing statistical test(s) and compare these results from those given by using one or more R (Bioconductor) libraries (mostly edgeR and DESeq2) 4.1 Pre-processing Given the results of the exploratory data analysis performed in chapter 3, you might have concluded that there are one or more samples that show (very) deviating expression patterns compared to samples from the same group. As mentioned before, if you have more then enough (&gt; 3) samples in a group, you might opt to remove a sample to reduce the noise as the statistical tests are very sensitive to this. Since we are performing all analysis steps programatically it is also very easy to test for DEGs with and without the sample(s) in question and see if the removal results in lower p-values (= higher significance). If this is the case you can continue on without those sample(s). As always, be sure to properly document this, including the reason why you chose to remove them. Another step we need to take - and this might be guided by your article - is to filter out (partially) inactive genes. Most datasets available contains a lot of 0-measurements, transcripts where no reads have mapped. In the R-studio Environment tab, click on your dataset (or perform the View(data) command and click on one of the sample columns to order the data ascending. You will now most likely (unless you have bacterial data for instance) see a lot of zero values in all columns. In an experiment with two groups, three replicates each, if three out of those six samples have 0-reads mapped, it is often adviced to remove the gene completely. But this can be very subjective to the experiment, it might be expected (thus important) when comparing different tissues or knockout experiments. Also, genes with a (very) low read count (&lt; 5) can give a very high (artificial) FC value (see the lefthand side of an MA-plot). Comparing two samples where one has a value of 2 and the other 11, this reads as an up-regulated gene by a factor of 5.5 while it might actually just be noise! Assignment: Search through your article for any advice on how to filter out zero values or low count genes. If there is nothing stated on this subject, think of your own tactic (or search the literature/ online!). It is perfectly fine to discuss with your peers. describe what you will be doing for this aspect; if you do not filter your data, clearly explain why not (most likely because the article stated a proper reason) Perform the filtering on your dataset. For this you will most likely need to use one of the apply functions, combined with maybe the which, all and any functions. Data transformations/ normalizations are often available from functions too, such as the cpm function from the edgeR library. Manually verify that the rows removed were correctly filtered. Properly document how many genes have been filtered out! 4.2 The Fold Change Value The FC is usually given as the calculated log2 of the case/control ratio. For example, gene A has an average expression of 30 mapped reads in the control group and 88 reads in the experiment group, the ratio case/control is 2.93. Ratio values &gt; 1 indicate increased expression in the experiment in relation to the control and values between 0 and 1 indicate lower expression. The log2 transformed value of the ratio is calculated with log2(2.93) and results in 1.55. If the counts were reversed, the ratio would have been 30/88, which is 0.34. The previously calculated value of 2.93 means a 3-fold up regulation while the 0.34 value means 3-fold down regulation but you can see the range of numbers is very different. Comparing log2 values this would be 1.55 and -1.55 which compares much better. While it is very easy to calculate the FC for all genes at once, a simple FC value doesn’t mean much, yet! We still need to use the power that lie within the replicates we have for each sample group. Using these replicates, we want to determine if the observed FC is not just biological noise or a sequencing error. Assignment: Create a histogram of your log2-FC values for all genes. Apply the following steps on your normalized data to do this for at least one comparison (no need to do it for all possible combinations): average the replicates for each group and add this as a new column to your data set, calculate the ratios; simply divide one group (averaged-column) by another, usually experiment/control, perform a log2-transformation (now you have the log2-FC values), plot the data using the hist(logFC_column, breaks=60) function (change the breaks argument if needed), Add two vertical lines at -1 and 1 (using abline(v=...)) to indicate some significance (2-fold change). If a calculated FC shows a large change in expression between groups this means nothing if the variation within a group is very high. For this reason we use some form of statistical test that checks both the variation in each group and the difference between groups of samples. In the simplest form this usually comes down to using a t-test: “It can be used to determine if two sets of data are significantly different from each other” It does however depend on your experimental setup when it comes down to deciding the proper statistical test to perform and for that it is best to look at your experiment article to see what the authors used for method. Most often though, the article only mentions the use of one or more R libraries of other software packages for finding DEGs. In this case you can find the appropriate method by determining the possible combinations to test. The output of finding DEG’s always includes - but is not limited to - a list of p-values; usually one p-value per gene. This value indicates whether that gene is a statistically-significant differentially expressed gene (SDEG) and to find these genes of interest all we need to do is get all genes with a p-value below our threshold (i.e. 0.05). Because we will get at least two p-values per gene (one found using manually statistical testing and one through the use of a library) we can make multiple selections, i.e. compare methods, find genes that have a low p-value in both methods and visualize these results in a venn-diagram (see section visualization). 4.3 Using Manual Methods (t-test &amp; ANOVA) Performing the statistical tests yourself consists of performing either a t-test or an ANOVA based analysis. Both of these methods have multiple forms and the one to choose fully depends on what question you’d like to ask. Again, refer to your experimental setup and any hints found in the article or formulate your own question and base the decision on that. Manually performing a t-test or ANOVA on gene expression opposed to using one of the specialized libraries can have advantages since every gene is processed individually, while edgeR and DESeq2 look at all genes and this might ‘smooth’ the results which is not always wanted. We will find out later if and how this affects your data set. If your data only consists of the read counts there is an extra normalisation step to perform (only for the manual methods explained in this section, keep the count data stored as well!). One generally accepted method of normalizing count data is to calculate the fragments per million mapped fragments (FPM) value and then transform this with log2. Opposed to FPKM and RPKM this does not include the gene-length in its calculation (which you most likely don’t have) but as said before we apply the test per gene and do not need to compare multiple genes. If your dataset also includes FPKM or RPKM (pre-normalized), you are allowed to use this data too. Always clearly document if you did so! Once you performed a manual statistical test, be sure to read the section on Multiple Testing Correction further on in this chapter. # Perform FPM normalization using DESeq2 &#39;fpm&#39; and perform log2 transformation # The &#39;ddsMat&#39; object is the &#39;DESeqDataSet&#39; created in chapter 3; EDA. # NOTE: only for count data and observe the pseudocount of 1 (remove if data does not contain 0-values) counts.fpm &lt;- log2( fpm(ddsMat, robust = TRUE) + 1 ) Next is selecting the test to perform, the following links show diagrams that can be used once the experimental design is known: What test to use links: Institute for Digital Research and Education Biochemia - image at the bottom of the article PracticallyScience These examples may include terms that are unclear. If so, please ask or perform a standard internet search as these terms are explained in multiple tutorials about performing these widely used statistical tests. Assignment: Think of a question (i.e. a comparison between two or more sample groups) to ask the data and perform a statistical test to find the relevant genes for this question. Briefly explain this comparison (maybe it was the one resulting in the most interesting results listed in the article) and introduce the test that you are going to perform. Report on the number of significant genes before and after Multiple Testing Correction for each comparison that has been done. 4.3.1 Students T-test In order to test if one or more genes are significantly differentially expressed between two conditions one can perform a t-test. The t-test will test the null hypothesis that there is no difference between the mean of the two populations. Usually, if the p-value is below the significance threshold chosen (also called alpha-value, usually set at \\(\\alpha\\) 0.05) you reject the null hypothesis and conclude that there is a significant difference between the means. Note that it is necessary to perform a single t-test for every gene (row) of your data set, including all replicates for the involved group. This means that you need to either use one of the apply() set of functions or find a different method (there is one specifically for this problem) to make sure that you test each gene separately. From the output of the t-test you only need to keep the calculated p-value; one value per gene (row) in your data set. In R, you can append this as a column to your dataframe/ matrix containing the normalized count values. ## Note: Single-gene only example ## Convert to simple numeric vectors, used for the variance test ipsc.exp &lt;- as.numeric(counts.fpm[&quot;KCTD14&quot;, di_IPSC]) neur.exp &lt;- as.numeric(counts.fpm[&quot;KCTD14&quot;, di_NEUR]) di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 di_NEUR_r1 di_NEUR_r2 di_NEUR_r3 4.39 4.36 4.456 1.102 1.083 1.058 Data for t-test, di_IPSC (case, 3 samples) vs di_NEUR (control, 3 samples), “KCTD14” gene First, we test (F-test) if the variance (the average of the squared differences from the mean) in the two sample groups is equal as we need to supply the t-test with this information (by default, it assumes different variance). Note that before performing an F-test, it should first be validated that the data is normally distributed, i.e. using a Shapiro-Wilk test. ## Test for equal variance (H0 = &#39;equal variance&#39;) variance &lt;- var.test(ipsc.exp, neur.exp, alternative = &#39;two.sided&#39;) Test statistic num df denom df P value Alternative hypothesis ratio of variances 5.03 2 2 0.3317 two.sided 5.03 F test to compare two variances: ipsc.exp and neur.exp In this case, it seems that the variance does not differ significantly (P-value of 0.3317 &gt; 0.05) between the two sample groups, therefore set the var.equal argument in the t.test function to TRUE. Most experiments are unpaired; meaning that the expression for a replicate in group A is from a different subject/ person then the expression for a replicate in group B. If however you have time-series data where the same subjects/ persons are used in multiple sample groups, you should set the paired option in the t.test function to TRUE. In most other situations, keep this setting to its default; FALSE. test &lt;- t.test(ipsc.exp, neur.exp, alternative = &#39;two.sided&#39;, var.equal = TRUE) The following table shows the numeric values from performing a t-test. Here we see that this gene is identified as a DEG because its \\(\\alpha\\) &lt; 0.05. Since the t-test compares the mean of two groups, the difference between these two values (4.4 for the ‘IPSC’ group and 1.08 for the ‘NEURON’ group) are deemed significant. Do note that ‘noise’ (often a large variance within a group of replicates) has a lot of effect on the p-value. Test statistic df P value Alternative hypothesis mean of x mean of y 106.5 4 4.665e-08 * * * two.sided 4.402 1.081 Two Sample t-test: ipsc.exp and neur.exp Again, we are only interested in the p-value for each gene, store this in either the original data frame or a new one where it is coupled to the gene identifier (name, symbol, etc.). t.test tutorials: M. A. Noback, DAVuR Course Syllabus Quick-R @ statmethods Whitehead Institute: based on microarray analysis, but it’s still expression data and the basics are the same. 4.3.2 Analysis of Variance (ANOVA) If you have decided that you need to perform ANOVA analysis, the experimental design as well as the question to ask become much more important. Usually it is required to perform ANOVA instead of a t-test when there are multiple (&gt; 2) conditions involved or it is a time series experiment. It is impossible to compare more than two conditions using a t-test unless using a divide-and-conquer approach (comparing all conditions against each other) but this is advised against since it will introduce Type I errors (false-positives). Experiments where conditions are combined (i.e. age + multiple drugs) can also be analysed using ANOVA where the question can either be the overall effect of a drug or the influence of age on the effect of a drug. The method of specifying these questions uses the same formula notation as for example the linear model (lm) function, i.e. ~ group + treatment + group:treatment where group could be an age group and treatment a different drug. The most simple form of this formula is ~ condition. Note that in this case condition is most likely an R factor dividing the samples in case/ control groups. Let us first consider a theoretical example and work towards a practical example in R. Given an experiment with \\(k\\) conditions as shown in the table below where each gene \\(i\\) is measured \\(n_i\\) times. \\(Condition\\) 1 2 … i … \\(k\\) \\(X_{11}\\) \\(X_{21}\\) … \\(X_{i1}\\) … \\(X_{k1}\\) \\(X_{12}\\) \\(X_{22}\\) … \\(X_{i2}\\) … \\(X_{k2}\\) \\(\\vdots\\) \\(\\vdots\\) … \\(\\vdots\\) … \\(\\vdots\\) \\(X_{1n_{1}}\\) \\(X_{2n_{2}}\\) … \\(X_{in_{i}}\\) … \\(X_{kn_{k}}\\) A typical question to ask is whether there are any differences between the expression level \\(X\\) of the given gene between these conditions. However we need to carefully formulate the null and research hypotheses. The null hypothesis is pretty easy; the different conditions are not really different and, therefore, all measurements actually come from a single distribution. The effect is that all means would be the same, for all conditions: \\(H_0 : \\mu_{1} = \\mu_{2} = \\cdots = \\mu_{k}\\). It is not clear however whether the alternate or research hypothesis requires all \\(k\\) conditions or a subset of these conditions to be different from each other. This results in the following three possible hypotheses for \\(H_a\\): \\(H_a\\) : All means are different for each other. \\(H_a\\) : Several but not all means are different from each other. \\(H_a\\) : There is at least one pair of means that are different from each other. The first two hyptheses are not suitable for the question asked thus the following set of hypotheses remain: \\(H_0\\): \\(\\mu_{1} = \\mu_{2} = \\cdots = \\mu_{k}\\) \\(H_a\\): There is at least one pair of means that are different from each other. This set of hypotheses is called a Model I or fixed effects, ANOVA. The general idea behind this form of ANOVA is very simple. The measurements of each condition vary around the condition-mean (within-group variance). At the same time, the means of each condition will vary around an overall mean (inter-group variance). The idea behind ANOVA is to study the relationship between the inter-group and the within-group variances. The following practical example uses the airway experiment data package in R which contains RNA-Seq read count data for 8 samples. From the abstract, a brief description of the RNA-Seq experiment on airway smooth muscle (ASM) cell lines(Himes BE 2014): Using RNA-Seq, a high-throughput sequencing method, we characterized transcriptomic changes in four primary human ASM cell lines that were treated with dexamethasone - a potent synthetic glucocorticoid (1 micromolar for 18 hours). The data available in the experiment package is of a certain class (Large RangedSummarizedExperiment) and contains both the count data as well as some metadata describing the experiment. Looking at the sample description table below, we can see a few columns describing variables that are used to define the experiment. In this case these are the cell (an identifier for a certain cell where each cell originates from the same subject) and dex (trt indicates treatment of each cell with 1 \\(\\mu M\\) dexamethasone, untrt indicates control) columns. library(airway) data(airway) ## Sample description colData(airway) SampleName cell dex albut Run avgLength Experiment Sample BioSample SRR1039508 GSM1275862 N61311 untrt untrt SRR1039508 126 SRX384345 SRS508568 SAMN02422669 SRR1039509 GSM1275863 N61311 trt untrt SRR1039509 126 SRX384346 SRS508567 SAMN02422675 SRR1039512 GSM1275866 N052611 untrt untrt SRR1039512 126 SRX384349 SRS508571 SAMN02422678 SRR1039513 GSM1275867 N052611 trt untrt SRR1039513 87 SRX384350 SRS508572 SAMN02422670 SRR1039516 GSM1275870 N080611 untrt untrt SRR1039516 120 SRX384353 SRS508575 SAMN02422682 SRR1039517 GSM1275871 N080611 trt untrt SRR1039517 126 SRX384354 SRS508576 SAMN02422673 SRR1039520 GSM1275874 N061011 untrt untrt SRR1039520 101 SRX384357 SRS508579 SAMN02422683 SRR1039521 GSM1275875 N061011 trt untrt SRR1039521 98 SRX384358 SRS508580 SAMN02422677 Sample metadata for the airway experiment ## Count data head(assay(airway)) SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 ENSG00000000003 679 448 873 408 1138 1047 770 572 ENSG00000000005 0 0 0 0 0 0 0 0 ENSG00000000419 467 515 621 365 587 799 417 508 ENSG00000000457 260 211 263 164 245 331 233 229 ENSG00000000460 60 55 40 35 78 63 76 60 ENSG00000000938 0 0 2 0 1 0 0 0 Count data for the airway experiment Two questions resulting in two different formulas can be asked with this data set, on a per-gene basis: Test for the effect of dexamethasone (the last factor), controlling for the effect of different cell line (the first factor): ~ cell + dex Test whether a fold change due to treatment with dexamethasone is different across cell line using an interaction term: ~ cell + dex + cell:dex Using a t-test we can only ask either the effect of dex or the cell line and not both. Note that for this experiment the paired argument for a t-test should be set to TRUE as each cell is used in both treated and untreated conditions. Using ANOVA we will now, for a single gene, show how to get the basic statistics. This can be done in two ways; either fit a model (using lm) and compute the analysis of variance tables (using anova) or using the wrapper function (aov) that models the data using lm from which the statistics can be gathered using the summary function. First we need to determine if we have a balanced or unbalanced design, i.e. do we have equal sample sizes within the levels of our independent grouping levels (independent variables are dex and cell, the count value is the dependent variable). See the code below on how to check for a balanced design. Note that all further code for this section only uses count data for one gene (the first one). ## First, format the data so we can use it in a formula. We need the samples as rows, ## and the gene expression values in a single column dex = colData(airway)$dex cell = colData(airway)$cell dat &lt;- data.frame(count=t(assay(airway))[,1], # Count data dex, cell) # Group description ## Check for design. If all values are equal, the design is balanced pander(table(dat$dex, dat$cell)) N052611 N061011 N080611 N61311 trt 1 1 1 1 untrt 1 1 1 1 pander(dat, caption = &quot;Count data for &#39;ENSG00000000003&#39;&quot;) count dex cell SRR1039508 679 untrt N61311 SRR1039509 448 trt N61311 SRR1039512 873 untrt N052611 SRR1039513 408 trt N052611 SRR1039516 1138 untrt N080611 SRR1039517 1047 trt N080611 SRR1039520 770 untrt N061011 SRR1039521 572 trt N061011 Count data for ‘ENSG00000000003’ ## Average per treatment group pander(tapply(dat$count, dat$dex, mean)) trt untrt 618.8 865 There is a noticable (~40%) difference in the average expression when looking at the treatment. Looking at the difference in average for the cells we see an even bigger difference. pander(tapply(dat$count, dat$cell, mean)) N052611 N061011 N080611 N61311 640.5 671 1092 563.5 ## Create a model for the first question listed above ## By using two independent variables, this is a TWO-way ANOVA model.airway &lt;- lm(count ~ cell + dex, data=dat) ## Alternative method, same result: # summary( aov(count ~ cell + dex, data=dat) ) ## Print the analysis of variance table pander(anova(model.airway)) Df Sum Sq Mean Sq F value Pr(&gt;F) cell 3 340111 113370 9.129 0.05108 dex 1 121278 121278 9.765 0.05227 Residuals 3 37257 12419 NA NA Analysis of Variance Table For this gene, both the treatment and the cell have an almost significant p-value (Pr(&gt;F) &lt; 0.05). With the treatment having only two options we can simply state that the treatment shows some effect, but we have to accept the \\(H_0\\) hypothesis (all means are equal) since \\(\\alpha\\) &gt; 0.05. Let’s assume for now that we found a gene with an \\(\\alpha\\) &lt; 0.05 where we reject \\(H_0\\). With there being 4 different cell sources, we can only say that at least one cell-type shows an effect. To find out exactly which of the cell sources that is requires multiple t-tests (\\(R = 1/2k(k-1)=6\\) separate tests comparing all groups). Alternatively, the pairwise.t.test function can be used for this. After performing ANOVA some steps should be performed to check if the results are viable, tests like the Levene’s test for checking the homogeneity of variances and checking for normality of the residuals. While this is a technical solution and fully understanding falls outside of the scope of this course, a simple plot can be made (not for all genes/ tests) to see if the residuals are normally distributed. We expect the datapoints to approximately follow a straight diagonal line. Samples that differ too much are annotated in the plot and can be reason for concern. ## Plotting the full model (remove the 2) shows other statistics as well plot(model.airway, 2) ANOVA tutorials M. A. Noback, DAVuR Course Syllabus Quick-R @ statmethods R-bloggers Statistical tools for high-throughput data analysis 4.4 Multiple Testing Correction Read the text about Multiple comparisons at the biostathandbook and the help of the p.adjust R function. Your article is a possible source to see which correction method they applied. If this is not mentioned there it is probably best to use the fdr (also called the Benjamini Hochberg) method. An expected side-effect of correcting for multiple-testing is the lower number of genes with a resulting p-value &lt; 0.05 (caused by hopefuly removing false-positives). For example, using a p-value of 0.05 expects 5% of all tests to result in false-positives. Thus when testing &gt;20.000 genes it is expected that 1000 tests are in fact a false-positive. In some cases though, you end up with 0 genes that have a low p-value and thus you have no DEGs. Using the Bonferroni correction for instance, multiplies the p-values by the number of comparisons. With 20.000 comparisons, a p-value must be &lt;= 0.0025 to ‘survive’ the correction and still be &lt; 0.05. One ‘solution’ is to increase the alpha-value from 0.05 to 0.1, this is also often found in articles where DEGs are determined with a combination of p-value &lt; 0.05 and p-adjusted &lt; 0.1. 4.4.1 Inspecting Manual Test Results After applying a form of multiple testing correction, you end up with two p-values for each gene in your data set. DEGs can then be filtered out using simple R commands looking for adjusted p-values &lt; 0.05 and optionally a threshold log-FC value. The resulting set of genes show a statistically significant difference between the sample groups that were used in the test(s) and these can be used to answer any relevant biological question. For now, keep the results as we will inspect them closer in chapter 5; Data Analysis. 4.5 Using Bioconductor Packages This section demonstrates the use of three packages to perform DEG-analysis on count data. There are many packages available on Bioconductor for similar analysis, such as DSS, EBSeq, NOISeq and BaySeq, but here we will focus on edgeR, DESeq2 and limma for processing count-based data. Chances are that either of these three packages are mentioned if the article described the use of R for the statistical analysis as they are the most widely accepted methods of processing gene expression data. All of the three packages apply their own normalization methods (described in the sections below) therefore they only work on the count data. You can choose one of these three packages to use, or use them all since it could increase statistical power and they are not very hard to use once you understand how to model the data as we’ll show next. Assignment: Perform data-analysis using at least one of the two packages. If possible, use both of them so we can compare the results between the manual-testing results and both packages. Start by documenting the relevant information from the article, also if they used other software for this analysis. The two listed packages come with a very complete and extensive manual. These manuals can be found as PDF on the BioConductor website, or opened from within R using the vignette function. To see all available manuals (note; these are very different from the function-help documentation) execute the function without arguments: vignette(). This lists the vignettes for edgeR and DESeq2 as: * vignette(&quot;DESeq2&quot;) (this whole course is partly designed on this document!) * vignette(&quot;edgeR&quot;) 4.5.1 The Design (matrix) For all of these packages you need to properly specify how your samples are grouped. We have seen examples of this using an R factor object with the heatmap, MDS and PCA visualizations to tell which groups of samples we have and to which group each sample belongs. Reading the documentation for the below packages shows that this is an important part of performing DE-analysis. For example, the following code is shown in de edgeR documentation on page 8 where two sample groups are defined (numbered 1 and 2), placed in a factor object and used as input in the model.matrix function. group &lt;- factor(c(1,1,2,2), labels = c(&quot;Control&quot;, &quot;Case&quot;)) (design &lt;- model.matrix( ~ group)) ## (Intercept) groupCase ## 1 1 0 ## 2 1 0 ## 3 1 1 ## 4 1 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$group ## [1] &quot;contr.treatment&quot; As mentioned before we need to think about the question we want to aks; which difference do we want to know? With two sample groups as used here the question is rather easy; ‘which genes show an effect between case/ control samples?’. With more then two sample groups however the question becomes more difficult. If we have an experiment comparing influence of three kinds of drugs (thus three groups) combined with effect over time, do we then want to focus on the influence of the drugs or the time? Both are valid questions but they define the way of how to create the design matrix. Documentation on this subject is plenty, however it often contains overwhelming information. This page contains some valuable details (you can safely start reading at Choice of design), including the following text which is based on the same example used in the edgeR documentation: “For the examples we cover here, we use linear models to make comparisons between different groups. Hence, the design matrices that we ultimately work with will have at least two columns: an intercept column, which consists of a column of 1’s, and a second column, which specifies which samples are in a second group. In this case, two coefficients are fit in the linear model: the intercept, which represents the population average of the first group, and a second coefficient, which represents the difference between the population averages of the second group and the first group. The latter is typically the coefficient we are interested in when we are performing statistical tests: we want to know if their is a difference between the two groups.” If you have more then two sample groups and you want to change the question (i.e. test the influence of a different group), read the section about Releveling the factor. 4.5.2 DESeq2 We have used the DESeq2 library before and for DEG analysis we could re-use the DESeqDataSet object but it is adviced to create a new object with the proper design formula instead of the ~ 1 we used before. There is no need to normalize the data using the previously used rlog function because the DESeq2 library will normalize the count data for you as follows: “DESeq computes a scaling factor for a given sample by computing the median of the ratio, for each gene, of its read count over its geometric mean across all samples. It then uses the assumption that most genes are not DE and uses this median of ratios to obtain the scaling factor associated with this sample.” For our example data with four sample groups we re-use the annotation dataframe as created in the EDA: Heatmap section (printed below). This allows us to do comparisons based on the celltype and ploidy of the samples. One minor note of importance is that the first level in each factor is taken as the reference, or for expression analysis, the control group. In our example (see the table below) the IPSC is the control-celltype and disomic the control-ploidy. In a simple case/ control study, the first level (check with the levels() function) should be the control group. Use the relevel() function if the levels need to be switched. Cell Ploidy di_IPSC_r1 IPSC disomic di_IPSC_r2 IPSC disomic di_IPSC_r3 IPSC disomic tri_IPSC_r1 IPSC trisomic tri_IPSC_r2 IPSC trisomic tri_IPSC_r3 IPSC trisomic di_NEUR_r1 Neuron disomic di_NEUR_r2 Neuron disomic di_NEUR_r3 Neuron disomic tri_NEUR_r1 Neuron trisomic tri_NEUR_r2 Neuron trisomic tri_NEUR_r3 Neuron trisomic Once you have a proper DESeqDataSet all you need to do is run the DESeq function on this object. Then, using the results function you can extract the DEGs as a DESeqResults object. Applying the summary function on these object(s) shows the number of up and down regulated DEGs as can be seen below that lists the impact of trisomy for the IPSC cell type (~800 DEGs) and comparing both the IPSC and Neuron disomic (control) cell types (as can be seen, a huge amount of genes are affected by differentiation from stem cell into cortical neurons): IPSC - Trisomic vs Disomic Disomic Neuron vs Disomic IPSC out of 44504 with nonzero total read count adjusted p-value 0 (up) : 432, 0.97% LFC out of 44504 with nonzero total read count adjusted p-value 0 (up) : 9594, 22% LFC The output of the results function contains the following columns for each gene: Column Type Description baseMean intermediate mean of normalized counts for all samples log2FoldChange results log2 fold change (MLE): group IPSC.trisomic vs IPSC.disomic lfcSE results standard error: group IPSC.trisomic vs IPSC.disomic stat results Wald statistic: group IPSC.trisomic vs IPSC.disomic pvalue results Wald test p-value: group IPSC.trisomic vs IPSC.disomic padj results BH adjusted p-values How you call the results function depends heavily on your experiment. As you can see from the output of the summary function, there are no details given about which comparison is shown (and also, by default a p-value of 0.1 is used instead of 0.05). Depending on the design used to create the DESeqDataSet with, one or more comparisons can be made (applying the DESeq function calculates all and you filter with the results function). Read the help for the results function carefully; especially regarding the contrast argument where you define the comparison to retrieve. The following code can be used for our example experiment to get the DEGs comparing the celltypes using an adjusted p-value of 0.05: res &lt;- results(dds, contrast = c(&quot;Cell&quot;, &quot;IPSC&quot;, &quot;Neuron&quot;), alpha = 0.05) The DESeq2 library contains a number of plotting functions that can be applied to a DESeqResults object (output of the results function), the most notable is the plotMA function. Note that the first MA-plot shown below has a very high range for the log fold changes (-10, 10) where the maximum value is 22.4 (shown as a triangle stating it is outside of the plotting range). A log fold change of 22 means &gt;5 million increased expression which seems artificially high. DESeq2 includes a function to perform downstream processing of the estimated log fold change values called lfcShrink which is adviced to always run afterwards. The reason for executing this function is described in the vignette with: “It is more useful visualize the MA-plot for the shrunken log2 fold changes, which remove the noise associated with log2 fold changes from low count genes without requiring arbitrary filtering thresholds.” Links Analyzing RNA-seq data with DESeq2 A very comprehensive guide to analyzing RNA-Seq data using DESeq2 (part of this document has been used in this material too!). It is adviced to read the first few sections of this guide and take a good look at the index of the document because there are many interesting sections that might be of help later. Publication, an accompanying article showing differences in performance compared to other methods and packages. 4.5.3 edgeR One of the most mature libraries for RNA-Seq data analysis is the edgeR library available on Bioconductor. There is a very complete (sometimes a bit complex) manual available of which you need to read Chapter 2 with a focus on 2.1 to 2.7, 2.9 and - if you have a more complex design - 2.10. Section 1.4 (Quickstart) shows a code example on the steps needed to do DEG analysis using count data using the two glm methods (quasi-likelihood F-tests and likelihood ratio tests). All the steps shown there are identical for the non-glm method up to calculating the fit object which can be replaced by performing the exactTest function as shown in section 2.9.2. The edgeR library will normalize the count data for you as follows: “The trimmed means of M values (TMM) from Robinson and Oshlack, which is implemented in edgeR, computes a scaling factor between two experiments by using the weighted average of the subset of genes after excluding genes that exhibit high average read counts and genes that have large differences in expression.” Running edgeR requires the raw count data together with the grouping-factor packaged in a DGEList object (with the DGEList() function). Furthermore, a proper model.matrix object (see the section on design) is needed as input for the estimateDisp function. The exact steps to take (there are more variations than with DESeq2) must be searched in the documentation linked above. TODO: Gene annotation has been added to the DGEList object that is used to run edgeR with the genes parameter. This data has been taken from the AnnotationDbi package as shown in Appendix B. explain BCV-plot Once the analysis is done you can retrieve the actual results with the topTags function: genes logFC logCPM F PValue FDR PCDHA10 PCDHA10 -3.602 5.676 499.9 8.164e-11 1.354e-06 CHGA CHGA 2.923 5.976 185.4 1.972e-08 0.0001635 ARRB1 ARRB1 -3.914 5.015 158.3 4.627e-08 0.0002019 TSSC2 TSSC2 3.175 3.301 156.8 4.869e-08 0.0002019 LRFN5 LRFN5 -4.638 4.118 144.9 7.435e-08 0.0002237 C11orf54 C11orf54 -1.953 4.915 139.9 8.985e-08 0.0002237 MAP2 MAP2 2.647 9.468 138.6 9.439e-08 0.0002237 WNT3 WNT3 -3.488 3.273 106.7 3.742e-07 0.0007758 TFAP2C TFAP2C -2.984 4.591 96.48 6.321e-07 0.001165 ASAP2 ASAP2 1.352 5.876 94.25 7.133e-07 0.001183 Most significant genes given by edgeR The package also contains a few plotting methods that you can use at intermediate steps during the analysis. For instance, after calculating the normalization factors (calcNormFactors), you can perform multi-dimensional scaling with the plotMDS function: plotMDS(y) Or the dispersion after running the estimateDisp function with the plotBCV function: plotBCV(y) Or the log-fold changes for all genes, once we have the output of the exactTest function (output et is an DGEExact object) with the plotSmear function. The abline shows a log-FC threshold: deGenes &lt;- decideTestsDGE(qlf, p=0.05) deGenes &lt;- rownames(qlf)[as.logical(deGenes)] plotSmear(qlf, de.tags=deGenes) abline(h=c(-1, 1), col=2) Links Differential Expression Analysis using edgeR tutorial Another tutorial hosted on GitHub References "],
["a1-batch-data-loading.html", "A Appendix A: Batch Loading Expression Data in R A.1 Decompressing A.2 Determining Data Format A.3 Loading Data", " A Appendix A: Batch Loading Expression Data in R This code example shows how to batch-load multiple files containing expression (count) data for a single sample. The data for this example can be found on GEO with ID GSE109798. Downloading the data for this experiment from GEO gives us a single .tar file called GSE109798_RAW.tar. Extracting this archive file nets us a folder with the following files: file.names &lt;- list.files(&#39;./data/GSE109798_RAW/&#39;) Files GSM2970149_4T1E274.isoforms.results.txt GSM2970150_4T1E266.isoforms.results.txt GSM2970151_4T1E247D.isoforms.results.txt GSM2970152_4T1P2247A.isoforms.results.txt GSM2970153_4T1P2247G.isoforms.results.txt GSM2970154_4T1P2247F.isoforms.results.txt GSM2970155_HCC1806E224B.isoforms.results.txt GSM2970156_HCC1806E224A.isoforms.results.txt GSM2970157_HCC1806E224C.isoforms.results.txt GSM2970158_HCC1806P2232A.isoforms.results.txt GSM2970159_HCC1806P2232B.isoforms.results.txt GSM2970160_HCC1806P2230.isoforms.results.txt A.1 Decompressing The file extension of all these files is .txt.gz which means that all files are compressed using gzip and need to be unpacked before they can be loaded. The easiest method is using the system gunzip command on all files which can be done from within R by applying the gunzip command using the system function on each file. ## Change directory to where the files are stored setwd(&#39;./data/GSE109798_RAW/&#39;) sapply(file.names, FUN = function(file.name) { system(paste(&quot;gunzip&quot;, file.name)) }) Now we can update the file.names variable since each file name has changed. file.names &lt;- list.files(&#39;./data/GSE109798_RAW/&#39;) A.2 Determining Data Format Next, we can inspect what the contents are of these files, assuming that they all have the same layout/ column names etc. to decide what we need to use for our analysis. ## Call the system &#39;head&#39; tool to &#39;peek&#39; inside the file system(paste0(&quot;head &quot;, &quot;./data/GSE109798_RAW/&quot;, file.names[1])) transcript_id gene_id length effective_length expected_count TPM FPKM IsoPct uc007aet.1 1 3608 3608.00 1.82 0.44 0.24 100.00 uc007aeu.1 1 3634 3634.00 0.00 0.00 0.00 0.00 uc011whv.1 10 26 26.00 0.00 0.00 0.00 0.00 uc007amd.1 100 1823 1823.00 0.00 0.00 0.00 0.00 uc007ame.1 100 4355 4355.00 1.32 0.26 0.15 100.00 uc007dac.1 1000 1403 1403.00 2.00 1.25 0.70 100.00 uc008ajp.1 10000 1078 1078.00 0.00 0.00 0.00 0.00 uc012ajs.1 10000 1753 1753.00 0.00 0.00 0.00 0.00 uc008ajq.1 10001 2046 2046.00 0.00 0.00 0.00 0.00 These files contain (much) more then just a count value for each gene as we can see columns such as (transcript) length, TPM, FPKM, etc. Also, the count-column is called expected_count which raises a few questions as well. The expected count value is usable as it contains more information - compared to the raw count - then we actually require. The expected part results from multimapped reads where a single read mapped to multiple positions in the genome. As each transcript originates only from one location, this multimapped read is usually discarded. With the expected count though, instead of discarding the read completely it is estimated where it originates from and this is added as a fraction to the count value. So the value of 1.32 that we see on line 5 in the example above means an true count of 1 (uniquely mapped read) and the .32 (the estimated part) results from an algorithm and can mean multiple things. As mentioned before, we require integer count data for use with packages such as DESeq2 and edgeR and there are two methods to convert the expected count to raw count data: + round the value to the nearest integer (widely accepted method and is well within the expected sampling variation), or + discard the fraction part by using for example the floor() function. A.3 Loading Data From all these columns we want to keep the transcript_id and expected_count columns and ignore the rest (we might be interested in this data later on though). As we need to lead each file separately we can define a function that reads in the data, keeping the columns of interest and returning a dataframe with this data. Note that the first line of each file is used as a header, but check before setting the header argument to TRUE, sometimes the expression data starts at line 1. The file name is then also used to name the column in the dataframe so that we know which column is which sample. This is done by splitting the file name (using strsplit) using the dot (‘.’) keeping the first part (i.e. ‘GSM2970156_HCC1806E224A’) and discarding the second part (‘isoforms.results.txt’). The strsplit function however always returns a list, in this case containing a vector with the 5 splitted elements: ## String splitting in R ## (the fixed = TRUE is required as the dot is a special character, see &#39;?strsplit&#39;) strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE) ## [[1]] ## [1] &quot;GSM2970155_HCC1806E224B&quot; &quot;isoforms&quot; ## [3] &quot;results&quot; &quot;txt&quot; ## [5] &quot;gz&quot; ## Keeping the sample identifier strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE)[[1]][1] ## [1] &quot;GSM2970155_HCC1806E224B&quot; ## Function for reading in files read_sample &lt;- function(file.name) { ## Extract the sample name for naming the column sample.name &lt;- strsplit(file.name, &quot;.&quot;, fixed = TRUE)[[1]][1] ## Read the data, setting the &#39;transcript_id&#39; as row.names (column 1) sample &lt;- read.table(file.name, header = TRUE, sep=&quot;\\t&quot;, row.names = NULL) ## Rename the count column names(sample)[5] &lt;- sample.name ## Return a subset containing the &#39;transcript_id&#39; and sample name columns return(sample[c(1, 5)]) } Applying the read_sample function to all file names gives us a set of data frames that we can merge together using the merge function. We merge the data based on the transcript id defined with the by = 1 argument pointing to the first column. We start by reading in just one file which is the ‘base’ dataframe to which we will merge the other files. During processing it seemed that this data set is divided into two groups which is also listed on the GEO website for this project: GPL11154 Illumina HiSeq 2000 (Homo sapiens) GPL13112 Illumina HiSeq 2000 (Mus musculus) where the first 6 files are from human source and the last 6 from the mouse. Therefore, the following code only shows how to read the first 6 samples and merge these into a single dataframe. Repeating this process for the other 6 files would result into another dataframe for those samples. setwd(&#39;./data/GSE109798_RAW/&#39;) ## Read the FIRST sample dataset &lt;- read_sample(file.names[1]) ## Read first sample group (6) for (file.name in file.names[2:6]) { sample &lt;- read_sample(file.name) dataset &lt;- merge(dataset, sample, by = 1) } pander(head(dataset)) transcript_id GSM2970149_4T1E274 GSM2970150_4T1E266 GSM2970151_4T1E247D uc007aet.1 1.82 0 0 uc007aeu.1 0 1 0.24 uc007aev.1 0 0 0 uc007aew.1 0 0 0.97 uc007aex.2 0 0 0 uc007aey.1 0 0 0 Table continues below GSM2970152_4T1P2247A GSM2970153_4T1P2247G GSM2970154_4T1P2247F 0 0 0 0.13 0 0 0 0 0 0 1 0 0 0 0 0 0 0 The dataset variable now contains all data for the first 6 samples in this experiment. It is advisable to compare the number of rows in this data set with the number of rows in a single sample. It is not guaranteed that all samples have exactly the same number of genes/transcripts present (i.e., 0-values might have been discarded) which results in a final data set that has as many rows as the smallest sample. See the help of merge if this is the case because the all argument can be used to introduce extra rows for missing data. "],
["a2-annotation.html", "B Annotating an RNA-Seq Experiment B.1 Downloading annotation data from GEO B.2 Manual Data Annotation B.3 References", " B Annotating an RNA-Seq Experiment This chapter describes annotate the data, meaning assigning names and functions to our Differentially Expressed Genes. This step can either be very easy or a bit more challenging depending on the data source of your project. The first example is relevant if your data set originated from the NCBI GEO, which ‘should’ always be annotated by default. If you have found your data set elsewhere (and do not have a GEO idetifier for your project), skip to the Manually annotate your data section below. The goal of this chapter is to - at least - find a Gene Symbol or common ID (NCBI/ Ensembl) for each gene. Using this information it will be much easier to find relevant information from other online sources to say something about the functionality and impact of your DEGs. Even though you might be lucky and either already have your data or a simple GEO query results in everything you want/ need, you are tasked to also perform the manual annotation phase even if you will not use any of the found annotation data. Having written the code to access these sources is a good excercise in using complex Bioconductor packages. Also, you can always use this to find more information should you require this later on. B.1 Downloading annotation data from GEO Given the GSE**** id of your experiment you can download any available (annotation)data from GEO using the GEOquery library. For the example experiment used previously, the following code downloads two files; a series_matrix.txt.gz file and a .soft file that are both also available for download from the NCBI GEO website. Note the destdir argument for storing it in a known location instead of the /tmp folder for later use. library(GEOquery) gse.id &lt;- &#39;GSE101942&#39; gse &lt;- getGEO(gse.id, destdir = &quot;./data/&quot;, getGPL = TRUE) Once downloaded, you can load the data later using: library(GEOquery) gse &lt;- getGEO(filename = &#39;./data/GSE101942_series_matrix.txt.gz&#39;) # Sample details gpl &lt;- getGEO(filename = &#39;./data/GPL11154.soft&#39;) # Platform details # Print some metadata of the gse object gse ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 0 features, 12 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: GSM2719212 GSM2719213 ... GSM2719223 (12 total) ## varLabels: title geo_accession ... ploidy:ch1 (50 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: GPL11154 # And the structure of the gpl object str(gpl) ## Formal class &#39;GPL&#39; [package &quot;GEOquery&quot;] with 2 slots ## ..@ dataTable:Formal class &#39;GEODataTable&#39; [package &quot;GEOquery&quot;] with 2 slots ## .. .. ..@ columns:&#39;data.frame&#39;: 0 obs. of 2 variables: ## .. .. .. ..$ Column : chr(0) ## .. .. .. ..$ Description: Factor w/ 0 levels: ## .. .. ..@ table :&#39;data.frame&#39;: 0 obs. of 0 variables ## ..@ header :List of 14 ## .. ..$ contact_country : chr &quot;USA&quot; ## .. ..$ contact_name : chr &quot;,,GEO&quot; ## .. ..$ data_row_count : chr &quot;0&quot; ## .. ..$ distribution : chr &quot;virtual&quot; ## .. ..$ geo_accession : chr &quot;GPL11154&quot; ## .. ..$ last_update_date: chr &quot;May 16 2018&quot; ## .. ..$ organism : chr &quot;Homo sapiens&quot; ## .. ..$ sample_id : chr [1:91613] &quot;GSM616127&quot; &quot;GSM616128&quot; &quot;GSM616129&quot; &quot;GSM663427&quot; ... ## .. ..$ series_id : chr [1:6258] &quot;GSE16256&quot; &quot;GSE16368&quot; &quot;GSE17312&quot; &quot;GSE18927&quot; ... ## .. ..$ status : chr &quot;Public on Nov 02 2010&quot; ## .. ..$ submission_date : chr &quot;Nov 02 2010&quot; ## .. ..$ taxid : chr &quot;9606&quot; ## .. ..$ technology : chr &quot;high-throughput sequencing&quot; ## .. ..$ title : chr &quot;Illumina HiSeq 2000 (Homo sapiens)&quot; The gse object may contain useful information about both samples and genes. Unfortunately for this example dataset there is only information available regarding the samples which we can access in the phenoData slot. Slots in R objects are a method of storing multiple data objects into a single R object, in this case this object is called an ExpressionSet (see ?ExpressionSet for a description of its contents and structure). For storing information about an experiment this is very handy; you can store a dataframe with the actual expression data, a list containing laboratory information, a chunk of text with an abstract, etc. all in a single object. The most common datasets found in an ExpressionSet object: * @assayData: the actual expression data. Most common for microarray experiments and (very) rare for RNA-seq experiments * @phenoData: sample information, should be present for each GEO dataset * @featureData: a dataframe holding feature (=gene) information, slightly rare for RNA-seq datasets * @experimentData: information about the lab which performed the experiment You already know how to access a column in a data frame using the $ notation, and accessing a complete dataframe in the gse object is done using the @ symbol: # Access a data frame containing phenotype information gse@phenoData@data Here you see that each sample is described using its sample identifier (GSM****), information about the the sample group (title column) and a number of characteristics. The actual information included depends on your dataset. title source_name_ch1 characteristics_ch1 GSM2719212 C2A trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719213 C2B trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719214 C2C trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719215 C244-A disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719216 C244-B disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719217 C243 disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) Contents of the gse@phenoData@data information (selection) (continued below) characteristics_ch1.1 GSM2719212 ploidy: trisomic GSM2719213 ploidy: trisomic GSM2719214 ploidy: trisomic GSM2719215 ploidy: disomic GSM2719216 ploidy: disomic GSM2719217 ploidy: disomic If your dataset has properly annotated gene information, this should be accessible in the @featureData slot. For instance, the dataset GSE20489 does have this annotation available. Once loaded, we see that the following annotation columns are present for each gene in the featureData slot (following is a small subset, in total there can be well over 20 columns of information per gene). Also, the metadata listing shows for how many features (=genes, 54675) and samples (54) the expression data is present (in the @assayData slot): # This gets a list with a single &#39;ExpressionSet&#39; object GSE20489 &lt;- getGEO(&quot;GSE20489&quot;, destdir = &#39;../data&#39;) # Print some metadata of the featureData slot GSE20489[[1]] ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 54675 features, 54 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: GSM514737 GSM514738 ... GSM514790 (54 total) ## varLabels: title geo_accession ... tissue:ch1 (39 total) ## varMetadata: labelDescription ## featureData ## featureNames: 1007_s_at 1053_at ... AFFX-TrpnX-M_at (54675 ## total) ## fvarLabels: ID GB_ACC ... Gene Ontology Molecular Function (16 ## total) ## fvarMetadata: Column Description labelDescription ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: GPL570 ID Gene Title Gene Symbol ENTREZ_GENE_ID 1007_s_at discoidin domain receptor tyrosine kinase … DDR1 /// MIR4640 780 /// 100616237 1053_at replication factor C (activator 1) 2, 40kDa RFC2 5982 117_at heat shock 70kDa protein 6 (HSP70B’) HSPA6 3310 121_at paired box 8 PAX8 7849 1255_g_at guanylate cyclase activator 1A (retina) GUCA1A 2978 1294_at microRNA 5193 /// ubiquitin-like modifier … MIR5193 /// UBA7 7318 /// 100847079 Gene information stored in an ExpressionSet object for experiment GSE20489 (continued below) Gene Ontology Biological Process 0001558 // regulation of cell growth // in… 0000278 // mitotic cell cycle // traceable… 0000902 // cell morphogenesis // inferred … 0001655 // urogenital system development /… 0007165 // signal transduction // non-trac… 0006464 // cellular protein modification p… B.2 Manual Data Annotation Luckely for us, R offers a number of libraries to automatically retrieve information for large sets of genes, unless you have chosen an organism for which not much data is available. This section demonstrates the use of two such libraries, starting with AnnotationDbi followed by biomaRt. The AnnotationDbi library downloads a local copy of an organism-specific database with gene information where biomaRt uses online databases to retrieve data given a query. biomaRt offers far more data (over 1000 data fields per organism) but is more complex to use. Since biomaRt is also relying on online databases it might be a good strategy to annotate only the genes of interest (the DEGs) instead of querying for &gt;20.000 genes while we might only retain 20 after statistical analysis. If you are planning to use biomaRt, skip to the Discovering Differentialy Expressed Genes (DEGs) chapter first and then return to the Using R Bioconductors biomaRt section to annotate the data. B.2.1 Using AnnotationDBI The AnnotationDbi offers data sets for many organisms in the form of installable libraries and depending on your experiment you need to find the proper library. The example experiment contains samples of the house mouse (Mus musculus) and therefore we select its data set. # Load the AnnotationDbi interface library library(AnnotationDbi) # Load the Bioconductor installation library (contains &#39;biocLite()&#39;) library(BiocInstaller) # Install and load the organism specific gene database # &#39;org&#39; for Organism # &#39;Hs&#39; for Homo sapiens # &#39;eg&#39; for Entrez Gene IDs # (try to load (using the library function) first before installing, it might already be present) biocLite(&#39;org.Hs.eg.db&#39;) library(org.Hs.eg.db) The following information types are available in this database (use the columns function to inspect). Columns Columns Columns Columns Columns ACCNUM ENZYME IPI PFAM UNIGENE ALIAS EVIDENCE MAP PMID UNIPROT ENSEMBL EVIDENCEALL OMIM PROSITE ENSEMBLPROT GENENAME ONTOLOGY REFSEQ ENSEMBLTRANS GO ONTOLOGYALL SYMBOL ENTREZID GOALL PATH UCSCKG Available fields in the database The table below shows the data available for all information types given a randomly chosen EntrezID of ‘1080’. Note that the table has been split to show the 24 data types with their values. Data Type Example Value Data Type Example Value ACCNUM AAA35680 IPI IPI00815998 ALIAS ABC35 MAP 7q31.2 ENSEMBL ENSG00000001626 OMIM 167800 ENSEMBLPROT NA ONTOLOGY MF ENSEMBLTRANS NA ONTOLOGYALL BP ENTREZID NA PATH 02010 ENZYME 3.6.3.49 PFAM PF14396 EVIDENCE IDA PMID 1284466 EVIDENCEALL ISS PROSITE PS50893 GENENAME cystic fibrosis transmembrane conductance regulator REFSEQ NM_000492 GO GO:0005254 SYMBOL CFTR GOALL GO:0000003 UCSCKG uc064hkb.1 Example values for each field in ‘org.Hs.eg.db’ Retrieving data from the locally stored annotation database can be done using the mapIds function which takes a number of arguments: x: the local database to query keys: the IDs from your own data set, most often you have ENSEMBL IDs or gene SYMBOLS. In the example data the rownames of the data set contain the gene SYMBOLS (see Table 1 on page 2) column: the data column to retrieve from the database keytype: the type of data that you provide. In the example data this is a gene SYMBOL multiVals: a number of columns contain more then one entry for a single gene, in that case we only want to store the first one. The output of the mapIds function is a single character vector that we can add to our data frame containing the DEGs (in this case the DEGs for the IPSC trisomic vs disomic comparison containing 828 genes stored in the ipsc.tri.vs.di data frame). # Retrieve the ENSEMBL gene ID, e.g. &#39;ENSG00000001626&#39; ipsc.tri.vs.di$Ensembl &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENSEMBL&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) # Retrieve the ENTREZ gene ID, e.g. &#39;12323&#39; ipsc.tri.vs.di$EntrezID &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENTREZID&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) # Retrieve the KEGG enzyme code, e.g. 2.7.11.17 ipsc.tri.vs.di$Enzyme &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENZYME&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) Unfortunately, not all organisms offer access to such information nor is the information always complete. For instance, the following table shows the number of available records for the Ensembl, Entrez en KEGG IDs downloaded: ENSEMBL ENTREZ ENZYME Available 620 649 60 Missing 208 179 768 Statistics for annotation columns using AnnotationDbi For other data sets you might have more luck, otherwise continue with the biomaRt method explained below. B.2.2 Using biomaRt A short explanation about biomart (source: Wikipedia): “The purpose of the BioMarts in Ensembl Genomes is to allow the user to mine and download tables containing all the genes for a single species, genes in a specific region of a chromosome or genes on one region of a chromosome associated with an InterPro domain. The BioMarts also include filters to refine the data to be extracted and the attributes (Variant ID, Chromosome name, Ensembl ID, location, etc.) that will appear in the final table file can be selected by the user.” The text above mentiones species, attributes and filters, and we need to combine these elements to query the biomart databases for our annotation. The following code ‘chunks’ show possible values for these elements and how to gather and store the relevant data. The biomaRt library interfaces with the biomart.org online database. Sometimes the biomart website (which offers a browsable database) is down due to maintanance, but its many mirrors can still be used, for instance at Ensembl. You are required to explore a number of objects and the given example code is most likely not suitable for your data/ organism. The project is well documented on the Bioconductor biomaRt website that links to the biomaRt users guide. # Load the library library(biomaRt) # Use an alternative database server as the regular one sometimes has issues.. ensembl=useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, host=&quot;www.ensembl.org&quot;) biomart version ENSEMBL_MART_ENSEMBL Ensembl Genes 92 ENSEMBL_MART_MOUSE Mouse strains 92 ENSEMBL_MART_SNP Ensembl Variation 92 ENSEMBL_MART_FUNCGEN Ensembl Regulation 92 Available databases in the Ensembl biomaRt # Select the &#39;ensembl&#39; database ensembl &lt;- useMart(&quot;ensembl&quot;) Using the listDatasets function you can get a full list of available datasets. Store this list in an R object and ‘browse’ this object in RStudio to see if your organism is included. Copy the name of the dataset, this is the species element we will use. mart.datasets &lt;- listDatasets(ensembl) # Select the correct dataset, for the example data we select the &#39;hsapiens_gene_ensembl&#39; ensembl &lt;- useDataset(&#39;hsapiens_gene_ensembl&#39;, mart = ensembl) dataset description version oanatinus_gene_ensembl Ornithorhynchus anatinus genes (OANA5) OANA5 cporcellus_gene_ensembl Cavia porcellus genes (cavPor3) cavPor3 gaculeatus_gene_ensembl Gasterosteus aculeatus genes (BROADS1) BROADS1 itridecemlineatus_gene_ensembl Ictidomys tridecemlineatus genes (spetri2) spetri2 lafricana_gene_ensembl Loxodonta africana genes (loxAfr3) loxAfr3 choffmanni_gene_ensembl Choloepus hoffmanni genes (choHof1) choHof1 Subset of the 69 datasets available Next is deciding on a filter. For this we can use the listFilters function on the ensembl object, storing the full list of filters. Here too it is wise to view this in RStudio to find the filter to use. The filter specifies what you will use to search on. For instance, the AnnotationDbi queries above gave us Ensembl gene ID’s and we could use those with the ensembl_gene_id filter. When viewing the list of filters in RStudio you can use the search text-box in the top-right of the view, for example with ensembl to look for filters to use with this type of identifier. filters &lt;- listFilters(ensembl) name description chromosome_name Chromosome/scaffold name start Start end End band_start Band Start band_end Band End marker_start Marker Start marker_end Marker End encode_region Encode region strand Strand chromosomal_region e.g. 1:100:10000:-1, 1:100000:200000:1 with_ccds With CCDS ID(s) with_chembl With ChEMBL ID(s) with_clone_based_ensembl_gene With Clone-based (Ensembl) gene ID(s) with_clone_based_ensembl_transcript With Clone-based (Ensembl) transcript ID(s) with_dbass3 With DataBase of Aberrant 3’ Splice Sites ID(s) Subset of the 340 available filters Finally we get to the point to determine what data we would like to get from the database. These are the attributes which we can get with the listAttributes function on the ensembl object. Again - and especially with the attributes since there are often &gt;1000 of selectable options - we store the attributes and view them in RStudio to look for data that we want. attributes &lt;- listAttributes(ensembl) name description page ensembl_gene_id Gene stable ID feature_page ensembl_gene_id_version Gene stable ID version feature_page ensembl_transcript_id Transcript stable ID feature_page ensembl_transcript_id_version Transcript stable ID version feature_page ensembl_peptide_id Protein stable ID feature_page ensembl_peptide_id_version Protein stable ID version feature_page ensembl_exon_id Exon stable ID feature_page description Gene description feature_page chromosome_name Chromosome/scaffold name feature_page start_position Gene start (bp) feature_page end_position Gene end (bp) feature_page strand Strand feature_page band Karyotype band feature_page transcript_start Transcript start (bp) feature_page transcript_end Transcript end (bp) feature_page transcription_start_site Transcription start site (TSS) feature_page Subset of the 1944 available attributes If we want to have the gene chromosome, start- and end-position as well as its description (note, just as an example, there is other, more interesting information available too!) we combine this in a character vector (attrs.get, see below). There is one caveat though, the order in which you get back the results are not the same as the input order! This means that we cannot simple combine the data with our original data set but we need to merge it together. However, to be able to merge the data we need to know which record belongs to which gene and therefore we add our selected filter (in our case the ensembl_gene_id) to the list of attributes to get. Then, when we get the results dataframe we can use the merge function in R to combine the gene information with the actual data: merge(x = ipsc.tri.vs.di, y = results, by.x = &#39;Ensembl&#39;, by.y = &#39;ensembl_gene_id&#39;) We now have all three needed elements (species, filter and attributes) so we are ready to query the database with the getBM function (read the help using ?getBM). The example below retrieves data using a set of five Ensembl gene IDs since not all genes have an Ensembl ID as we’ve seen above, so we filter those out first and use this as the values parameter below. # Set the &#39;attributes&#39; values attrs.get &lt;- c(&quot;ensembl_gene_id&quot;, &quot;chromosome_name&quot;, &quot;start_position&quot;,&quot;end_position&quot;, &quot;description&quot;) # Perform a biomaRt query using &#39;getBM&#39; results &lt;- getBM(attributes = attrs.get, filters = &quot;ensembl_gene_id&quot;, values = ipsc.tri.vs.di$Ensembl[1:5], mart = ensembl) results$gene_length &lt;- abs(results$end_position - results$start_position) The results object is a data.frame with 5 columns that we can merge with our data set giving us the following annotation columns (combined from the AnnotationDBI and biomaRt libraries). Ensembl EntrezID Enzyme chromosome_name start_position ENSG00000131969 145447 NA 14 50872160 ENSG00000183044 18 2.6.1.22 16 8674565 ENSG00000243064 150000 NA 21 14236206 Table continues below end_position gene_length description 50904970 32810 abhydrolase domain containing 12B [Source:HGNC Symbol;Acc:HGNC:19837] 8784575 110010 4-aminobutyrate aminotransferase [Source:HGNC Symbol;Acc:HGNC:23] 14362754 126548 ATP binding cassette subfamily C member 13 (pseudogene) [Source:HGNC Symbol;Acc:HGNC:16022] This was just an example on how to use the biomaRt library and it comes down to selecting the correct filter and looking for interesting attributes to retrieve. Further information can be found in the documentation avaialble with `vignette(‘biomaRt’) B.3 References "]
]
