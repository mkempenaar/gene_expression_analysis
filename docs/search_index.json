[["index.html", "Transcriptomics Project - Gene Expression Analysis Introduction 1.1 Transcriptomics Project - Analysis of Gene Expression 1.2 Project Deliverables 1.3 Project Schedule 1.4 Learning Outcomes (LOs) 1.5 Grading 1.6 Lab Journal 1.7 Dashboard 1.8 Article", " Transcriptomics Project - Gene Expression Analysis Marcel Kempenaar 2025-04-25 Introduction 1.1 Transcriptomics Project - Analysis of Gene Expression The use of RNA-Sequencing techniques for measuring gene expression is relatively new and replaces microarrays, though in some cases microarrays are still used. Gene expression data gives valuable insights into the workings of cells in certain conditions. Especially when comparing for instance healthy and diseased samples it can become clear which genes are causal or under influence of a specific condition. Finding the genes of interest (genes showing differing expression across conditions, called the Differentially Expressed Genes (DEGs)) is the goal of this project. While there is no golden standard for analyzing RNA-sequencing datasets as there are many tools (all manufacturers of sequencing equipment also deliver software packages) we will use R combined with proven libraries for processing, visualizing and analyzing publicly available data sets. While in some cases you are allowed to use the actual raw (read) data that is available, it is highly recommended to use the pre-processed data which often is a table with a count value for each gene. This count is the number of reads that was mapped to that gene which corresponds to the relative number of transcripts (mRNA sequences) of that gene present in the cell at the time of sampling. 1.2 Project Deliverables The end products of this course consist of three deliverables; a knitted ‘lab journal’ where you have logged all steps performed to get to the end result, a separate Shiny dashboard application and a final report in the form of a short article. This chapter briefly describes the requirements and contents of these products and ends with instructions on how to use an RMarkdown template for writing the article. See the grading section below for more details on the grading of these products. 1.3 Project Schedule The aim is to keep to the below schedule during this course. Use the first two weeks to see if you need to focus more on one of the points below (depending on your data set) and discuss changes to the planning with your teacher. Find a public experiment of interest [week one] Using online resources (sections 2.1, 2.2) Data Gathering and Literature Research [week two] Make a final project choice Retrieve the accompanying publication Starting with Exploratory Data Analysis (chapter 3) Data Analysis [weeks three and four] Finalizing Exploratory Data Analysis Discovering Differentially Expressed Genes (chapter 4) Data Annotation Techniques used: R with bioconductor, the EdgeR and/ or DESeq2 packages, RMarkdown Result Analysis [week five and six] Analyzing and Visualizing your results (chapter 5) Techniques used: clustering, pathway analysis, gene-enrichment analysis Creating an Interactive Shiny Dashboard [week six and seven] Techniques used: Shiny, shinydashboard Note: final techniques will be presented later Finalizing analysis and start writing the final report (article) [weeks seven and eigth] Further reading material supplied are in the appendices: A1: Batch Loading Expression Data in R A2: Annotating an RNA-Seq Experiment 1.4 Learning Outcomes (LOs) This module has the following learning outcomes: You write a project proposal in which you explain the processing steps to be followed and the materials to be used in the context of selected research found in a relevant database. You apply statistical analyses to transcriptomics data in relation to phenotypes and external factors and visually represent and interpret the results. You link biological knowledge through annotation to the findings. For example, interpreting the effects of a transcriptomics experiment based on pathway and/or (gene-set) enrichment analysis results. You can develop a basic interactive R Shiny dashboard displaying the relevant results from the downstream DEG-analysis within the biological context. You describe the findings and key results of the analysis in article form according to the style of a relevant journal. 1.5 Grading This project is assessed by taking the weighted average of the following deliverables: Analysis, 7EC The written project proposal clearly describes the experimental setup, the techniques and tools used for processing the raw RNA-Seq sequencing reads into count matrices for further downstream analysis (LO 1). Proven methods have been researched and applied to the count data to identify the differentially expressed genes. The lab journal clearly specifies the decisions used to determine the performed comparison(s) and reflects on the acquired results (LOs 2, 3). Results of the DEG analysis are clearly presented using an interactive dashboard that – depending on the experimental setup and experiment – can be used to browse through taken steps and allows comparing between sample groups (LO 4). Article, 3EC Results from multiple functional analysis steps (e.g. clustering, pathway and enrichment analysis) are related to the context of the experiment (LO 5). 1.6 Lab Journal As you know from previous projects and most likely from working in the laboratory, it is essential to keep a proper lab journal detailing every step you have done during the experiment. Here, the log is to be kept in an R markdown file, showing which steps have been taken in the analysis of the data set. This markdown should be knitted into a single PDF-file once the project is completed thus containing text detailling the steps and any decisions you’ve made, R-code (always visible!) and their resulting output/ images. Notes: As a general advice; do not wait with knitting this whole document until the project is done as knitting is very prone to errors and trying to fix these in a large document is not easy. Give each code chunk the proper attributes, including a name at the minimum. This helps spot errors during knitting as that process mentiones which chunk has been processed. Note that chunk-names must be unique. And try to make proper use of chapters and sections and include an (optional) table of contents. 1.7 Dashboard The dashboard is an interactive web application that allows you to visualize the results of your analysis. It should be user-friendly and provide clear insights into the data. The dashboard should include the following features: A summary of the dataset, including the number of samples, genes, and any relevant metadata. Interactive visualizations of the data, such as heatmaps, PCA plots, and volcano plots. The ability to filter and subset the data based on user-defined criteria. A section for displaying the results of the differential expression analysis, including tables of DEGs and their associated statistics. The techniques used will be further explained during lectures. 1.8 Article The final report is written in article form which is a bit different from a usual report, mainly in its size. The article-report has a maximum number of pages of 4 including all images and references (no appendices!). Contents for this article should be extracted from the lab journal combined with part introduction and part conclusion/ discussion. The sections below describe a template that is available for writing this report and example report(s) will be made available for inspiration. Refer to this chapter again once you start writing the report. 1.8.1 Installing the Article Template The templates are available in an R package and contains both RMarkdown and (another layer on top of the actual markup language, yes, another language…) files. RStudio can use templates for a number of documents, including article-templates. These templates can be installed from a package called rticles by running the install.packages function (note: might already be installed): install.packages(&quot;rticles&quot;, type = &quot;source&quot;) 1.8.2 Using the Template Now that you have the templates package (rticles) installed you can download and use the template project (ZIP-file) available from here. Download this file to your project folder, extract its contents and open the report-template.Rmd file contained within the folder in RStudio. Verify that everything is setup correctly by hitting the Knit button at the top, this should create a PDF version of the report. Note that - somehow - the resulting PDF file is named RJwrapper.pdf instead of the expected report-template.pdf. If everything checks out OK you can rename the file to your liking and start editing. This template is based on the R Journal Submission template that you can also find in RStudio in the New file -&gt; R Markdown -&gt; From Template menu. Articles published in the R-journal are based on this template which you can browse for inspiration at the R Journal Website. The available template shows an example of segments/ chapters and briefly describes what each section could or should contain. If you want to write your report in the Dutch language, you can create a new file from the template and change the segment names and content to Dutch. There will however still be some headings added by the template in English which is fine with me, but if you want to keep everything Dutch you can either edit the RJournal.sty file manually (not recommended) or start a new file by yourself and add some nice headings and page options. The top part of the template (between the dashes ---) contains some settings that you need to change such as title, authors and abstract. Compare for instance a newly created article from this template with the one offered from the project-repository website. "],["datasets.html", " Discovering RNA-Seq Data Sets 2.1 Finding Public Data Sets of Interest 2.2 The NCBI Gene Expression Omnibus", " Discovering RNA-Seq Data Sets This chapter describes a method of finding public RNA-Seq data sets of interest. A data set in the context of this course refers to all data belonging to a certain gene-expression experiment, usually consisting of a number of sequencing-samples combined with meta-data describing the experiment. 2.1 Finding Public Data Sets of Interest There are a number of very large databases online that offer access to thousands of - published - experiments and here we will focus on searching and downloading gene expression data from high-throughput (NGS) sequencing techniques (RNA-Seq). With thousands of freely available data sets it is possible to start performing research without the need of actually performing your own lab-experiment. For all common conditions, organisms and tissues you can download samples and compare them over multiple studies to find novel relations between genes and conditions or to verify experiments performed in your laboratory. For instance, the power of public data sets was demonstrated jointly by three of our alumni in an article called Calling genotypes from public RNA-sequencing data enables identification of genetic variants that affect gene-expression levels.(al. 2015). It’s method section begins with the sentence “We downloaded the raw reads for all available human RNA-seq data sets …” of which the amount of data and work will become clear later on. If you are interested in expression Quantitative Trait Loci (eQTLs) or Allele-Specific Expression (ASE) analysis, please read this very interesting paper. But before we start diving into large and complex databases potentially containing thousands of experiments with millions of samples and terabytes of data, we need to get a rough idea on what we would like to do once we have found something of interest in order to know what we are looking for in the first place. While there is no definitive guide or protocol that can be followed for processing and analyzing a gene-expression data set, the following goals can be considered: Re-do (part of) the analysis described in the accompanying publication. As all data sets - except for the ones that were added very recently - are accompanied by a publication, you can gather a lot of information regarding experimental design and results from just this paper. Most often, the researchers already performed the most interesting research on this data set and it is therefore a good exercise to try and reproduce their results. Challenges here are getting a proper understanding of the techniques used by the researchers (from the article) and translating these to your own analysis steps. Often though, not all details are clear from the article which requires your own interpretation of their results and performing analysis steps to arrive at the same conclusion. Alternatively you can come up with your own ideas and (biological) question(s) that you could try and answer given a data set instead of redoing the published work. This is of course more challenging and not suitable for all data sets. You might notice that some publications only focus on a small set of genes instead of the whole transcriptome. For instance, when you perform an experiment on yeast where you want to measure the activity of genes involved in alcohol fermentation, researchers might only look at genes from the Glycolysis pathway. This leaves the other 6000+ yeast genes for you to explore and possibly come up with novel relations of gene expression and experimental setup. If the analysis approach explained in the paper is not focusing on a pre-selected list of genes, depending on the experiment you might come up with comparisons not done in the original research. For instance, in an experiment where the maternal age (at birth) is correlated to autism in their offspring, the paternal age is also known but not addressed in the publication. This could be a subject of further research (spoiler alert; no clear conclusion can be drawn from including this extra factor…). Another type of project is to evaluate different methods of either normalization or statistical analysis to either confirm the published results or find novel genes involved in the experiment. This can be viewed more as a technical research subject which is ok to pursue, however the final report should mainly concern the biological impact of your findings. This means that when the accompanying publication shows very conclusive results, it will probably come down to acknowledging their results (therefore, you extend on point 1 from this list). Because with a very conclusive result you hope to find the same results and if this is not the case it might become hard to formulate a good conclusion in the end. However, if the publication isn’t very specific and only states a conclusion such as ‘Benzene exposure shows increased risk of leukemia’ followed by a list of a few genes that might be involved, you could try to see if you can find other genes that might be involved by changing the analysis approach. Although this last project goal has many risks involved and will need a very sound project proposal, it might result in the most interesting project. As shown before with the linked article, it is possible to combine data from multiple experiments. You could for instance find two very related experiments (i.e. researching the effect of a certain drug) both measuring expression in different tissue types (i.e. liver and brain tissue). After analyzing both experiments, you could present a set of genes that show an effect in both tissues and - more interestingly - genes showing an effect in only one tissue. The list above is a guideline to be kept in mind while browsing for suitable data sets and it will be extended in week three. Also, don’t worry if some of the terms above are unclear, getting a good understanding on the used terminology is part of these first few weeks 2.2 The NCBI Gene Expression Omnibus The easiest method of exploring and finding interesting data sets is by simply using your web-browser to access a data-repository. One such repository is the NCBI Gene Expression Omnibus (GEO). This repository was primarily used to store micro array data sets and describe those experiments, linking to raw data, processed data and an accompanying publication. There are however many more sources of data browsable in the GEO, and for this project we will limit our searches to experiments using the RNA-Seq technology for which over 60.000 experiments are listed. Following the “Expression profiling by high throughput sequencing” link in this summary table we get the complete listing in which you can find an interesting experiment. Besides finding an experiment that you are interested in, there are some further important requirements that you need to account for when browsing listed below. Note that at this point we do not need to download the data yet, this is described in chapter 3. The experiment must be published and the publication must be fully available, free or otherwise through the Hanze University library. For each sample group, a minimum of three replicates must be present. If one or more groups have less then three replicates, that group cannot be used (the rest of the data might still be usable if there are at minimum two groups that do have 3 replicates). Replicates are often not very easy to spot. When lucky samples are named “WildType - rep 2” for instance but most often you need to read the “Overall design” part of the page or click on the samples (with the “GSM…” ID) to make sure. The available data must contain at least the count data. Data is offered in multiple formats, always including the RAW data (the actual reads which we don’t use), but also in further processed data such as FPKM, RPKM and TPM (see this interesting blog-post discussing the use of these data). The tools and R analysis libraries that we will use for the downstream analysis rely on unnormalized and unprocessed data which is the count data. These counts simply represent the number of reads mapped to a transcript. This is often the hardest part to assess; sometimes it is not clear if the provided data are the actual raw count values. Usually, you can find some (sometimes rather cryptic) information by clicking on a sample ID under the ‘Data processing’ section. In this paragraph of text is often a mention of the ‘Supplementary_files_format_and_content’ that describes the actual content of the downloadable files. Following are a number of examples of valid and invalid data provided by the experiment. These are the single-sample pages where you can find such a description. Unfortunately, the authors of each experiment are free to explain their data as they please, so you will most likely not encounter the exact same descriptions: Valid: Global effects of SUPT4H1 RNAi on gene expression of HEK293 cells; description: Supplementary_files_format_and_content: tab-delimited, gzip-compressed text file with matrix of raw quantitation results for each sample. Mentions of raw-anything are most likely valid. Valid: Transcriptome profiling of normal and transformed mouse urothelial cells; description: Supplementary_files_format_and_content: raw_counts_unfiltered.txt.gz: Raw counts.. Even though 4 different file types are available for download, one of these contains the actual raw count data. Valid: Quantitative Analysis of PPARD RNA-Seq Transcriptomes of Mouse Gastric Corpus Epithenial Cells by Next Generation Sequencing (NGS); description: Supplementary_files_format_and_content: raw counts of sequencing reads for the genes. Single file download containing only the raw counts. Valid: Impact of shRNA-mediated KLF4 down-modulation on the transcriptome profile of human keratinocyte precursor cells; description: Supplementary_files_format_and_content: raw and normalized counts. Valid: Role of TAF5L and TAF6L in controlling the mouse ESC state; description: Supplementary_files_format_and_content: Comma-separated text files containing raw counts, normalized counts, FPKM expression values and TPM expression values.. Very common to include both normalized and raw values. Invalid: Co-chaperone Mzb1 is a key effector of Blimp1 in plasma cell differentiation and b1 integrin function; description: Supplementary_files_format_and_content: The normalized gene expression values (FPKM) calculated by Cuffnorm (v2.2.1) is provided in TXT format. This clearly states that there is only normalized data (FPKM values) available. Invalid: EGFR-targeted therapy-induced resistance mechanism in glioblastoma; description: Supplementary_files_format_and_content: .txt file including gene name, RefSeq id, locus, RPKM value of each gene in each sample. Only the RPKM values are available and these are normalized raw count values, not usable. Invalid: RNA-seq of subcellular fractions from nESC and EpiLC; description: Supplementary_files_format_and_content: _genes.fpkm_table.txt: Tab-delimited text files include FPKM values for each replicate.*. Only FPKM data available. References al., Patrick Deelen et. 2015. “Calling Genotypes from Public RNA-Sequencing Data Enables Identification of Genetic Variants That Affect Gene-Expression Levels.” Genome Medicine 7 (30). "],["EDA.html", " Exploratory Data Analysis 3.1 Loading data into R 3.2 Example Data 3.3 Visualizing using boxplot and density plot 3.4 Visualizing using heatmap and MDS 3.5 Cleaning Data", " Exploratory Data Analysis We start with performing some exploratory data analysis steps with the goal of getting to grips with your chosen data set to properly identify a strategy for the actual analysis steps. During this exploration we will also keep an eye on the quality of the data. Even though the downloadable data is ‘processed’, there might be samples present that deviate too much from the other group of samples (a so called outlier). Creating basic visualizations of the data will give the necessary insight before we continue. 3.1 Loading data into R Once you have one or more column based text files they can be read into R simply by using the read.table() function. Follow the following steps to read in the data and start the exploratory data analysis. The resulting document should be treated as a lab journal where you log the process from loading the data to the final analysis steps. Open RStudio Create a new R Markdown document Give it a proper title and select the PDF format Give the document some structure; e.g. create a segment (using single hash #) called Exploratory Data Analysis. Whenever you add code to your document make sure that it is both readable (keep the maximum line length &lt; 100 if possible) and there is sufficient documentation either by text around the code ‘chunks’ (preferred) or by using comments within the code chunk. Read in the data file(s) Preferably use the read.table function and carefully set its arguments. Open the file in a text editor first to check its contents; does it have a header? can we set the row.names? Are all columns needed? etc. Note: if the data set consists of separate files (i.e. one per sample) or for general tips on reading in data, see the Appendix A: Batch Loading Expression Data chapter. For the remainder of the document, try to show either the contents, structure or - in this case - dimensions of relevant R objects Show the first five lines of the loaded data set. Including tables in a markdown document can be done using the pander function from the pander(Daróczi and Tsegelskyi 2017) R-library. Give the dimensions (with dim() and the structure (with str()) of the loaded data set. Check the output of the str function to see if all columns are of the expected R data type (e.g. values, factors, character, etc.) Examine the samples included in the experiment and create as many R character objects as needed to store the classification. Note: this is an important item and is often overlooked. See the last 4 lines in the code chunk in the section Example Data below. For instance, if you have eight samples divided into case/ control columns you create a variable called case in which you store the column indices (only numbers!) of the respective columns in the data set and a variable called control with the remaining four data column indices. These are for later use. These variables allows us to repeatedly access the same data during the rest of this course, i.e. the code boxplot(counts[case]) immediately shows that we are plotting the case samples from the counts data which is more clear than reading boxplot(counts[c(5:8)]) as that gives no reference to what these indices mean and can lead to typing mistakes. For some data sets the order in which the samples are listed in your loaded data set is different from the order that is shown on the GEO website. Most often, the names are different too or they are lacking any description and all you have are non descriptive IDs. When creating these variables such as control that need to point to all control samples, you need to make sure that you have the right columns from your data frame. Go to Appendix A2; annotation if the order is unclear or you just have a large number of samples as there might be supporting data available that can help make sense of your sample layout. If you want to include external images to your log and to better control properties such as height and width for individual images you can use the following code. Note: the code below is an example and you need to replace the img_to_include.png file path to an actual image.): # Add the following to the code chunk header to control figure height, width and add a caption: # {r, fig.height=5, fig.width=8, echo=FALSE, fig.cap=&quot;Image description&quot;} # The `dpi` argument is to remove any scaling knitr::include_graphics(&quot;figures/img_to_include.png&quot;, dpi = NA) See the Knitr chunk options page for a full listing of all chunk options. 3.2 Example Data This section lists all (publically available) data set(s) used in this chapter. Each chapter contains this section if new data sets are used there. Note that for all examples, your data will be different from the examples and one of the challenges during this course will be translating the examples to your own data. Keep in mind that simple copy-pasting of most code will fail for that reason. Most examples therefore will print the input data for comparison to your own data. From the section Normalization onwards, the experiment with identifier GSE101942 is used for visualizing the normalized raw count values (the earlier sections explore the unnormalized data). This experiment is titled “Transcriptome analysis of genetically matched human induced pluripotent stem cells disomic or trisomic for chromosome 21” and the experimental setup is described as follows: “12 total polyA selected samples. 6 IPSC samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples. 6 IPSC derived neuronal samples with 3 biological repeats for trisomic samples and 3 biological repeats for disomic samples.” The following code shows how to load the two available files containing the raw-count data (one file per 6 samples), stored locally in the data/gse101942/ folder. Some simple ‘cleanup’ steps are performed which aren’t required but helpful for demonstration purposes (i.e., renaming samples to identify groups). GSE101942_IPSC_rawCounts.txt: raw count data for the induced pluripotent stem cells (iPSCs), both trisomic and disomic GSE101942_Neuron_rawCounts.txt: raw count data for the IPSC-derived Neurons, both trisomic and disomic. These cells were treated to remove chromosome 21 from the iPSCs, as described by the treatment protocol as: “Targeted removal of CHR 21 in IPSC using TKNEO transgene”. # Load the two sets of 6 samples ipsc.data &lt;- read.table(&#39;./data/gse101942/GSE101942_IPSC_rawCounts.txt&#39;, header = TRUE) neuron.data &lt;- read.table(&#39;./data/gse101942/GSE101942_Neuron_rawCounts.txt&#39;, header=TRUE) # Merge by rowname (by = 0, see the help of &#39;merge&#39;) counts &lt;- merge(ipsc.data, neuron.data, by = 0, all.x = TRUE, all.y = TRUE, sort = FALSE) # Change all NA&#39;s introduced by the merge to zeros counts[is.na(counts)] &lt;- 0 # Show column names coming from the input files print(names(counts)) ## [1] &quot;Row.names&quot; &quot;c244A&quot; &quot;c244B&quot; &quot;c243&quot; &quot;c2A&quot; &quot;c2B&quot; ## [7] &quot;c2C&quot; &quot;C3_1&quot; &quot;C3_2&quot; &quot;C3_4&quot; &quot;C2_2_1&quot; &quot;C2_2_2&quot; ## [13] &quot;C2_2_3&quot; # Set the row names to the gene IDs stored in the &#39;Row.names&#39; column row.names(counts) &lt;- counts$Row.names # Remove the gene ID column counts &lt;- counts[-1] # Rename samples to include their group name names(counts) &lt;- c(paste0(&#39;di_IPSC_r&#39;, 1:3), # Disomic IPSC, replicates 1-3 paste0(&#39;tri_IPSC_r&#39;, 1:3), # Trisomic IPSC, replicates 1-3 paste0(&#39;di_NEUR_r&#39;, 1:3), # Disomic Neuron, replicates 1-3 paste0(&#39;tri_NEUR_r&#39;, 1:3)) # Trisomic Neuron, replicates 1-3 # Show results of renaming the samples print(names(counts)) ## [1] &quot;di_IPSC_r1&quot; &quot;di_IPSC_r2&quot; &quot;di_IPSC_r3&quot; &quot;tri_IPSC_r1&quot; &quot;tri_IPSC_r2&quot; ## [6] &quot;tri_IPSC_r3&quot; &quot;di_NEUR_r1&quot; &quot;di_NEUR_r2&quot; &quot;di_NEUR_r3&quot; &quot;tri_NEUR_r1&quot; ## [11] &quot;tri_NEUR_r2&quot; &quot;tri_NEUR_r3&quot; # Assign column -indices- to variables for later use (i.e. counts[di_NEUR] selects the relevant columns) di_IPSC &lt;- 1:3 tri_IPSC &lt;- 4:6 di_NEUR &lt;- 7:9 tri_NEUR &lt;- 10:12 See Table 1 in the Normalization section for the resulting contents in the counts dataframe. 3.3 Visualizing using boxplot and density plot This segment describes some of the basic steps and a visualization that can be performed during Exploratory Data Analysis. We start by inspecting the unnormalized count data. For the next steps normalization is applied as shown in the Normalization section below. As mentioned above, the focus of EDA is to get an overview/ perform a bit of Quality Control of the data set and while this often requires visualizing the data, these figures do not need to be very pretty. Simple figures are perfectly fine in this stage. Try to create these figures for your own data (and keep them in your log) and add a small description for each figure pointing out anything that is different from what you expect. Also note that these steps are just a selection. Furthermore, make sure that for every visualization you make, add proper axis-labels containing the measurement units (important!). As you might be able to see, all values are log-transformed using the log2 function because very often the numerical values have a very high range which will ‘hide’ the details on the plots. See the section about the Fold Change value in chapter 4 for further details. It is fine to use non-log-transformed (simply the raw-)data, otherwise use for instance boxplot(log2(dataset)) for plotting. Instead of using the basic R-plotting library (i.e. plot, boxplot, hist, etc.) you can also opt for using the (challenging) ggplot2 library that is also used for the boxplot in the following section. While constructing a ggplot2 plot feels like learning yet another language, there are many resources available online that you can follow. 3.3.1 Statistics Even the most basic statistics can give some insight into the data such as performing a 6-number-statistic on the data columns using the summary() function. Note: you can also use the pander function to pretty-print a summary in the knitted output. What do you notice if you look at the numbers produced by executing this function on the complete data set? 3.3.2 Boxplots A visual representation of these values can be shown using a boxplot. Boxplots are very easy to create from an R data frame object by just passing in the data columns. The following boxplot shows the data for an experiment with a separate boxplot for each sample. This allows a quick overview for spotting irregularities (i.e. checking if the replicates within a sample-group show similar expression profiles). Of course, if we consider the amount of data in this single plot, it can only hint at any problems, we need to look in much more detail when doing any form of quality control. Creating a boxplot from a dataframe is easy, but as we saw with using the summary function; the data has a large range with the maximum and average values being very far apart. This will create a lot of outliers in the plot which will be interesting later on, but for the boxplot we can either: hide them completely using the outline = FALSE argument to boxplot() (do say so in the figure description!), or perform a log2() transformation as you can see below. When creating plots of expression data without performing log-transformations we see that (for instance with the boxplot) the range of data is very large (from 0 up to 1.000.000) with a lot of outliers in the upper range. This makes for not very informative figures, so placing a simple log2() function call around the data almost resolves this issue. However, this might introduce another problem (taking the boxplot as an example) because this log-transformation results in data that contains negative infinity (-Inf) values. These are caused by all the 0-values in the data since log2(0) == -Inf. To circumvent this issue, we can add a pseudo count to the data by simply adding the value 1 to all count values since the log2(1) == 0; log2(counts + 1). There are more situations other than the boxplot where adding a pseudo count value to the complete dataset can be useful. Always add a pseudocount in-place, meaning within the plotting code instead of overwriting your original data set with a pseudo count added as not all steps require this. In some cases (see the density plot below) it can be useful to clearly separate the 0-values from the rest in which case a 0 &lt; pseudo count &lt; 1 value, such as 0.1 can be used as this generates a negative value (-3.3) that is far away from the rest of the data. All other count values are always &gt;0 which result in a positive log2 value. Figure 3.1: Boxplot comparing basic statistics for all genes across multiple samples 3.3.3 Density Plots Another form of visualizing the same data is using a density plot. This method shows a distribution of the (log2-transformed) count data for all samples and allows for easy spotting of problems. While this plot is more commonly used in analysing microarrays, it is still useful for comparing the complete dataset. The code and figure below show an example distribution for 12 samples. A few things to note about this figure is that there is a huge peak at exactly -3.321928 which can be ignored because this is value is calculated from log2(0.1) explained in the box above. This peak consists of all 0-values (inactive genes) which isn’t very important to us now. Therefore, we added a vertical line to indicate the left-part is of little interest. Note: a lot of the code below is extra, for a simple inspection using only the line plotDensity(log2(counts + 0.1)) is enough, the rest is extra example code. To assess the quality of the data we look at the peaks in the plot. The heigth of the peak doesn’t matter much, shifted peaks do however as those indicate either a lower or higher amount of reads sequenced for that sample. This can be confirmed with the first plot demonstrated below in the next section where the library-sizes are visualized. ## The affy library has a density plotting function library(affy) ## Create a list of 4 colors to use which are the same used throughout this chapter library(scales) myColors &lt;- hue_pal()(4) ## Plot the log2-transformed data with a 0.1 pseudocount plotDensity(log2(counts + 0.1), col=rep(myColors, each=3), lty=c(1:ncol(counts)), xlab=&#39;Log2(count)&#39;, main=&#39;Expression Distribution&#39;) ## Add a legend and vertical line legend(&#39;topright&#39;, names(counts), lty=c(1:ncol(counts)), col=rep(myColors, each=3)) abline(v=-1.5, lwd=1, col=&#39;red&#39;, lty=2) Figure 3.2: Density plot comparing count distribution for 12 samples 3.4 Visualizing using heatmap and MDS This section adds a few Exploratory Data Analysis techniques where we will measure and look at distances between samples based on normalized data. Measuring distances between two data objects (samples in our case) is a common task in cluster analysis to compare similarity (low distance indicates similar data). In this case we will calculate the distances between our samples and visualize them in a heatmap and using a multidimensional scaling (MDS) technique. In the previous section we used the raw count data. You might have one or more samples that have different values (i.e. shifted) compared to other samples. While we need the raw count data to use R packages such as edgeR (Chen et al. 2018) and DESeq2 (Love, Anders, and Huber 2017), calculating sample distances (used in the visualizations in this section) should be done on some form of normalized data. This data can either be RPKM/FPKM/TPM/CPM or vst-transformed (raw-)read counts. A proper method of transforming raw read count data is using the vst method from the DESeq2 R Bioconductor library which is shown below. This ‘variance stabilizing transformation’ normalized data will only be used in this chapter, in chapter 4 we will again normalize using a different technique. The following code examples shows how to use this library to normalize the count data from the GSE101942 experiment to vst-normalized data before we calculate a distance metric.   di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 tri_IPSC_r1 5S_rRNA 4 6 4 17 7SK 79 57 28 96 A1BG 16 8 12 15 A1BG-AS1 267 253 166 316 A1CF 2 0 0 4 A2M 10 8 7 48 Table 1; Raw count data for GSE101942 (continued below)   tri_IPSC_r2 tri_IPSC_r3 di_NEUR_r1 di_NEUR_r2 5S_rRNA 13 12 22 16 7SK 134 38 26 26 A1BG 39 22 583 938 A1BG-AS1 339 211 680 659 A1CF 0 0 0 1 A2M 8 4 59 910 Table continues below   di_NEUR_r3 tri_NEUR_r1 tri_NEUR_r2 tri_NEUR_r3 5S_rRNA 0 26 9 0 7SK 71 32 14 40 A1BG 809 1030 461 987 A1BG-AS1 1165 636 449 1257 A1CF 2 0 0 0 A2M 520 592 155 74 If you look at Table 1 you immediately see a huge difference between the groups for each of the genes. The gene (A1BG) shows &gt;100 more expression in the tri_NEUR group compared to the di_IPSC group, but within most groups there is a very high variation too indicating that it might not be the actual expression that is different. One very simple and quick method of inspection is looking at the total number of mapped reads per sample (the sum of each sample/ column), as the sequencing-depth might vary across samples. Hint: Simple version of this plot can be achieved with barplot(colSums(counts) / 1e6) di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 tri_IPSC_r1 tri_IPSC_r2 tri_IPSC_r3 108 108 81 112 134 83 Table 2; Mapped reads per sample (millions) (continued below) di_NEUR_r1 di_NEUR_r2 di_NEUR_r3 tri_NEUR_r1 tri_NEUR_r2 tri_NEUR_r3 70 94 114 108 48 140 Figure 3.3: Library sizes for all samples from GSE101942 The numbers shown in Table 2 and the barplot above immediately clarify that it is not just the experimental condition that might have caused this large difference between sample counts but the sequencing depth shows a substantial difference too. Both the minimum and maximum sequencing depth are within the trisomic neuron group (purple in the barplot). The average expression of the A1BG gene within this group is 820, but with a minimum of 461 and a maximum of 1010 there is a lot of variation. This is one more reason telling us that we will have to normalize the data. The following code chunks show how to do this with DESeq2’s vst method (variance stabilizing transformation). Check the Packages tab in Rstudio to see if you have the DESeq2 package installed and load it with the library command. 3.4.1 Normalization To use the vst function from the DESeq2 library we must construct a DESeqDataSet-object consisting of the count data combined with sample annotation. Since we only want to use it (for now) for performing a vst-transformation we use the most basic form with a very simple design and the sample names as annotation (the colData argument): # Load the library library(&#39;DESeq2&#39;) # DESeq2 will construct a SummarizedExperiment object and combine this # into a &#39;DESeqDataSet&#39; object. The &#39;design&#39; argument usually indicates the # experimental design using the condition(s) names as a &#39;factor&#39;, for now we use just &#39;~ 1&#39; (ddsMat &lt;- DESeqDataSetFromMatrix(countData = counts, colData = data.frame(samples = names(counts)), design = ~ 1)) ## class: DESeqDataSet ## dim: 56640 12 ## metadata(1): version ## assays(1): counts ## rownames(56640): 5S_rRNA 7SK ... C1orf220 C2orf15 ## rowData names(0): ## colnames(12): di_IPSC_r1 di_IPSC_r2 ... tri_NEUR_r2 tri_NEUR_r3 ## colData names(1): samples We now have a proper DESeqDataSet object as you can see above, containing 56640 rows and 12 columns (genes and samples) with the gene symbols as row names. Usually this object would hold more data, but as this is only a requirement to perform the vst transformation it is good enough for now. Next step is performing this transformation (this can take a while depending on the size of the experiment and results in a Large DESeqTransform object) and retrieving the actual data from this with the assay function as this object too contains a lot of meta-data. The table below shows the updated values which are now comparable across genes whereas the raw count data was harder to compare. # Perform normalization rld.dds &lt;- vst(ddsMat) # &#39;Extract&#39; normalized values rld &lt;- assay(rld.dds)   di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 5S_rRNA 3.704 4.015 3.876 7SK 6.726 6.36 5.747 A1BG 4.842 4.243 4.813 A1BG-AS1 8.385 8.37 8.083 A1CF 3.322 2.343 2.343 A2M 4.397 4.243 4.312 Table 2; VST transformed disomic IPSC count data Note that when again looking at the A1BG gene (the one displayed in figure 3.8), the normalized expression values show much less variation across the samples (not shown, but also within each sample group). Therefore, we assume that the large difference in expression we observed earlier might be non-existent (there might still be a significant difference though!). 3.4.2 Distance Calculation We now have normalized data that we can use for distance calculation. This is a standard procedure for many data analysis tasks as it calculates a distance metric for each combination of samples that we will use to check for variation within the sample groups. We first need to transpose the matrix (rld) of normalized values using the t-function, because the dist function expects the different samples as rows and the genes as columns. Note that the output matrix is symmetric. Table 3 below shows the calculated distances within the disomic IPSC group where the distance between samples varies from 100 to 140. The maximum distance between any samples is 380 as will be demonstrated with the visualizations in the next sections. # Calculate basic distance metric (using euclidean distance, see &#39;?dist&#39;) sampledists &lt;- dist( t( rld ))   di_IPSC_r1 di_IPSC_r2 di_IPSC_r3 di_IPSC_r1 0 107.6 147.7 di_IPSC_r2 107.6 0 139.7 di_IPSC_r3 147.7 139.7 0 Table 3; Sample distances for the disomic IPSCs (Euclidean method) 3.4.3 Sample Distances using a Heatmap If you have both (raw-)count data and an other normalized format (TPM, RPKM, etc.), you can optionally follow the above procedure for your count data and create a heatmap for both formats to see if this makes any difference. The reason for this is that while the RNA-Seq method exists for over 10 years, there are still ongoing discussions on the subject of data processing, especially regarding subjects like which normalization technque to use for which data analysis. The following code block creates a heatmap using the pheatmap library which offers on of the many available heatmap functions. Using the annotation data frame (you can inspect the contents of it yourself) we identify the samples based on both the cell type and the ploidy. The clustering shown in the heatmap clearly separates the data based on the cell type and the differences between the ploidy seems to be minimal. Looking further still, the differences within a single group are minimal too meaning that we are not - yet - inclined to remove any outlier-samples. Since we have only have 3 samples per category, we would also lose statistical power if we eventually were to remove one or more samples (also, always check the article to see if they did remove any samples prior to the data analysis). # We use the &#39;pheatmap&#39; library (install with install.packages(&#39;pheatmap&#39;)) library(pheatmap) # Convert the &#39;dist&#39; object into a matrix for creating a heatmap sampleDistMatrix &lt;- as.matrix(sampledists) # The annotation is an extra layer that will be plotted above the heatmap columns # indicating the cell type annotation &lt;- data.frame(Cell = factor(rep(1:2, each = 6), labels = c(&quot;IPSC&quot;, &quot;Neuron&quot;)), Ploidy = factor(rep(rep(1:2, each = 3), 2), labels = c(&quot;disomic&quot;, &quot;trisomic&quot;))) # Set the rownames of the annotation dataframe to the sample names (required) rownames(annotation) &lt;- names(counts) pheatmap(sampleDistMatrix, show_colnames = FALSE, annotation_col = annotation, clustering_distance_rows = sampledists, clustering_distance_cols = sampledists, main = &quot;Euclidean Sample Distances&quot;) Figure 3.4: Heatmap showing the Euclidean distances between all samples The resulting heatmap shows an interesting comparison across all samples where the order of samples is purely determined using the distance and is often different from the order in the data set. Ideally, we want the sample groups to be clustered together as we see with this data. This particular heatmap clearly shows a large difference in overal expression between the induced pluripotent stem cells (iPSCs) and their differentiated cortical neurons while the difference - within cell type - between disomic and trisomic is much lower. 3.4.4 Multi-Dimensional Scaling The following code example shows how to perform Multi-Dimensional Scaling (MDS) that displays similarly calculated distances in a 2D-plot. With an experiment like this with two groups of samples, we hope to see two clearly separated clusters formed, however as we’ve seen in the heatmap, sample KO1B showed a large deviation which we will also see (confirm) using MDS. The figure below is plotted using ggplot2, a more advanced method of plotting in R. While these plots are preferred over base-R plotting, it is always sufficient to use just that as it can be very challenging to alter the example code shown in this section. The data object plotted is shown and in this case contains simple X- and Y-coordinates. For MDS we use a slightly different distance metric (Poisson instead of the default euclidean used in the dist function). The following code shows how to calculate a more fitting distance for sequencing data called the Poisson Distance with a number of optimizations available in the PoiClaClu library. This library was specifically designed to handle read count data and is less influenced by read-count differences across samples. Note that again, the code below is only usable for count data as this function requires: An n-by-p data matrix with observations on the rows, and p features on the columns. The (i,j) element of x is the number of reads in observation i that mapped to feature (e.g. gene or exon) j. library(&#39;PoiClaClu&#39;) # Note: uses the raw-count data, PoissonDistance performs normalization # set by the &#39;type&#39; parameter (uses DESeq) dds &lt;- assay(ddsMat) poisd &lt;- PoissonDistance( t(dds), type = &quot;deseq&quot;) # Extract the matrix with distances samplePoisDistMatrix &lt;- as.matrix(poisd$dd) # Calculate the MDS and get the X- and Y-coordinates mdsPoisData &lt;- data.frame( cmdscale(samplePoisDistMatrix) ) # And set some better readable names for the columns names(mdsPoisData) &lt;- c(&#39;x_coord&#39;, &#39;y_coord&#39;) x_coord y_coord -22016 -3828 -22069 -8.734 -19000 -394.9 -19955 1561 -21758 1435 -18369 2094 17182 -988.6 17455 -28.01 22613 -11249 19312 6807 16493 -792.7 30111 5392 Table 5: MDS coordinates used for plotting the distances using Poisson Distance # Separate the annotation factor (as the variable name is used as label) groups &lt;- factor(rep(1:4, each=3), labels = c(&quot;di_IPSC&quot;, &quot;tri_IPSC&quot;, &quot;di_NEUR&quot;, &quot;tri_NEUR&quot;)) coldata &lt;- names(counts) # Create the plot using ggplot ggplot(mdsPoisData, aes(x_coord, y_coord, color = groups, label = coldata)) + geom_text(size = 4) + ggtitle(&#39;Multi Dimensional Scaling&#39;) + labs(x = &quot;Poisson Distance&quot;, y = &quot;Poisson Distance&quot;) + theme_bw() Figure 3.5: Multi Dimensional Scaling - Poisson Distance for all samples ## Using base-R plot and legend functions: # plot(mdsPoisData, col=rep(myColors, each=3), pch=20, lwd=2) # legend(x=-20000, y=11000, legend = levels(groups), col=myColors, pch=20) We clearly see a separation on cell type and a less clear separation on ploidy. The spread within the disomic groups is much lower compared to the trisomic groups. Note that the y-axis range is about a 4th of the x-axis range. The sample di_NEUR_r3 shows a large difference in the MDS plot. Once we have a set of genes identified as being differentially expressed we could repeat this step with the expectation of a more clear clustering. 3.5 Cleaning Data Conclude this chapter by cleaning your dataset if the visualizations show the necessity for this. Cleaning in this case means removing complete samples if they can be classified as an outlier within its group. As with most tasks during this course, there is no clear advice on when to decide to remove one or more samples. The only clear rule is that each group must retain at least three samples. If one of the three samples visibly deviates from the other replicates, even after normalization, it will stay in your data set and it should be mentioned in your analysis report/ final article that noise may be introduced by this sample. Note: Once you are satisfied with the data and reported on your findings, we do not return to any of these steps in the next chapters. If there are mentions of creating a heatmap for instance, this means a heatmap of expression data instead of the sample distances we’ve shown in this chapter. References Chen, Yunshun, Aaron Lun, Davis McCarthy, Xiaobei Zhou, Mark Robinson, and Gordon Smyth. 2018. edgeR: Empirical Analysis of Digital Gene Expression Data in r. http://bioinf.wehi.edu.au/edgeR. Daróczi, Gergely, and Roman Tsegelskyi. 2017. Pander: An r ’Pandoc’ Writer. https://CRAN.R-project.org/package=pander. Love, Michael, Simon Anders, and Wolfgang Huber. 2017. DESeq2: Differential Gene Expression Analysis Based on the Negative Binomial Distribution. https://github.com/mikelove/DESeq2. "],["chapter-4.html", " Discovering Differentialy Expressed Genes (DEGs) 4.1 Pre-processing 4.2 The Fold Change Value 4.3 Using Bioconductor Packages", " Discovering Differentialy Expressed Genes (DEGs) The first and most important ‘real’ analysis step we will do is finding genes that show a difference in expression between sample groups; the differentially expressed genes (DEGs). The concept might sound rather simple; calculate the ratios for all genes between samples to determine the fold-change (FC) denoting the factor of change in expression between groups. Then, filter out only those genes that actually show a difference. While this does give a list of genes showing different behaviour across samples, we need to focus on genes that do not only show a difference, but where that observed difference is also statistically significant! To determine whether or not a gene can be classified as a significant DEG we are going to use existing libraries, one of which your own article might refer to. Before we do that we’re going to a brief manual examination of the count data to make sure that you properly understand what we’re looking for: significance and the Fold-Change value. 4.1 Pre-processing Given the results of the exploratory data analysis performed in chapter 3, you might have concluded that there are one or more samples that show (very) deviating expression patterns compared to samples from the same group. As mentioned before, if you have more then enough (&gt; 3) samples in a group, you might opt to remove a sample to reduce the noise as the statistical tests are very sensitive to this. Since we are performing all analysis steps programatically it is also very easy to test for DEGs with and without the sample(s) in question and see if the removal results in lower p-values (= higher significance). Remember to always properly document your choice and reasoning! To perform a few more manual steps on our data before using other libraries, we will re-do the normalization with a different technique. A simple method of normalizing count data is to calculate the fragments per million mapped fragments (FPM, equal to CPM) value and then transform this with log2. Opposed to FPKM and RPKM this does not include the gene-length in its calculation (which you most likely don’t have). If your dataset also includes FPKM or RPKM (pre-normalized), you are allowed to use this data too. FPM and CPM are simply dividing the count values for a sample by the total number of reads of that sample divided by 1 million as shown below: # Perform a naive FPM normalization # Note: log transformation includes a pseudocount of 1 counts.fpm &lt;- log2( (counts / (colSums(counts) / 1e6)) + 1 ) Another step we can do - and this might be guided by your article - is to filter out (partially) inactive genes. While this is only required when using the edgeR library (DESeq2 includes its own low-count filtering methods), it is a good exercise to do manually and we will use it below when calculating the lFC values. Most data sets available contains a lot of 0-measurements (see for instance your density plot from the previous chapter); genes where no reads have mapped. In the R-studio Environment tab, click on your data set (or perform the View(data) command and click on one of the sample columns to order the data ascending. You will now most likely (unless you have bacterial data for instance) see a lot of zero values in all columns. In an experiment with two groups, three replicates each, if three out of those six samples have 0-reads mapped, it is often advised to remove the gene completely. But this can be very subjective to the experiment, it might be expected (thus important) when comparing different tissues or knockout experiments. Also, genes with a (very) low read count (&lt; 5) can give a very high (artificial) FC value (see the left hand side of an MA-plot). Comparing two samples where one has a value of 2 and the other 11, this reads as an up-regulated gene by a factor of 5.5 while it might actually just be noise! Assignment: Search through your article for any advice on how to filter out zero values or low count genes. If there is nothing stated on this subject, think of your own tactic (or search the literature/ online!). It is perfectly fine to discuss with your peers. describe what you will be doing for this aspect; if you do not filter your data, clearly explain why not (most likely because the article stated a proper reason) Perform the filtering on your data set. For this you will most likely need to use one of the apply, rowSums, rowMeans functions, combined with maybe the which, all and any functions. Manually verify that the rows removed were correctly filtered. Properly document how many genes have been filtered out! 4.2 The Fold Change Value The FC is usually given as the calculated log2 of the case/control ratio. For example, gene A has an average expression of 30 mapped reads in the control group and 88 reads in the experiment group, the ratio case/control is 2.93. Ratio values &gt; 1 indicate increased expression in the experiment in relation to the control and values between 0 and 1 indicate lower expression. The log2 transformed value of the ratio is calculated with log2(2.93) and results in 1.55. If the counts were reversed, the ratio would have been 30/88, which is 0.34. The previously calculated value of 2.93 means a 3-fold up regulation while the 0.34 value means 3-fold down regulation but as you can see the range of numbers is very different. Comparing log2 values this would be 1.55 and -1.55 which compares much better. While it is very easy to calculate the FC for all genes at once, a simple FC value doesn’t mean much, yet! We still need to use the power that lie within the replicates we have for each sample group. Using these replicates, we want to determine if the observed FC is not just biological noise or a sequencing error. Assignment: Create a histogram of your log2-FC values for all genes. Apply the following steps on your FPM-normalized data for at least one comparison (no need to do it for all possible combinations): average the replicates for two groups separately and add this as new columns to your FPM-normalized data set, calculate the log Fold Change (lFC); simply subtract one group (averaged-column) by another, usually experiment-control Note that since we already have log-transformed data, we do not calculate the ratio (experiment/control). Also, because of the Logarithm quotient rule, we do not divide both average columns as with the ratio calculation, but we subtract: \\(log_b{(x/y)} = log_b{(x)} - log_b{(y)}\\) plot the data using the hist(logFC_column, breaks=60) function (change the breaks argument if needed), Add two vertical lines at -1 and 1 (using abline(v=...)) to indicate some significance (2-fold change). If your data shows very low fold-change values you might see no genes outside of this boundary. This does not mean that we will not find any significantly changed genes (DEGs). If a calculated FC shows a large change in expression between groups this means nothing if the variation within a group is very high. For this reason we use some form of statistical test that checks both the variation in each group and the difference between groups of samples. In the simplest form this usually comes down to using a t-test: “It can be used to determine if two sets of data are significantly different from each other” Most often instead of manually performing t-tests or ANOVA analysis, the article mentions the use of one or more R libraries of other software packages for finding DEGs. This is exactly what we will also do in this chapter. The output of finding DEG’s always includes - but is not limited to - a list of p-values; one p-value per gene. This value indicates whether that gene is a statistically-significant differentially expressed gene (SDEG) and to find these genes of interest all we need to do is get all genes with a p-value below our threshold (i.e. 0.05). 4.3 Using Bioconductor Packages This section demonstrates the use of two packages to perform DEG-analysis on count data. There are many packages available on Bioconductor for RNA-Seq analysis, such as DSS, EBSeq, NOISeq and BaySeq, but here we will focus on edgeR and DESeq2 for processing our count-based data. Chances are that one of these two packages are mentioned if the article described the use of R for the statistical analysis as they are the most widely accepted methods of processing gene expression data. Both packages apply their own normalization methods (described in the sections below) therefore they only work on the raw count data. If your data consists of expected count data, you need to round these values down (using the floor function). You can choose one of these two packages to use, or use them both since it could increase statistical power and they are not very hard to use once you understand how to model the data as we’ll show next. Assignment: Perform data-analysis using at least one of the two packages. If however your article mentioned a different R package for their analysis (for instance the limma package) it is allowed to use this one instead. Other non-R packages can only be done as an extra (each dataset can always be analyzed using both edgeR and DESeq2). Optionally, use more than one package so we can compare the results between the these packages. Start by documenting the relevant information from the article, also if they used other software for this analysis. The two listed packages come with a very complete and extensive manual. Therefore, the course manual (this document) does not guide you exactly on how to use them; that is for you to find out. These manuals can be found as PDF on the BioConductor website, or opened from within R using the vignette function. To see all available manuals (note; these are very different from the function-help documentation) execute the function without arguments: vignette(). This lists the vignettes for edgeR and DESeq2 as: vignette(\"DESeq2\") (this whole course is partly designed on this document!) vignette(\"edgeR\") 4.3.1 The Design (matrix) For all of these packages you need to properly specify how your samples are grouped. We have seen examples of this using an R factor object with the heatmap and MDS visualizations to tell which groups of samples we have and to which group each sample belongs. Reading the documentation for the below packages shows that this is an important part of performing DE-analysis. For example, the following code is shown in the edgeR documentation on page 8 where two sample groups are defined (numbered 1 and 2), placed in a factor object and used as input in the model.matrix function. group &lt;- factor(c(1,1,2,2), labels = c(&quot;Control&quot;, &quot;Case&quot;)) (design &lt;- model.matrix( ~ group)) ## (Intercept) groupCase ## 1 1 0 ## 2 1 0 ## 3 1 1 ## 4 1 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$group ## [1] &quot;contr.treatment&quot; This can often be a very complex part and while it seems very theoretical, in practice it’s not that difficult. To really grasp this topic it is strongly adviced to watch these two videos on how to define your design: StatQuest on YouTube: Using Linear Models for t-tests and ANOVA, Clearly Explained StatQuest on YouTube: Design Matrices For Linear Models, Clearly Explained These videos expects some basic understanding of topics already discussed in the statistics course, otherwise for each mentioned topic in those videos he has other videos available explaining them. See the video-index for a handy overview of all topics discussed on that channel. As mentioned before we need to think about the question we want to ask; which difference do we want to know? With two sample groups as shown above the question is rather easy; ‘which genes show an effect between case/ control samples?’. With more than two sample groups however the question becomes more complex. If we have an experiment comparing influence of three kinds of drugs (thus three groups) combined with effect over time, do we then want to focus on the influence of the drugs or the time? Both are valid questions but they define the way of how to create the design matrix. Documentation on this subject is plenty, however it often contains overwhelming information. This page contains some valuable details (you can safely start reading at Choice of design), including the following text which is based on the same example used in the edgeR documentation: “For the examples we cover here, we use linear models to make comparisons between different groups. Hence, the design matrices that we ultimately work with will have at least two columns: an intercept column, which consists of a column of 1’s, and a second column, which specifies which samples are in a second group. In this case, two coefficients are fit in the linear model: the intercept, which represents the population average of the first group, and a second coefficient, which represents the difference between the population averages of the second group and the first group. The latter is typically the coefficient we are interested in when we are performing statistical tests: we want to know if their is a difference between the two groups.” If you have more then two sample groups and you want to change the question (i.e. test the influence of a different group), read the section about Releveling the factor. 4.3.2 DESeq2 We have used the DESeq2 library before and for DEG analysis we could re-use the DESeqDataSet object but it is adviced to create a new object with the proper design formula instead of the ~ 1 we used before. There is no need to normalize the data using the previously used vst function because the DESeq2 library will normalize the count data for you as follows: “DESeq computes a scaling factor for a given sample by computing the median of the ratio, for each gene, of its read count over its geometric mean across all samples. It then uses the assumption that most genes are not DE and uses this median of ratios to obtain the scaling factor associated with this sample.” For our example data with four sample groups we re-use the annotation data frame as created in the EDA: Heatmap section (printed below). This allows us to do comparisons based on the celltype and ploidy of the samples. One minor note of importance is that the first level in each factor is taken as the reference, or for expression analysis, the control group. In our example (see the table below) the IPSC is the control-celltype and disomic the control-ploidy. In a simple case/ control study, the first level (check with the levels() function) should be the control group. Use the relevel() function if the levels need to be switched.   Cell Ploidy di_IPSC_r1 IPSC disomic di_IPSC_r2 IPSC disomic di_IPSC_r3 IPSC disomic tri_IPSC_r1 IPSC trisomic tri_IPSC_r2 IPSC trisomic tri_IPSC_r3 IPSC trisomic di_NEUR_r1 Neuron disomic di_NEUR_r2 Neuron disomic di_NEUR_r3 Neuron disomic tri_NEUR_r1 Neuron trisomic tri_NEUR_r2 Neuron trisomic tri_NEUR_r3 Neuron trisomic Once you have a proper DESeqDataSet all you need to do is run the DESeq function on this object. Then, using the results function you can extract the DEGs as a DESeqResults object. Applying the summary function on these object(s) shows the number of up and down regulated DEGs as can be seen below that lists the impact of trisomy for the IPSC cell type (~800 DEGs) and comparing both the IPSC and Neuron disomic (control) cell types (as can be seen, a huge amount of genes are affected by differentiation from stem cell into cortical neurons): IPSC - Trisomic vs Disomic Neuron - Trisomic vs Disomic out of 44504 with nonzero total read count adjusted p-value 0 (up) : 432, 0.97% LFC out of 44504 with nonzero total read count adjusted p-value 0 (up) : 8224, 18% LFC The output of the results function contains the following columns for each gene: Column Type Description baseMean intermediate mean of normalized counts for all samples log2FoldChange results log2 fold change (MLE): group IPSC.trisomic vs IPSC.disomic lfcSE results standard error: group IPSC.trisomic vs IPSC.disomic stat results Wald statistic: group IPSC.trisomic vs IPSC.disomic pvalue results Wald test p-value: group IPSC.trisomic vs IPSC.disomic padj results BH adjusted p-values How you call the results function depends heavily on your experiment. As you can see from the output of the summary function, there are no details given about which comparison is shown (and also, by default a p-value of 0.1 is used instead of 0.05). Depending on the design used to create the DESeqDataSet with, one or more comparisons can be made (applying the DESeq function calculates all and you filter with the results function). Read the help for the results function carefully; especially regarding the contrast argument where you define the comparison to retrieve. The following code can be used for our example experiment to get the DEGs comparing the celltypes using an adjusted p-value of 0.05: res &lt;- results(dds, contrast = c(&quot;Cell&quot;, &quot;IPSC&quot;, &quot;Neuron&quot;), alpha = 0.05) The DESeq2 library contains a number of plotting functions that can be applied to a DESeqResults object (output of the results function), the most notable is the plotMA function. Note that the first MA-plot shown below has a very high range for the log fold changes (-10, 10) where the maximum value is 22.4 (shown as a triangle stating it is outside of the plotting range). A log fold change of 22 means &gt;5 million increased expression which seems artificially high. DESeq2 includes a function to perform downstream processing of the estimated log fold change values called lfcShrink which is advised to always run afterwards. The reason for executing this function is described in the vignette with: “It is more useful visualize the MA-plot for the shrunken log2 fold changes, which remove the noise associated with log2 fold changes from low count genes without requiring arbitrary filtering thresholds.” Figure 4.1: MA-plots with the standard DESeq2 output (left) and after shrinking with ‘lfcShrink’ (right) Links Analyzing RNA-seq data with DESeq2 A very comprehensive guide to analyzing RNA-Seq data using DESeq2 (part of this document has been used in this material too!). It is adviced to read the first few sections of this guide and take a good look at the index of the document because there are many interesting sections that might be of help later. Publication, an accompanying article showing differences in performance compared to other methods and packages. 4.3.3 edgeR One of the most mature libraries for RNA-Seq data analysis is the edgeR library available on Bioconductor. There is a very complete (sometimes a bit complex) manual available of which you need to read Chapter 2 with a focus on 2.1 to 2.7, 2.9 and - if you have a more complex design - 2.10. Section 1.4 (Quickstart) shows a code example on the steps needed to do DEG analysis using count data using the two glm methods (quasi-likelihood F-tests and likelihood ratio tests). All the steps shown there are identical for the non-glm method up to calculating the fit object which can be replaced by performing the exactTest function as shown in section 2.9.2. The edgeR library will normalize the count data for you as follows: “The trimmed means of M values (TMM) from Robinson and Oshlack, which is implemented in edgeR, computes a scaling factor between two experiments by using the weighted average of the subset of genes after excluding genes that exhibit high average read counts and genes that have large differences in expression.” Running edgeR requires the raw count data together with the grouping-factor packaged in a DGEList object (with the DGEList() function). Furthermore, a proper model.matrix object (see the section on design) is needed as input for the estimateDisp function. The exact steps to take (there are more variations than with DESeq2) must be searched in the documentation linked above. Once the analysis is done you can retrieve the actual results with the topTags function: ## Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;as.data.frame&#39;: object &#39;qlf&#39; not found Error in h(): ! error in evaluating the argument ‘x’ in selecting a method for function ‘as.data.frame’: object ‘qlf’ not found Backtrace: ▆ 1. ├─pander::pander(as.data.frame(topTags(qlf)), caption = “Most significant genes given by edgeR”) 2. ├─BiocGenerics::as.data.frame(topTags(qlf)) 3. ├─edgeR::topTags(qlf) 4. └─base::.handleSimpleError(…) 5. └─base (local) h(simpleError(msg, call)) Warning messages: 1: package ‘GenomeInfoDb’ was built under R version 4.3.3 2: package ‘matrixStats’ was built under R version 4.3.3 3: package ‘DESeq2’ was built under R version 4.3.3 4: package ‘ggplot2’ was built under R version 4.3.3 5: package ‘pheatmap’ was built under R version 4.3.3 6: package ‘PoiClaClu’ was built under R version 4.3.3 The package also contains a few plotting methods that you can use at intermediate steps during the analysis. For instance, after calculating the normalization factors (calcNormFactors), you can perform multi-dimensional scaling with the plotMDS function which is similar to what we’ve done manually in chapter three: plotMDS(y) Figure 4.2: edgeR MDS plot based on the calculated log2 fold changes Or the dispersion after running the estimateDisp function with the plotBCV function: plotBCV(y) Figure 4.3: edgeR plot of several dispersion methods Or the log-fold changes for all genes, once we have the output of the exactTest function (output et is an DGEExact object) with the plotSmear function. The abline shows a log-FC threshold: deGenes &lt;- decideTestsDGE(qlf, p=0.05) deGenes &lt;- rownames(qlf)[as.logical(deGenes)] plotSmear(qlf, de.tags=deGenes) abline(h=c(-1, 1), col=2) Links Differential Expression Analysis using edgeR tutorial Another tutorial hosted on GitHub "],["chapter-5.html", " Data Analysis and Visualization 5.1 Volcano Plot 5.2 Venn Diagram 5.3 Clustering 5.4 Pathway Analysis", " Data Analysis and Visualization At this point we have one or more sets of DEGs from our experiment. It might be just one set or it might be a large number of comparisons made with a set of DEGs per comparison and/ or results from multiple approaches (manual, packages). This chapter offers some guidance to visualize, summarize and - finally - connect some biological relevance to the results. Browse this chapter first before choosing the visualization(s) to make for your data! If your article contains relevant figures that you would like to try and reproduce (given that enough data is available) that can be done as well instead of the visualizations shown here. Note however that the Volcano plot described below is required. 5.1 Volcano Plot A volcano plot is often the first visualization of the data once the statistical tests are completed. This plot shows data for all genes and we highlight those genes that are considered DEG by using thresholds for both the (adjusted) p-value and a fold-change. Many articles describe values used for these thresholds in their methods section, otherwise a good default is 0.05 for the adjusted p-value and around 1.0 for the log-FC. Here we use the EnhancedVolcano library that creates a ggplot2 volcano plot as shown below. This is packaged in a function deseq.volcano just because we create two figures and only need to change the input data set and plot title. The resulting scatterplot places the -log10(pvalue) values on the y-axis and the log-FC on the x-axis. This often results in a plot representing a volcanic ‘eruption’ where the fold-change influences the spread and the significance the height. Coloring is done based on the thresholds (-log10(0.05) for the adjusted and log2(2) for the log-FC) as follows: (dark) gray: not significant; both the FC and adjusted p-value is below the thresholds. green: high FC, but adjusted p-value is below the threshold. blue: adjusted p-value indicates significance, but FC below the threshold red: significant DEG You can adjust the pCutoff and FCcutoff values depending on your own requirements, see the elaborate documentation and examples on GitHub. The code below wraps the EnhancedVolcano function in a custom function (deseq.volcano) that can be used when there are multiple comparisons to make. library(EnhancedVolcano) ## Simple function for plotting a Volcano plot, returns a ggplot object deseq.volcano &lt;- function(res, datasetName) { return(EnhancedVolcano(res, x = &#39;log2FoldChange&#39;, y = &#39;padj&#39;, lab=rownames(res), title = paste(datasetName, &quot;trisomic vs disomic&quot;), subtitle = bquote(italic(&#39;FDR &lt;= 0.05 and absolute FC &gt;= 2&#39;)), # Change text and icon sizes labSize = 3, pointSize = 1.5, axisLabSize=10, titleLabSize=12, subtitleLabSize=8, captionLabSize=10, # Disable legend legendPosition = &quot;none&quot;, # Set cutoffs pCutoff = 0.05, FCcutoff = 2)) } ## Note: input data is the corrected DESeq2 output using the &#39;lfcShrink&#39; function (see chapter 4) deseq.volcano(res = res.ipsc.lfc, datasetName = &quot;IPSC&quot;) deseq.volcano(res = res.neuron.lfc, datasetName = &quot;Neuron&quot;) ## Note that the most simplest case is: EnhancedVolcano(res.ipsc, x = &#39;log2FoldChange&#39;, y = &#39;padj&#39;, lab = rownames(res.ipsc)) Notes: When you have used edgeR instead you need to change the names of the columns (i.e. log2FoldChange == logFC and padj == FDR) used in this example. In this example there are too many DEGs to annotate with their name in the plots. The EnhancedVolcano plot function has many options for annotating significant genes, see the linked manual for further information. 5.2 Venn Diagram Another common visualization is a Venn-diagram. With our data set we’ve shown two comparisons; trisomic vs disomic in two cell types. Using a Venn-diagram we can show both the shared and unique DEGS for the IPSC trisomic vs IPSC disomic and NEUR trisomic vs NEUR disomic comparisons. Creating a Venn-diagram can be a good method for selecting a (sub)set of DEGs, for instance by selecting the set of DEGs that are shared between groups (the Intersection, see the Wikipedia page on Set Theory or the other way around, only the DEGs that are unique for a group (the Difference). Keep this in mind when continuing on to the clustering section below where it is most likely not useful to cluster all combinations you might have. Code examples: Data preparation (first code-chunk): creating vectors of DEGs for both packages. These DEGs are the gene names (taken from the row names of each DEG-analysis). The output of the DESeq2 results and lfcShrink functions are stored in the res.ipsc.lfc and res.ipsc.neuron objects. Using the VennDiagram library (second code-chunk): good for complex/ large number of comparison, highly configurable but more difficult to use. To see some examples, run the following function in the R console: example('draw.quad.venn') Using the gplots library (third code-chunk): Easier to use and support for complex Venn diagrams, but not good for adjusting appearance. See the blog post “VENN DIAGRAM WITH R OR RSTUDIO: A MILLION WAYS” for more methods of creating Venn diagrams. ## Data preparation pval_threshold &lt;- 0.05 ipsc.degs &lt;- row.names(res.ipsc.lfc[which(res.ipsc.lfc$padj &lt;= pval_threshold), ]) neur.degs &lt;- row.names(res.neuron.lfc[which(res.neuron.lfc$padj &lt;= pval_threshold), ]) The two created vectors of gene names can be used by either taking their lengths (for the VennDiagram library) or just their names (for the gplots library). ## Venn-diagram using the `VennDiagram` library (see below for alternative method) library(VennDiagram) # Arguments for a pairwise (two-sets) venn-diagram are sizes for set1, set2 and overlap (intersect) # Many more functions are available for triple, quad and quantuple diagrams (starting with &#39;draw.***&#39;) venn.plot &lt;- draw.pairwise.venn(length(ipsc.degs), length(neur.degs), # Calculate the intersection of the two sets length( intersect(ipsc.degs, neur.degs) ), category = c(&quot;IPSC Trisomic vs Disomic&quot;, &quot;NEURON Trisomic vs Disomic&quot;), scaled = F, fill = c(&quot;light blue&quot;, &quot;pink&quot;), alpha = rep(0.5, 2), cat.pos = c(0, 0)) # Actually plot the plot grid.draw(venn.plot) Figure 5.1: Venn diagram comparing IPSC and NEUR DEGs using ‘VennDiagram’ library As mentioned, following is an easier alternative (especially when you have &gt; 2 groups) with the venn function from the gplots library. This only needs a list containing either the row-numbers or the gene-names of the DEGs which is easier (but offers less adjustability to make it prettier). ## Alternative method using the `gplots` library library(gplots) # Create a Venn-diagram given just the list of gene-names for both sets venn(list(&quot;IPSC Trisomic vs Disomic&quot; = ipsc.degs, &quot;NEURON Trisomic vs Disomic&quot; = neur.degs), ) Figure 5.2: Venn diagram comparing IPSC and NEUR DEGs using ‘gplots’ library While both analysis result in a similar amount of DEGS (849 for the IPSC and 769 for the Neuron cell types) only 96 DEGs are shared between both cell types and most are thus unique. 5.3 Clustering Here, we can create a heatmap of the found DEGs for all samples where the colors show the fold-change value. Usually this heatmap is shown with a two color gradient; from red (downregulated) to green (upregulated). This is however optional and using the default colors from pheatmap for example is perfectly fine as long as a proper legend is present. There are multiple methods of creating a heatmap (one of which you’ve already used) and most of these directly apply clustering in the visualization. This clustering can be applied to the expression pattern of a gene (row-clustering), the expression pattern of a sample (column-clustering) or both (default for pheatmap). The following example is commonly found in publications as shows the log2 Fold Change values for the comparison that was done. The first and fifth columns show the log2(FC) as calculated by DESeq2 for the two trisomic vs disomic comparisons. The remaining columns are the log fold changes of the separate trisomic replicates vs the average disomic expression (manually calculating the Log(FC) as done in section 4.1 using the DESeq2 normalized count data from the rld.dds object). The DESeq2 calculated FC values show greater changes compared to the manually calculated FC values. The value in showing the manually calculated values too is to see if all replicates show the same patterns within groups. They are however optional unless you only have one comparison as you cannot create a heatmap with a single column. Note that column-clustering has been turned off for this type of plot by setting cluster_cols = FALSE (all replicates are from the same group, clustering will not add any useful information). Figure 5.3: Heatmap showing the fold changes of the Trisomic vs Disomic comparisons for both cell types. The shown genes are all shared DEGS (the 96 shown in the Venn-diagrams) but also filtered for an absolute log2(FC) value of &gt;2. A limitation of such a heatmap will show itself when more then 100 DEGs are found; this just doesn’t fit well in a single figure and causes the clustering and especially the gene names/ labels to be unreadable. It is therefore always adviced to not only filter on the adjusted p-value, but also on the log-FC value to reduce the number of genes shown. Alternatively if the data is properly annotated, instead of showing gene names for each row, showing a GO-term clustering might reveal expression patterns for certain gene groups. 5.4 Pathway Analysis In some cases you’ll see hundreds or even thousands of DEGs as a result of an analysis. These large amounts of DEGs are too much for most visualization approaches or to easily say something about the biological context for that many genes. One approach for tackling such a large set of DEGs is pathway analysis where through different methods genes are grouped by pathway to get an overview of affected pathways in the experiment. This section will demonstrate two methods for this analysis, one using an online platform for gene-annotation enrichment analysis and an R-method for signaling pathway impact analysis. 5.4.1 KEGG-pathway Visualization (&gt;10 DEGs) Knowing the interesting KEGG-pathway(s) upfront (i.e. it is listed in the article) allows for visualizations applied to that selected pathway. For this we can use the pathview website or Bioconductor library: “… It maps and renders a wide variety of biological data on relevant pathway graphs. All users need is to supply their data and specify the target pathway. Pathview automatically downloads the pathway graph data, parses the data file, maps user data to the pathway, and render pathway graph with the mapped data.” This section demonstrates only the use of the bioconductor package for visualizing not only the DEGs onto pathways, but also the change in expression for each gene. ## Bioconductor library (install with bioclite() if missing) library(pathview) ## Example pathway IDs (for human, change organism key for other organisms) data(&quot;paths.hsa&quot;) pander(head(paths.hsa, n=5)) hsa00010 hsa00020 Glycolysis / Gluconeogenesis Citrate cycle (TCA cycle) Table continues below hsa00030 hsa00040 hsa00051 Pentose phosphate pathway Pentose and glucuronate interconversions Fructose and mannose metabolism ## Check &#39;gene.idtype&#39; argument possibilities data(gene.idtype.list); pander(gene.idtype.list) SYMBOL, GENENAME, ENSEMBL, ENSEMBLPROT, UNIGENE, UNIPROT, ACCNUM, ENSEMBLTRANS, REFSEQ, ENZYME, TAIR, PROSITE and ORF ## Prepare data for visualization deseq.degs.logfc &lt;- subset(deseq.results, padj &lt; pval_threshold, select = log2FoldChange) pander(head(deseq.degs.logfc)) pathview(gene.data=deseq.degs.logfc, pathway.id=&quot;00020&quot;, # TCA-cycle species=&quot;hsa&quot; # Organism key ) "],["a2-data_loading.html", "A Batch Loading Expression Data in R", " A Batch Loading Expression Data in R source(&#39;utils.R&#39;) library(pander) This code example shows how to batch-load multiple files containing expression (count) data for a single sample. The data for this example can be found on GEO with ID GSE109798. Depending on the tool used to convert the mapping data into a per-sample count file, contents of these files can be (very) different. Downloading the data for this example experiment from GEO gives us a single .tar file called GSE109798_RAW.tar. Extracting this archive file nets us a folder with the following files: file.names &lt;- list.files(&#39;./data/GSE109798_RAW/&#39;) Files GSM2970149_4T1E274.isoforms.results.txt GSM2970150_4T1E266.isoforms.results.txt GSM2970151_4T1E247D.isoforms.results.txt GSM2970152_4T1P2247A.isoforms.results.txt GSM2970153_4T1P2247G.isoforms.results.txt GSM2970154_4T1P2247F.isoforms.results.txt GSM2970155_HCC1806E224B.isoforms.results.txt GSM2970156_HCC1806E224A.isoforms.results.txt GSM2970157_HCC1806E224C.isoforms.results.txt GSM2970158_HCC1806P2232A.isoforms.results.txt GSM2970159_HCC1806P2232B.isoforms.results.txt GSM2970160_HCC1806P2230.isoforms.results.txt A.0.1 Decompressing The file extension of all these files is .txt.gz which means that all files are compressed using gzip and need to be unpacked before they can be loaded. The easiest method is using the system gunzip command on all files which can be done from within R by applying the gunzip command using the system function on each file. ## Change directory to where the files are stored setwd(&#39;./data/GSE109798_RAW/&#39;) sapply(file.names, FUN = function(file.name) { system(paste(&quot;gunzip&quot;, file.name)) }) Now we can update the file.names variable since each file name has changed. file.names &lt;- list.files(&#39;./data/GSE109798_RAW/&#39;) A.0.2 Determining Data Format Next, we can inspect what the contents are of these files, assuming that they all have the same layout/ column names etc. to decide what we need to use for our analysis. ## Call the system &#39;head&#39; tool to &#39;peek&#39; inside the file system(paste0(&quot;head &quot;, &quot;./data/GSE109798_RAW/&quot;, file.names[1])) transcript_id gene_id length effective_length expected_count TPM FPKM IsoPct uc007aet.1 1 3608 3608.00 1.82 0.44 0.24 100.00 uc007aeu.1 1 3634 3634.00 0.00 0.00 0.00 0.00 uc011whv.1 10 26 26.00 0.00 0.00 0.00 0.00 uc007amd.1 100 1823 1823.00 0.00 0.00 0.00 0.00 uc007ame.1 100 4355 4355.00 1.32 0.26 0.15 100.00 uc007dac.1 1000 1403 1403.00 2.00 1.25 0.70 100.00 uc008ajp.1 10000 1078 1078.00 0.00 0.00 0.00 0.00 uc012ajs.1 10000 1753 1753.00 0.00 0.00 0.00 0.00 uc008ajq.1 10001 2046 2046.00 0.00 0.00 0.00 0.00 These files contain (much) more then just a count value for each gene as we can see columns such as (transcript) length, TPM, FPKM, etc. Also, the count-column is called expected_count which raises a few questions as well. The expected count value is usable as it contains more information - compared to the raw count - then we actually require. The expected part results from multimapped reads where a single read mapped to multiple positions in the genome. As each transcript originates only from one location, this multimapped read is usually discarded. With the expected count though, instead of discarding the read completely it is estimated where it originates from and this is added as a fraction to the count value. So the value of 1.32 that we see on line 5 in the example above means an true count of 1 (uniquely mapped read) and the .32 (the estimated part) results from an algorithm and can mean multiple things. As mentioned before, we require integer count data for use with packages such as DESeq2 and edgeR and there are two methods to convert the expected count to raw count data: + round the value to the nearest integer (widely accepted method and is well within the expected sampling variation), or + discard the fraction part by using for example the floor() function. A.0.3 Loading Data From all these columns we want to keep the transcript_id and expected_count columns and ignore the rest (we might be interested in this data later on though). As we need to lead each file separately we can define a function that reads in the data, keeping the columns of interest and returning a dataframe with this data. Note that the first line of each file is used as a header, but check before setting the header argument to TRUE, sometimes the expression data starts at line 1. The file name is then also used to name the column in the dataframe so that we know which column is which sample. This is done by splitting the file name (using strsplit) using the dot (‘.’) keeping the first part (i.e. ‘GSM2970156_HCC1806E224A’) and discarding the second part (‘isoforms.results.txt’). The strsplit function however always returns a list, in this case containing a vector with the 5 splitted elements: ## String splitting in R ## (the fixed = TRUE is required as the dot is a special character, see &#39;?strsplit&#39;) strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE) ## [[1]] ## [1] &quot;GSM2970155_HCC1806E224B&quot; &quot;isoforms&quot; ## [3] &quot;results&quot; &quot;txt&quot; ## [5] &quot;gz&quot; ## Keeping the sample identifier strsplit(&#39;GSM2970155_HCC1806E224B.isoforms.results.txt.gz&#39;, &#39;.&#39;, fixed = TRUE)[[1]][1] ## [1] &quot;GSM2970155_HCC1806E224B&quot; ## Function for reading in files read_sample &lt;- function(file.name) { ## Extract the sample name for naming the column sample.name &lt;- strsplit(file.name, &quot;.&quot;, fixed = TRUE)[[1]][1] ## Read the data, setting the &#39;transcript_id&#39; as row.names (column 1) sample &lt;- read.table(file.name, header = TRUE, sep=&quot;\\t&quot;, row.names = NULL) ## Rename the count column names(sample)[5] &lt;- sample.name ## Return a subset containing the &#39;transcript_id&#39; and sample name columns return(sample[c(1, 5)]) } Applying the read_sample function to all file names gives us a set of data frames that we can merge together using the merge function. We merge the data based on the transcript id defined with the by = 1 argument pointing to the first column. We start by reading in just one file which is the ‘base’ dataframe to which we will merge the other files. During processing it seemed that this data set is divided into two groups which is also listed on the GEO website for this project: GPL11154 Illumina HiSeq 2000 (Homo sapiens) GPL13112 Illumina HiSeq 2000 (Mus musculus) where the first 6 files are from human source and the last 6 from the mouse. Therefore, the following code only shows how to read the first 6 samples and merge these into a single dataframe. Repeating this process for the other 6 files would result into another dataframe for those samples. setwd(&#39;./data/GSE109798_RAW/&#39;) ## Read the FIRST sample dataset &lt;- read_sample(file.names[1]) ## Read first sample group (6) for (file.name in file.names[2:6]) { sample &lt;- read_sample(file.name) dataset &lt;- merge(dataset, sample, by = 1) } pander(head(dataset)) transcript_id GSM2970149_4T1E274 GSM2970150_4T1E266 GSM2970151_4T1E247D uc007aet.1 1.82 0 0 uc007aeu.1 0 1 0.24 uc007aev.1 0 0 0 uc007aew.1 0 0 0.97 uc007aex.2 0 0 0 uc007aey.1 0 0 0 Table continues below GSM2970152_4T1P2247A GSM2970153_4T1P2247G GSM2970154_4T1P2247F 0 0 0 0.13 0 0 0 0 0 0 1 0 0 0 0 0 0 0 The dataset variable now contains all data for the first 6 samples in this experiment. It is advisable to compare the number of rows in this data set with the number of rows in a single sample. It is not guaranteed that all samples have exactly the same number of genes/transcripts present (i.e., 0-values might have been discarded) which results in a final data set that has as many rows as the smallest sample. See the help of merge if this is the case because the all argument can be used to introduce extra rows for missing data. "],["a3-annotation.html", "B Annotating an RNA-Seq Experiment B.1 Downloading annotation data from GEO B.2 Manual Data Annotation", " B Annotating an RNA-Seq Experiment This chapter describes annotate the data, meaning assigning names and functions to our Differentially Expressed Genes. This step can either be very easy or a bit more challenging depending on the data source of your project. The first example is relevant if your data set originated from the NCBI GEO, which ‘should’ always be annotated by default. If you have found your data set elsewhere (and do not have a GEO identifier for your project), skip to the Manually annotate your data section below. The goal of this chapter is to - at least - find a Gene Symbol or common ID (NCBI/ Ensembl) for each gene. Using this information it will be much easier to find relevant information from other online sources to say something about the functionality and impact of your DEGs. Even though you might be lucky and either already have your data or a simple GEO query results in everything you want/ need, you are tasked to also perform the manual annotation phase even if you will not use any of the found annotation data. Having written the code to access these sources is a good excercise in using complex Bioconductor packages. Also, you can always use this to find more information should you require this later on. B.1 Downloading annotation data from GEO Given the GSE**** id of your experiment you can download any available (annotation)data from GEO using the GEOquery library. For the example experiment used previously, the following code downloads two files; a series_matrix.txt.gz file that is also available for download from the NCBI GEO website. Note the destdir argument for storing it in a known location instead of the /tmp folder for later use. library(GEOquery) getGEO(GEO = &#39;GSE101942&#39;, destdir = &quot;./data/&quot;) Once downloaded, you can load the data using: library(GEOquery) gse &lt;- getGEO(filename = &#39;./data/GSE101942_series_matrix.txt.gz&#39;) The gse object may contain useful information about both samples and genes. Unfortunately for this example data set there is only information available regarding the samples which we can access in the phenoData slot. Slots in R objects are a method of storing multiple data objects into a single R object, in this case this object is called an ExpressionSet (see ?ExpressionSet for a description of its contents and structure). For storing information about an experiment this is very handy; you can store a data frame with the actual expression data, a list containing laboratory information, a chunk of text with an abstract, etc. all in a single object. The most common data sets found in an ExpressionSet object: * @assayData: the actual expression data. Most common for microarray experiments and (very) rare for RNA-seq experiments * @phenoData: sample information, should be present for each GEO data set * @featureData: a data frame holding feature (=gene) information, slightly rare for RNA-seq data sets * @experimentData: information about the lab which performed the experiment You already know how to access a column in a data frame using the $ notation, and accessing a complete data frame in the gse object is done using the @ symbol: # Access a data frame containing phenotype information gse@phenoData@data Here you see that each sample is described using its sample identifier (GSM****), information about the the sample group (title column) and a number of characteristics. The actual information included depends on your data set.   title source_name_ch1 characteristics_ch1 GSM2719212 C2A trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719213 C2B trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719214 C2C trisomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719215 C244-A disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719216 C244-B disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) GSM2719217 C243 disomic_IPSC cell type: human induced pluripotent stem cells (iPSCs) Contents of the gse@phenoData@data information (selection) (continued below)   characteristics_ch1.1 GSM2719212 ploidy: trisomic GSM2719213 ploidy: trisomic GSM2719214 ploidy: trisomic GSM2719215 ploidy: disomic GSM2719216 ploidy: disomic GSM2719217 ploidy: disomic If your data set has properly annotated gene information, this should be accessible in the @featureData slot. For instance, the data set GSE20489 does have this annotation available. Once loaded, we see that the following annotation columns are present for each gene in the featureData slot (following is a small subset, in total there can be well over 20 columns of information per gene). Also, the metadata listing shows for how many features (=genes, 54675) and samples (54) the expression data is present (in the @assayData slot): # This gets a list with a single &#39;ExpressionSet&#39; object GSE20489 &lt;- getGEO(GEO = &quot;GSE20489&quot;, destdir = &#39;./data&#39;) # Print some metadata of the featureData slot GSE20489[[1]] ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 54675 features, 54 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: GSM514737 GSM514738 ... GSM514790 (54 total) ## varLabels: title geo_accession ... tissue:ch1 (39 total) ## varMetadata: labelDescription ## featureData ## featureNames: 1007_s_at 1053_at ... AFFX-TrpnX-M_at (54675 total) ## fvarLabels: ID GB_ACC ... Gene Ontology Molecular Function (16 total) ## fvarMetadata: Column Description labelDescription ## experimentData: use &#39;experimentData(object)&#39; ## pubMedIds: 23883607 ## Annotation: GPL570 ID Gene Title Gene Symbol ENTREZ_GENE_ID 1007_s_at discoidin domain receptor tyrosine kinase … DDR1 /// MIR4640 780 /// 100616237 1053_at replication factor C (activator 1) 2, 40kDa RFC2 5982 117_at heat shock 70kDa protein 6 (HSP70B’) HSPA6 3310 121_at paired box 8 PAX8 7849 1255_g_at guanylate cyclase activator 1A (retina) GUCA1A 2978 1294_at microRNA 5193 /// ubiquitin-like modifier … MIR5193 /// UBA7 7318 /// 100847079 Gene information stored in an ExpressionSet object for experiment GSE20489 (continued below) Gene Ontology Biological Process 0001558 // regulation of cell growth // in… 0000278 // mitotic cell cycle // traceable… 0000902 // cell morphogenesis // inferred … 0001655 // urogenital system development /… 0007165 // signal transduction // non-trac… 0006464 // cellular protein modification p… B.2 Manual Data Annotation Luckily for us, R offers a number of libraries to automatically retrieve information for large sets of genes, unless you have chosen an organism for which not much data is available. This section demonstrates the use of two such libraries, starting with AnnotationDbi followed by biomaRt. The AnnotationDbi library downloads a local copy of an organism-specific database with gene information where biomaRt uses online databases to retrieve data given a query. biomaRt offers far more data (over 1000 data fields per organism) but is more complex to use. Since biomaRt is also relying on online databases it might be a good strategy to annotate only the genes of interest (the DEGs) instead of querying for &gt;20.000 genes while we might only retain 20 after statistical analysis. If you are planning to use biomaRt, skip to the Discovering Differentialy Expressed Genes (DEGs) chapter first and then return to the Using R Bioconductors biomaRt section to annotate the data. B.2.1 Using AnnotationDBI The AnnotationDbi offers data sets for many organisms in the form of installable libraries and depending on your experiment you need to find the proper library. The example experiment contains samples of the house mouse (Mus musculus) and therefore we select its data set. # Load the AnnotationDbi interface library library(AnnotationDbi) # Load the Bioconductor installation library (contains &#39;biocLite()&#39;) library(BiocInstaller) # Install and load the organism specific gene database # &#39;org&#39; for Organism # &#39;Hs&#39; for Homo sapiens # &#39;eg&#39; for Entrez Gene IDs # (try to load (using the library function) first before installing, it might already be present) biocLite(&#39;org.Hs.eg.db&#39;) library(org.Hs.eg.db) The following information types are available in this database (use the columns function to inspect). Columns Columns Columns Columns Columns ACCNUM ENZYME GOALL PATH UCSCKG ALIAS EVIDENCE IPI PFAM UNIPROT ENSEMBL EVIDENCEALL MAP PMID ENSEMBLPROT GENENAME OMIM PROSITE ENSEMBLTRANS GENETYPE ONTOLOGY REFSEQ ENTREZID GO ONTOLOGYALL SYMBOL Available fields in the database The table below shows the data available for all information types given a randomly chosen EntrezID of ‘1080’. Note that the table has been split to show the 24 data types with their values. .Hs.data.table &lt;- cbind(.Hs.data[1:12,], .Hs.data[13:24,]) colnames(.Hs.data.table) &lt;- c(&#39;Data Type&#39;, &#39;Example Value&#39;, &#39;Data Type&#39;, &#39;Example Value&#39;) pander(.Hs.data.table, caption=&quot;Example values for each field in &#39;org.Hs.eg.db&#39;&quot;) Data Type Example Value Data Type Example Value ACCNUM AAA35680 GOALL GO:0000003 ALIAS ABC35 IPI IPI00816721 ENSEMBL ENSG00000001626 MAP 7q31.2 ENSEMBLPROT NA OMIM 167800 ENSEMBLTRANS NA ONTOLOGY MF ENTREZID NA ONTOLOGYALL BP ENZYME 3.6.3.49 PATH 02010 EVIDENCE IDA PFAM PF00664 EVIDENCEALL ISS PMID 1284466 GENENAME CF transmembrane conductance regulator PROSITE PS50929 GENETYPE protein-coding REFSEQ NM_000492 GO GO:0005254 SYMBOL CFTR Example values for each field in ‘org.Hs.eg.db’ Retrieving data from the locally stored annotation database can be done using the mapIds function which takes a number of arguments: x: the local database to query keys: the IDs from your own data set, most often you have ENSEMBL IDs or gene SYMBOLS. In the example data the rownames of the data set contain the gene SYMBOLS (see Table 1 on page 2) column: the data column to retrieve from the database keytype: the type of data that you provide. In the example data this is a gene SYMBOL multiVals: a number of columns contain more then one entry for a single gene, in that case we only want to store the first one. The output of the mapIds function is a single character vector that we can add to our data frame containing the DEGs (in this case the DEGs for the IPSC trisomic vs disomic comparison containing 828 genes stored in the ipsc.tri.vs.di data frame). # Retrieve the ENSEMBL gene ID, e.g. &#39;ENSG00000001626&#39; ipsc.tri.vs.di$Ensembl &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENSEMBL&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) # Retrieve the ENTREZ gene ID, e.g. &#39;12323&#39; ipsc.tri.vs.di$EntrezID &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENTREZID&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) # Retrieve the KEGG enzyme code, e.g. 2.7.11.17 ipsc.tri.vs.di$Enzyme &lt;- mapIds(x = org.Hs.eg.db, keys=row.names(ipsc.tri.vs.di), column=&quot;ENZYME&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) Unfortunately, not all organisms offer access to such information nor is the information always complete. For instance, the following table shows the number of available records for the Ensembl, Entrez en KEGG IDs downloaded:   ENSEMBL ENTREZ ENZYME Available 626 642 62 Missing 223 207 787 Statistics for annotation columns using AnnotationDbi For other data sets you might have more luck, otherwise continue with the biomaRt method explained below. B.2.2 Using biomaRt A short explanation about biomart (source: Wikipedia): “The purpose of the BioMarts in Ensembl Genomes is to allow the user to mine and download tables containing all the genes for a single species, genes in a specific region of a chromosome or genes on one region of a chromosome associated with an InterPro domain. The BioMarts also include filters to refine the data to be extracted and the attributes (Variant ID, Chromosome name, Ensembl ID, location, etc.) that will appear in the final table file can be selected by the user.” The text above mentiones species, attributes and filters, and we need to combine these elements to query the biomart databases for our annotation. The following code ‘chunks’ show possible values for these elements and how to gather and store the relevant data. The biomaRt library interfaces with the biomart.org online database. Sometimes the biomart website (which offers a browsable database) is down due to maintenance, but its many mirrors can still be used, for instance at Ensembl. You are required to explore a number of objects and the given example code is most likely not suitable for your data/ organism. The project is well documented on the Bioconductor biomaRt website that links to the biomaRt users guide. # Load the library library(biomaRt) # Use an alternative database server as the regular one sometimes has issues.. ensembl=useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, host=&quot;https://www.ensembl.org&quot;) # Select the &#39;ensembl&#39; database ensembl &lt;- useMart(&quot;ensembl&quot;) Using the listDatasets function you can get a full list of available datasets. Store this list in an R object and ‘browse’ this object in RStudio to see if your organism is included. Copy the name of the dataset, this is the species element we will use. mart.datasets &lt;- listDatasets(ensembl) # Select the correct dataset, for the example data we select the &#39;hsapiens_gene_ensembl&#39; ensembl &lt;- useDataset(&#39;hsapiens_gene_ensembl&#39;, mart = ensembl) dataset description version abrachyrhynchus_gene_ensembl Pink-footed goose genes (ASM259213v1) ASM259213v1 acalliptera_gene_ensembl Eastern happy genes (fAstCal1.3) fAstCal1.3 acarolinensis_gene_ensembl Green anole genes (AnoCar2.0v2) AnoCar2.0v2 acchrysaetos_gene_ensembl Golden eagle genes (bAquChr1.2) bAquChr1.2 acitrinellus_gene_ensembl Midas cichlid genes (Midas_v5) Midas_v5 amelanoleuca_gene_ensembl Giant panda genes (ASM200744v2) ASM200744v2 Subset of the 214 datasets available Next is deciding on a filter. For this we can use the listFilters function on the ensembl object, storing the full list of filters. Here too it is wise to view this in RStudio to find the filter to use. The filter specifies what you will use to search on. For instance, the AnnotationDbi queries above gave us Ensembl gene ID’s and we could use those with the ensembl_gene_id filter. When viewing the list of filters in RStudio you can use the search text-box in the top-right of the view, for example with ensembl to look for filters to use with this type of identifier. filters &lt;- listFilters(ensembl) name description chromosome_name Chromosome/scaffold name start Start end End band_start Band Start band_end Band End marker_start Marker Start marker_end Marker End strand Strand chromosomal_region e.g. 1:100:10000:-1, 1:100000:200000:1 with_biogrid With BioGRID Interaction data, The General Repository for Interaction Datasets ID(s) with_ccds With CCDS ID(s) with_chembl With ChEMBL ID(s) with_dbass3 With DataBase of Aberrant 3’ Splice Sites ID(s) with_dbass5 With DataBase of Aberrant 5’ Splice Sites ID(s) with_entrezgene_trans_name With EntrezGene transcript name ID(s) Subset of the 438 available filters Finally we get to the point to determine what data we would like to get from the database. These are the attributes which we can get with the listAttributes function on the ensembl object. Again - and especially with the attributes since there are often &gt;1000 of selectable options - we store the attributes and view them in RStudio to look for data that we want. attributes &lt;- listAttributes(ensembl) name description page ensembl_gene_id Gene stable ID feature_page ensembl_gene_id_version Gene stable ID version feature_page ensembl_transcript_id Transcript stable ID feature_page ensembl_transcript_id_version Transcript stable ID version feature_page ensembl_peptide_id Protein stable ID feature_page ensembl_peptide_id_version Protein stable ID version feature_page ensembl_exon_id Exon stable ID feature_page description Gene description feature_page chromosome_name Chromosome/scaffold name feature_page start_position Gene start (bp) feature_page end_position Gene end (bp) feature_page strand Strand feature_page band Karyotype band feature_page transcript_start Transcript start (bp) feature_page transcript_end Transcript end (bp) feature_page transcription_start_site Transcription start site (TSS) feature_page Subset of the 3163 available attributes If we want to have the gene chromosome, start- and end-position as well as its description (note, just as an example, there is other, more interesting information available too!) we combine this in a character vector (attrs.get, see below). There is one caveat though, the order in which you get back the results are not the same as the input order! This means that we cannot simple combine the data with our original data set but we need to merge it together. However, to be able to merge the data we need to know which record belongs to which gene and therefore we add our selected filter (in our case the ensembl_gene_id) to the list of attributes to get. Then, when we get the results dataframe we can use the merge function in R to combine the gene information with the actual data: merge(x = ipsc.tri.vs.di, y = results, by.x = &#39;Ensembl&#39;, by.y = &#39;ensembl_gene_id&#39;) We now have all three needed elements (species, filter and attributes) so we are ready to query the database with the getBM function (read the help using ?getBM). The example below retrieves data using a set of five Ensembl gene IDs since not all genes have an Ensembl ID as we’ve seen above, so we filter those out first and use this as the values parameter below. # Set the &#39;attributes&#39; values attrs.get &lt;- c(&quot;ensembl_gene_id&quot;, &quot;chromosome_name&quot;, &quot;start_position&quot;,&quot;end_position&quot;, &quot;description&quot;) # Perform a biomaRt query using &#39;getBM&#39; results &lt;- getBM(attributes = attrs.get, filters = &quot;ensembl_gene_id&quot;, values = ipsc.tri.vs.di$Ensembl[1:5], mart = ensembl) results$gene_length &lt;- abs(results$end_position - results$start_position) The results object is a data.frame with 5 columns that we can merge with our data set giving us the following annotation columns (combined from the AnnotationDBI and biomaRt libraries). This was just an example on how to use the biomaRt library and it comes down to selecting the correct filter and looking for interesting attributes to retrieve. Further information can be found in the documentation avaialble with `vignette(‘biomaRt’) al., Patrick Deelen et. 2015. “Calling Genotypes from Public RNA-Sequencing Data Enables Identification of Genetic Variants That Affect Gene-Expression Levels.” Genome Medicine 7 (30). Chen, Yunshun, Aaron Lun, Davis McCarthy, Xiaobei Zhou, Mark Robinson, and Gordon Smyth. 2018. edgeR: Empirical Analysis of Digital Gene Expression Data in r. http://bioinf.wehi.edu.au/edgeR. Daróczi, Gergely, and Roman Tsegelskyi. 2017. Pander: An r ’Pandoc’ Writer. https://CRAN.R-project.org/package=pander. Love, Michael, Simon Anders, and Wolfgang Huber. 2017. DESeq2: Differential Gene Expression Analysis Based on the Negative Binomial Distribution. https://github.com/mikelove/DESeq2. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
